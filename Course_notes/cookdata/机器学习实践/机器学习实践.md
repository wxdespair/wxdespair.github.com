æœºå™¨å­¦ä¹ å’Œsklearnä»‹ç»

## æœºå™¨å­¦ä¹ æ¦‚è¿°

æœºå™¨å­¦ä¹ æœ‰ä¸‹é¢å‡ ç§å®šä¹‰ï¼š

*   æœºå™¨å­¦ä¹ æ˜¯ä¸€é—¨äººå·¥æ™ºèƒ½çš„ç§‘å­¦ï¼Œè¯¥é¢†åŸŸçš„ä¸»è¦ç ”ç©¶å¯¹è±¡æ˜¯äººå·¥æ™ºèƒ½ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åœ¨ç»éªŒå­¦ä¹ ä¸­æ”¹å–„å…·ä½“ç®—æ³•çš„æ€§èƒ½ã€‚
*   æœºå™¨å­¦ä¹ æ˜¯å¯¹èƒ½é€šè¿‡ç»éªŒè‡ªåŠ¨æ”¹è¿›çš„è®¡ç®—æœºç®—æ³•çš„ç ”ç©¶ã€‚
*   æœºå™¨å­¦ä¹ æ˜¯ç”¨æ•°æ®æˆ–ä»¥å¾€çš„ç»éªŒï¼Œä»¥æ­¤ä¼˜åŒ–è®¡ç®—æœºç¨‹åºçš„æ€§èƒ½æ ‡å‡†ã€‚



å¯¹æ•°æ®è¿›è¡Œèšç±»åˆ†ç±»ç­‰å¤„ç†ï¼Œæœ€ç»ˆå¾—åˆ°é¢„æµ‹æˆ–æ£€æµ‹å‡ºå¼‚å¸¸å€¼ã€‚

#### æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ–¹æ³•

ä¸€èˆ¬æœºå™¨å­¦ä¹ çš„æ–¹æ³•åˆ†ä¸ºï¼š

*   æœ‰ç›‘ç£å­¦ä¹ ï¼ˆsupervised learningï¼‰
    *   æ•°æ®é›†ä¸­çš„æ ·æœ¬å¸¦æœ‰æ ‡ç­¾ï¼Œæœ‰æ˜ç¡®ç›®æ ‡
    *   å›å½’å’Œåˆ†ç±»
*   æ— ç›‘ç£å­¦ä¹ ï¼ˆunsupervised learningï¼‰
    *   æ•°æ®é›†ä¸­çš„æ ·æœ¬æ²¡æœ‰æ ‡ç­¾ï¼Œæ²¡æœ‰æ˜ç¡®ç›®æ ‡
    *   èšç±»ã€é™ç»´ã€æ’åºã€å¯†åº¦ä¼°è®¡ã€å…³è”è§„åˆ™æŒ–æ˜
*   å¼ºåŒ–å­¦ä¹ ï¼ˆreinforcement learningï¼‰
    *   æ™ºæ…§å†³ç­–çš„è¿‡ç¨‹ï¼Œé€šè¿‡è¿‡ç¨‹æ¨¡æ‹Ÿå’Œè§‚å¯Ÿæ¥ä¸æ–­å­¦ä¹ ã€æé«˜å†³ç­–èƒ½åŠ›
    *   ä¾‹å¦‚ï¼šAlphaGo

ä¸€äº›åŸºæœ¬æ¦‚å¿µï¼š

*   æ•°æ®é›†ï¼šä¸€ç»„æ ·æœ¬çš„é›†åˆã€‚
*   æ ·æœ¬ï¼šæ•°æ®é›†çš„ä¸€è¡Œã€‚ä¸€ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªç‰¹å¾ï¼Œæ­¤å¤–è¿˜å¯èƒ½åŒ…å«ä¸€ä¸ªæ ‡ç­¾ã€‚
*   ç‰¹å¾ï¼šåœ¨è¿›è¡Œé¢„æµ‹æ—¶ä½¿ç”¨çš„è¾“å…¥å˜é‡ã€‚
*   è®­ç»ƒé›†ï¼šç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†
*   æµ‹è¯•é›†ï¼šç”¨äºæµ‹è¯•æ¨¡å‹çš„æ•°æ®é›†
*   æ¨¡å‹ï¼šå»ºç«‹æ•°æ®çš„è¾“å…¥xå’Œè¾“å‡ºğ‘¦ä¹‹é—´çš„æ˜ å°„å…³ç³»
    *   $ y=f(x) $ 
*   æŸå¤±å‡½æ•°ï¼š $ L(f(x_i),y_i) = (f(x_i)-y_i)^2 $ 
*   ä¼˜åŒ–ç›®æ ‡ï¼š $ \min \limits_{f \in F} \frac{1}{n} \overset{n}{\sum \limits_{i=1}} L(y_i,f(x_i)) $  

**æœ‰ç›‘ç£å­¦ä¹ **

æ•°æ®é›†ä¸­çš„æ ·æœ¬å¸¦æœ‰æ ‡ç­¾ 

ç›®æ ‡ï¼šæ‰¾åˆ°æ ·æœ¬åˆ°æ ‡ç­¾çš„æœ€ä½³æ˜ å°„

å…¸å‹æ–¹æ³•ï¼š

*   å›å½’æ¨¡å‹ï¼šçº¿æ€§å›å½’ã€å²­å›å½’ã€LASSOå’Œå›å½’æ ·æ¡ç­‰

    *   å…¸å‹çš„æœ‰ç›‘ç£ä»»åŠ¡ï¼Œæ ·æœ¬çš„æ ‡ç­¾ä¸ºè¿ç»­å‹ï¼Œå¦‚æ”¶å…¥ã€é”€é‡ç­‰

    *   åº”ç”¨åœºæ™¯ï¼š

        æµè¡Œç—…å­¦ï¼šå¸çƒŸå¯¹æ­»äº¡ç‡å’Œå‘ç—…ç‡å½±å“çš„æ—©æœŸè¯æ®æ¥è‡ªé‡‡ç”¨äº†å›å½’åˆ†æçš„è§‚å¯Ÿæ€§è¯æ®

        é‡‘èï¼šèµ„æœ¬èµ„äº§å®šä»·æ¨¡å‹åˆ©ç”¨çº¿æ€§å›å½’ä»¥åŠBetaç³»æ•°çš„æ¦‚å¿µåˆ†æå’Œè®¡ç®—æŠ•èµ„çš„ç³»ç»Ÿé£é™©

        ç»æµå­¦ï¼šé¢„æµ‹æ¶ˆè´¹æ”¯å‡ºï¼Œå›ºå®šæŠ•èµ„æ”¯å‡ºï¼Œå­˜è´§æŠ•èµ„ï¼Œä¸€å›½å‡ºå£äº§å“è´­ä¹°ï¼ŒåŠ³åŠ¨åŠ›éœ€æ±‚ï¼ŒåŠ³åŠ¨åŠ›ä¾›ç»™

*   åˆ†ç±»æ¨¡å‹ï¼šé€»è¾‘å›å½’ã€Kè¿‘é‚»ã€å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºç­‰

    *   å…¸å‹çš„æœ‰ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œæ ·æœ¬æ ‡ç­¾ä¸ºç¦»æ•£å‹ã€‚

    *   åŒ…æ‹¬äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»é—®é¢˜ã€‚

    *   åº”ç”¨åœºæ™¯ï¼š

        ä¿¡ç”¨é£é™©è¯„ä¼°

        é¢„æµ‹è‚¿ç˜¤ç»†èƒæ˜¯è‰¯æ€§è¿˜æ˜¯æ¶æ€§

        é‚®ä»¶çš„åˆ†ç±»ï¼šæ­£å¸¸é‚®ä»¶/åƒåœ¾é‚®ä»¶

        ç”µä¿¡å®¢æˆ·æµå¤±åˆ†æ

**æ— ç›‘ç£å­¦ä¹ **

å¯ä»¥å¤„ç†æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®

æ ¹æ®æ•°æ®æœ¬èº«çš„åˆ†å¸ƒç‰¹ç‚¹ï¼ŒæŒ–æ˜åæ˜ æ•°æ®çš„å†…åœ¨ç‰¹æ€§

å…¸å‹æ–¹æ³•ï¼š

*   èšç±»

    *   ç›®çš„ï¼šå°†æ•°æ®é›†ä¸­ç›¸ä¼¼çš„æ ·æœ¬è¿›è¡Œåˆ†ç»„ï¼Œä½¿å¾—ï¼šåŒä¸€ç»„å¯¹è±¡ä¹‹é—´å°½å¯èƒ½ç›¸ä¼¼ï¼›ä¸åŒç»„å¯¹è±¡ä¹‹é—´å°½å¯èƒ½ä¸ç›¸ä¼¼ã€‚

    *   åº”ç”¨åœºæ™¯ï¼š

        åŸºå› è¡¨è¾¾æ°´å¹³èšç±»ï¼šæ ¹æ®ä¸åŒåŸºå› è¡¨è¾¾çš„æ—¶åºç‰¹å¾è¿›è¡Œèšç±»ï¼Œå¾—åˆ°åŸºå› è¡¨è¾¾å¤„äºä¿¡å·é€šè·¯ä¸Šæ¸¸è¿˜æ˜¯ä¸‹æ¸¸çš„ä¿¡æ¯

        ç¯®çƒè¿åŠ¨å‘˜åˆ’åˆ†ï¼šæ ¹æ®çƒå‘˜ç›¸å…³æ•°æ®ï¼Œå°†å…¶åˆ’åˆ†åˆ°ä¸åŒç±»å‹ï¼ˆæˆ–è€…ä¸åŒç­‰çº§ï¼‰çš„è¿åŠ¨å‘˜é˜µè¥ä¸­

        å®¢æˆ·åˆ†æï¼šæŠŠå®¢æˆ·ç»†åˆ†æˆä¸åŒå®¢æˆ·ç¾¤ï¼Œæ¯ä¸ªå®¢æˆ·ç¾¤æœ‰ç›¸ä¼¼è¡Œä¸ºï¼Œåšåˆ°ç²¾å‡†è¥é”€

*   é™ç»´ã€å…³è”è§„åˆ™æŒ–æ˜ç­‰

#### è¿‡åº¦æ‹Ÿåˆé—®é¢˜

æ¨¡å‹è¿‡äºå¤æ‚(ä¾‹å¦‚å‚æ•°è¿‡å¤š)ï¼Œå¯¼è‡´æ‰€é€‰æ¨¡å‹å¯¹å·²çŸ¥æ•°æ®é¢„æµ‹å¾—å¾ˆå¥½ï¼Œä½†å¯¹æœªçŸ¥æ•°æ®é¢„æµ‹å¾ˆå·®ã€‚

#### æ¨¡å‹é€‰æ‹©

*   æ­£åˆ™åŒ–ï¼šåœ¨è¯¯å·®å‡½æ•°ä¸ŠåŠ ä¸€ä¸ªæ­£åˆ™é¡¹ï¼Œæ­£åˆ™é¡¹é€šå¸¸ä¸ºå‚æ•°å‘é‡çš„èŒƒæ•°ã€‚åœ¨è®­ç»ƒè¯¯å·®å’Œæ¨¡å‹å¤æ‚åº¦ä¹‹é—´çš„æƒè¡¡ã€‚ $ \min \limits_{f \in F} \frac{1}{n} \overset{n}{\sum \limits_{i=1}} L(y_i,f(x_i)) + \lambda J(f) $ 
*   äº¤å‰éªŒè¯ï¼šåŸºæœ¬æƒ³æ³•æ˜¯é‡å¤åœ°ä½¿ç”¨æ•°æ®ã€‚å°†æ•°æ®é›†éšæœºåˆ‡åˆ†ï¼Œå°†åˆ‡åˆ†çš„æ•°æ®é›†ç»„åˆä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šåå¤è¿›è¡Œè®­ç»ƒï¼Œæµ‹è¯•å’Œæ¨¡å‹é€‰æ‹©ã€‚
*   KæŠ˜äº¤å‰éªŒè¯ï¼šéšæœºåœ°å°†æ•°æ®åˆ‡åˆ†ä¸ºğ‘˜ä¸ªäº’ä¸ç›¸åŒå¤§å°ç›¸åŒçš„å­é›†ï¼›æ¯æ¬¡åˆ©ç”¨ğ‘˜âˆ’1ä¸ªå­é›†çš„æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œä½™ä¸‹çš„æ•°æ®æµ‹è¯•æ¨¡å‹ï¼›æœ€åé€‰æ‹©åœ¨ğ‘˜æ¬¡æµ‹è¯„ä¸­å¹³å‡æ€§èƒ½æœ€å¥½çš„æ¨¡å‹ã€‚

### æœºå™¨å­¦ä¹ å¸¸ç”¨å·¥å…·

![](image/ç”Ÿæ€åœˆ.png)

NumPy

Pandas

Scikit-learn ï¼šPythonç”¨äºæ•°æ®å»ºæ¨¡çš„ç¬¬ä¸‰æ–¹åº“ã€‚å®ç°ä¸»è¦çš„æœºå™¨å­¦ä¹ ã€æ•°æ®æŒ–æ˜ç®—æ³•ã€‚ä¸»è¦åŠŸèƒ½ï¼šæ•°æ®é›†é¢„å¤„ç†ã€æ•°æ®é›†åˆ’åˆ†ã€æ„å»ºæ¨¡å‹ã€æ¨¡å‹æå‡ã€æ¨¡å‹è¯„ä¼°ã€‚

### Scikit-learnæ¦‚è§ˆ

![](image/sklearnå»ºæ¨¡æµç¨‹.png)

#### Scikit-learn å¸¸ç”¨å‡½æ•°

*   transform å‡½æ•°ï¼šæ•°æ®è½¬æ¢

    *   ```python
        from sklearn import preprocessing
        scaler = preprocessing.StandardScaler().fit(X_train)
        X_train = scaler.transform(X_train)
        X_test = scaler.transform(X_test)
        ```

*   fit å‡½æ•°ï¼šæ¨¡å‹è®­ç»ƒ

    *   ```python
        from sklearn.linear_model import LinearRegression
        lr = LinearRegression()
        lr.fit(X_train, y_train)
        ```

*   predict å‡½æ•°ï¼šæ¨¡å‹é¢„æµ‹

    *   ```python
        y_pred = lr.predict(X_test)
        ```

#### **Scikit-learnä¸»è¦æ¨¡å—**

| **æ¨¡å—**              | **è¯´æ˜**           |
| --------------------- | ------------------ |
| preprocessing         | æ•°æ®é¢„å¤„ç†å’Œæ ‡å‡†åŒ– |
| feature_extraction    | ç‰¹å¾æå–           |
| feature_selection     | ç‰¹å¾é€‰æ‹©           |
| linear_model          | çº¿æ€§æ¨¡å‹           |
| tree                  | åŸºäºæ ‘çš„æ¨¡å‹       |
| cluster               | æ— ç›‘ç£èšç±»ç®—æ³•     |
| discriminant_analysis | çº¿æ€§åˆ¤åˆ«åˆ†æ       |
| ensemble              | é›†æˆæ¨¡å‹           |
| metrics               | æ¨¡å‹è¯„ä»·           |
| model_selection       | æ¨¡å‹é€‰æ‹©ä¸å‚æ•°è°ƒä¼˜ |

#### **Scikit-learnå›å½’æ¨¡å—çš„ä¸»è¦ç®—æ³•**

| **ç±»**                         | **è¯´æ˜**                         |
| ------------------------------ | -------------------------------- |
| linear_model.LinearRegression  | æ™®é€šæœ€å°äºŒä¹˜æ³•                   |
| linear_model.Ridge             | å²­å›å½’                           |
| linear_model.Lasso             | ç”¨L1ä½œä¸ºæ­£åˆ™é¡¹çš„çº¿æ€§æ¨¡å‹         |
| linear_model.ElasticNet        | å°†L1å’ŒL2ç»„åˆä½œä¸ºæ­£åˆ™é¡¹çš„çº¿æ€§å›å½’ |
| linear_model.BayesianRidge     | è´å¶æ–¯å²­å›å½’                     |
| linear_model.TheilSenRegressor | æ³°å°”æ£®å›å½’ï¼šç¨³å¥çš„å¤šå…ƒå›å½’æ¨¡å‹   |
| linear_model.HuberRegressor    | å¯¹å¼‚å¸¸å€¼å…·æœ‰é²æ£’æ€§çš„çº¿æ€§å›å½’æ¨¡å‹ |
| linear_model.RANSACRegressor   | éšæœºé‡‡æ ·ä¸€è‡´æ€§ç®—æ³•               |

#### **Scikit-learnåˆ†ç±»æ¨¡å—çš„ä¸»è¦ç®—æ³•**

| **ç±»**                          | **è¯´æ˜**         |
| ------------------------------- | ---------------- |
| linear_model.LogisticRegression | é€»è¾‘å›å½’         |
| neighbors.KNeighborsClassifier  | Kè¿‘é‚»            |
| tree.DecisionTreeClassifier     | å†³ç­–æ ‘ç”¨äºåˆ†ç±»   |
| naive_bayes.BernoulliNB         | Bernoulli è´å¶æ–¯ |
| naive_bayes.GaussianNB          | Gaussianè´å¶æ–¯   |
| naive_bayes.MultinomialNB       | å¤šé¡¹å¼è´å¶æ–¯     |
| svm.LinearSVC                   | çº¿æ€§æ”¯æŒå‘é‡åˆ†ç±» |

#### **Scikit-learnèšç±»æ¨¡å—çš„ä¸»è¦ç®—æ³•**

| **ç±»**                          | **è¯´æ˜**                             |
| ------------------------------- | ------------------------------------ |
| cluster.KMeans                  | K-Meansèšç±»                          |
| cluster.AgglomerativeClustering | å±‚æ¬¡èšç±»                             |
| cluster.DBSCAN                  | åŸºäºå¯†åº¦çš„å™ªå£°åº”ç”¨ç©ºé—´èšç±»ï¼ˆDBSCANï¼‰ |
| cluster.MeanShift               | å¹³å‡ç§»ä½èšç±»                         |
| cluster.SpectralClustering      | è°±èšç±»                               |

#### **Scikit-learné™ç»´æ¨¡å—çš„ä¸»è¦ç®—æ³•**

| **ç±»**                                           | **è¯´æ˜**                                          |
| ------------------------------------------------ | ------------------------------------------------- |
| decomposition.PCA                                | ä¸»æˆåˆ†åˆ†æ (PCA)                                  |
| decomposition.LatentDirichletAllocation          | LDA                                               |
| discriminant_analysis.LinearDiscriminantAnalysis | çº¿æ€§åˆ¤åˆ«å¼åˆ†æ(Linear Discriminant Analysis, LDA) |
| decomposition.FastICAï¼Œdecomposition.fastica     | ç‹¬ç«‹æˆåˆ†åˆ†æ                                      |
| decomposition.NMF                                | éè´ŸçŸ©é˜µåˆ†è§£ (NMF)                                |
| manifold.LocallyLinearEmbedding                  | å±€éƒ¨çº¿æ€§æ˜ å°„                                      |
| manifold.MDS                                     | å¤šç»´ç¼©æ”¾                                          |

### æ¡ˆä¾‹ï¼šä½¿ç”¨çº¿æ€§å›å½’æ„å»ºæˆ¿ä»·é¢„æµ‹æ¨¡å‹

åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼šï¼ˆåªçœ‹ä»£ç ï¼Œæ‰€æœ‰æ²¡æœ‰æ·»åŠ æ•°æ®æ–¹é¢çš„ä¿¡æ¯ï¼‰

1.  å¯¼å…¥å·¥å…·åŒ…
2.  åŠ è½½æ•°æ®
3.  è®­ç»ƒé›†-æµ‹è¯•é›†åˆ’åˆ†
4.  æ¨¡å‹æ„å»º
5.  æ¨¡å‹é¢„æµ‹ä¸è¯„ä»·

```python
import pandas as pd 

# ä»sklearnçš„æ¨¡å‹é€‰æ‹©æ¨¡å—å¯¼å…¥è®­ç»ƒé›†-æµ‹è¯•é›†åˆ’åˆ†ç±»
from sklearn.model_selection import train_test_split
# ä»sklearnçš„çº¿æ€§æ¨¡å‹æ¨¡å—å¯¼å…¥çº¿æ€§å›å½’ç±»
from sklearn.linear_model import LinearRegression
# ä»sklearnçš„æ¨¡å‹è¯„ä»·æ¨¡å—å¯¼å…¥å†³å®šç³»æ•°
from sklearn.metrics import r2_score
# ä»sklearnçš„æ¨¡å‹è¯„ä»·æ¨¡å—å¯¼å…¥å‡æ–¹è¯¯å·®
from sklearn.metrics import mean_squared_error

data = pd.read_csv('BostonHousingData.csv') # è¯»å–csv
y = data['target'] # æ•°æ®æ ‡ç­¾ï¼Œä¸€èˆ¬ä¸ºæ•°æ®é›†çš„æœ€åä¸€åˆ—ï¼Œæœ‰äº‹ä¹Ÿå¯èƒ½æ˜¯ç¬¬ä¸€åˆ—ï¼Œå› æ•°æ®é›†è€Œå¼‚
X = data.copy().drop(['target'], axis=1) # å»æ‰æ ‡ç­¾åˆ—çš„å®Œæ•´æ•°æ®

# X_trainä¸ºè®­ç»ƒæ•°æ®ï¼Œ y_trainä¸ºè®­ç»ƒé›†æ ‡ç­¾ï¼ŒX_testä¸ºæµ‹è¯•æ•°æ®ï¼Œy_testä¸ºæµ‹è¯•é›†æ ‡ç­¾ã€‚ä¸€èˆ¬70%è®­ç»ƒï¼Œ30%æµ‹è¯•ã€‚
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

lr = LinearRegression() # åˆ›å»ºä¸€ä¸ªçº¿æ€§å›å½’å¯¹è±¡
# LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=None)
lr.fit(X_train, y_train) # æ‹Ÿåˆ

y_pred = lr.predict(X_test) # æ¨¡å‹é¢„æµ‹
print(r2_score(y_test, y_pred)) # æ¨¡å‹è¯„ä»·, å†³å®šç³»æ•°
print(mean_squared_error(y_test, y_pred)) # å‡æ–¹è¯¯å·®
''' è¿è¡Œç»“æœ
0.7561429458476269 
20.50487238687095 
'''

```

## æ•°æ®é¢„å¤„ç†

### æ•°æ®é¢„å¤„ç†ä¸é¢„å¤„ç†æ¨¡å—

æ¨¡å‹è¾“å…¥æ•°æ®è´¨é‡ç›´æ¥å½±å“å»ºæ¨¡æ•ˆæœã€‚åœ¨æ­£å¼æ„å»ºæ¨¡å‹ä¹‹å‰å¾€å¾€éœ€è¦å¯¹æ•°æ®è¿›è¡Œæ°å½“çš„é¢„å¤„ç†ã€‚

å¸¸ç”¨çš„æ•°æ®é¢„å¤„ç†æ–¹æ³•ï¼š

*   ç¼ºå¤±å€¼å¤„ç†ï¼šçœŸå®çš„æ•°æ®å¾€å¾€å› ä¸ºå„ç§åŸå› å­˜åœ¨ç¼ºå¤±å€¼ï¼Œéœ€è¦ç”¨åˆ é™¤æ³•æˆ–å¡«è¡¥æ³•æ¥å¾—åˆ°ä¸€ä¸ªå®Œæ•´çš„æ•°æ®å­é›†ã€‚
*   ç¦»ç¾¤å€¼æ£€æµ‹å’Œå¤„ç†ï¼šæ£€æµ‹æ•°æ®é›†ä¸­é‚£äº›æ˜æ˜¾åç¦»æ•°æ®é›†ä¸­çš„å…¶ä»–æ ·æœ¬ï¼Œä¸ºæ•°æ®åˆ†ææä¾›é«˜è´¨é‡çš„æ•°æ®ã€‚
*   æ ‡å‡†åŒ–ï¼šæ•°æ®åˆ†æåŠå»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•éœ€è¦å…¶è¾“å…¥ç‰¹å¾ä¸ºæ ‡å‡†åŒ–å½¢å¼ï¼›è‹¥æ ·æœ¬çš„ç‰¹å¾ä¹‹é—´çš„é‡çº²å·®å¼‚å¤ªå¤§ï¼Œæ ·æœ¬ä¹‹é—´ç›¸ä¼¼åº¦è¯„ä¼°ç»“æœå°†å­˜åœ¨åå·®ã€‚
*   ç‰¹å¾ç¼–ç ï¼šæ¨¡å‹è¾“å…¥çš„ç‰¹å¾é€šå¸¸éœ€è¦æ˜¯æ•°å€¼å‹çš„ï¼Œæ‰€ä»¥éœ€è¦å°†éæ•°å€¼å‹ç‰¹å¾è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾ã€‚
*   ç¦»æ•£åŒ–ï¼šåœ¨æ•°æ®ä¿¡æ¯æŸå¤±å°½é‡å°‘çš„å‰æä¸‹ï¼Œå°½å¯èƒ½å‡å°‘å…ƒæ•°ã€‚

å¸¸ç”¨çš„sklearnå’Œpandasé¢„å¤„ç†æ¨¡å—ï¼š

| å¤„ç†æ–¹æ³•         | å¯¹åº”çš„sklearnå’Œpandasçš„ç±»                                    |
| ---------------- | ------------------------------------------------------------ |
| ç¼ºå¤±å€¼å¤„ç†       | sklearn.preprocessing.Imputer                                |
| ç¦»ç¾¤å€¼æ£€æµ‹å’Œå¤„ç† | sklearn.neighbors.LocalOutlierFactor                         |
| æ ‡å‡†åŒ–           | sklearn.preprocessing.StandardScaler, sklearn.preprocessing.MinMaxScaler, sklearn.preprocessing.RobustScaler |
| ç‰¹å¾ç¼–ç          | sklearn.preprocessing.OneHotEncoder, pandas.get_dummiesï¼Œsklearn.preprocessing.LabelEncoder |
| ç¦»æ•£åŒ–           | pandas.cut, pandas.qcut, sklearn.preprocessing.Binarizer     |

Ref: 

*   https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing
*   http://pandas.pydata.org/pandas-docs/stable/api.html

### ç¼ºå¤±å€¼å¤„ç†

#### åˆ é™¤æ³•

åˆ é™¤æ³•é€šè¿‡åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®ï¼Œæ¥å¾—åˆ°ä¸€ä¸ªå®Œæ•´çš„æ•°æ®å­é›†ã€‚ æ•°æ®çš„åˆ é™¤æ—¢å¯ä»¥ä»æ ·æœ¬çš„è§’åº¦è¿›è¡Œï¼Œä¹Ÿå¯ä»¥ä»ç‰¹å¾çš„è§’åº¦è¿›è¡Œã€‚

*   åˆ é™¤ç‰¹å¾ï¼šå½“æŸä¸ªç‰¹å¾ç¼ºå¤±å€¼è¾ƒå¤šï¼Œä¸”è¯¥ç‰¹å¾å¯¹æ•°æ®åˆ†æçš„ç›®æ ‡å½±å“ä¸å¤§æ—¶ï¼Œ å¯ä»¥å°†è¯¥ç‰¹å¾åˆ é™¤ã€‚
*   åˆ é™¤æ ·æœ¬ï¼šåˆ é™¤å­˜åœ¨æ•°æ®ç¼ºå¤±çš„æ ·æœ¬ã€‚è¯¥æ–¹æ³•é€‚åˆæŸäº›æ ·æœ¬æœ‰å¤šä¸ªç‰¹å¾å­˜åœ¨ç¼ºå¤±å€¼ï¼Œä¸”å­˜åœ¨ç¼ºå¤±å€¼çš„æ ·æœ¬å æ•´ä¸ªæ•°æ®é›†æ ·æœ¬æ•°é‡çš„æ¯”ä¾‹ä¸é«˜çš„æƒ…å½¢ã€‚
*   ç¼ºç‚¹ï¼š
    å®ƒä»¥å‡å°‘æ•°æ®æ¥æ¢å–ä¿¡æ¯çš„å®Œæ•´ï¼Œä¸¢å¤±äº†å¤§é‡éšè—åœ¨è¿™äº›è¢«åˆ é™¤æ•°æ®ä¸­çš„ä¿¡æ¯ï¼›åœ¨ä¸€äº›å®é™…åœºæ™¯ä¸‹ï¼Œæ•°æ®çš„é‡‡é›†æˆæœ¬é«˜ä¸”ç¼ºå¤±å€¼æ— æ³•é¿å…ï¼Œåˆ é™¤æ³•å¯èƒ½ä¼šé€ æˆå¤§é‡çš„èµ„æºæµªè´¹ã€‚







#### å¡«è¡¥æ³•

è®¡ç®—è¯¥ç‰¹å¾ä¸­éç¼ºå¤±å€¼çš„å¹³å‡å€¼ï¼Œä¸­ä½æ•°æˆ–ä¼—æ•°ï¼Œç„¶åä½¿ç”¨å¹³å‡å€¼,ä¸­ä½æ•°æˆ–ä¼—æ•°æ¥ä»£æ›¿ç¼ºå¤±å€¼ã€‚

*   å¹³å‡å€¼ï¼ˆæ•°å€¼å‹ç‰¹å¾ï¼‰
*   ä¼—æ•°ï¼ˆéæ•°å€¼å‹ç‰¹å¾ï¼‰
*   ä¸­ä½æ•°

åœ¨å®é™…åº”ç”¨è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥æ ¹æ®ä¸€å®šçš„è¾…åŠ©ç‰¹å¾ï¼Œå°†æ•°æ®é›†åˆ†æˆå¤šç»„ï¼Œç„¶ååœ¨æ¯ä¸€ç»„æ•°æ®ä¸Šåˆ†åˆ«ä½¿ç”¨å‡å€¼æ’è¡¥. ä¾‹å¦‚å­¦ç”Ÿä¿¡æ¯æ•°æ®é›†ï¼Œ``å…¥å­¦å¹´ä»½''ç‰¹å¾è®°å½•äº†å­¦ç”Ÿå…¥å­¦çš„å¹´ä»½ï¼Œå¯ä»¥é¦–å…ˆæ ¹æ®æ¯•ä¸šå¹´ä»½å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ï¼Œç„¶åä½¿ç”¨æ¯ä¸€ä¸ªåˆ†ç»„æ•°æ®ä¸­``å¹´é¾„''ç‰¹å¾çš„å¹³å‡å€¼æ¥å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¡«è¡¥ã€‚

*   ç¼ºç‚¹ä¸€ï¼šå‡å€¼å¡«è¡¥æ³•ä¼šä½¿å¾—æ•°æ®è¿‡åˆ†é›†ä¸­åœ¨å¹³å‡å€¼æˆ–ä¼—æ•°ä¸Šï¼Œå¯¼è‡´ç‰¹å¾çš„æ–¹å·®è¢«ä½ä¼°
*   ç¼ºç‚¹äºŒï¼šç”±äºå®Œå…¨å¿½ç•¥ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå‡å€¼å¡«è¡¥æ³•ä¼šå¤§å¤§å¼±åŒ–ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§

å…¶ä»–å¡«è¡¥æ–¹æ³•ï¼š

*   éšæœºå¡«è¡¥
    éšæœºå¡«è¡¥æ˜¯åœ¨å‡å€¼å¡«è¡¥çš„åŸºç¡€ä¸ŠåŠ ä¸Šéšæœºé¡¹ï¼Œé€šè¿‡å¢åŠ ç¼ºå¤±å€¼çš„éšæœºæ€§æ¥æ”¹å–„ç¼ºå¤±å€¼åˆ†å¸ƒè¿‡äºé›†ä¸­çš„ç¼ºé™·
*   åŸºäºæ¨¡å‹çš„å¡«è¡¥
    å°†ç¼ºå¤±ç‰¹å¾ y å½“ä½œé¢„æµ‹ç›®æ ‡ï¼›ä½¿ç”¨å…¶ä½™ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨ç¼ºå¤±ç‰¹å¾éç¼ºå¤±æ ·æœ¬æ„
*   å»ºåˆ†ç±»æˆ–å›å½’æ¨¡å‹
*   å“‘å˜é‡æ–¹æ³•
    å¯¹äºç¦»æ•£å‹ç‰¹å¾ï¼Œå°†ç¼ºå¤±å€¼ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„å–å€¼è¿›è¡Œå¤„ç†
*   EMç®—æ³•å¡«è¡¥

#### Pandasç¼ºå¤±å€¼å¤„ç†

ç¼ºå¤±æ•°æ®é€šå¸¸ç”¨NaNï¼ˆnot a numberï¼‰è¡¨ç¤ºï¼Œå®ƒæ˜¯ä¸€ä¸ªå¯ä»¥è¢«æ£€æµ‹å‡ºæ¥çš„æ ‡è®°ã€‚é»˜è®¤åœ¨æ‰€æœ‰çš„ç»Ÿè®¡æè¿°ä¸­éƒ½æ’é™¤ç¼ºå¤±æ•°æ®ã€‚

| **æ–¹æ³•**  | **æè¿°**                                                     |
| --------- | ------------------------------------------------------------ |
| dropna()  | æ ¹æ®å„æ ‡ç­¾çš„å€¼ä¸­æ˜¯å¦å­˜åœ¨ç¼ºå¤±æ•°æ®å¯¹è½´æ ‡ç­¾è¿›è¡Œè¿‡æ»¤ï¼Œå¯é€šè¿‡é˜ˆå€¼è°ƒèŠ‚å¯¹ç¼ºå¤±å€¼çš„å®¹å¿åº¦ |
| fillna()  | ç”¨æŒ‡å®šå€¼æˆ–æ’å€¼æ–¹æ³•å¡«å……ç¼ºå¤±æ•°æ®                               |
| isnull()  | è¿”å›ä¸€ä¸ªå«æœ‰å¸ƒå°”å€¼çš„å¯¹è±¡ï¼Œè¿™äº›å¸ƒå°”å€¼è¡¨ç¤ºå“ªäº›å€¼æ˜¯ç¼ºå¤±å€¼NAï¼Œè¯¥å¯¹è±¡çš„ç±»å‹ä¸æºç±»å‹ä¸€æ · |
| notnull() | isnullçš„å¦å®šå¼                                               |



#### Scikit-learnç¼ºå¤±å€¼å¤„ç†

Scikit-learnä¸­ç¼ºå¤±å€¼å¡«è¡¥å‡½æ•°ï¼š

```python
import sklearn
sklearn.preprocessing.Imputer(
    missing_values=â€™NaNâ€™, # ç¼ºå¤±å€¼çš„å ä½ç¬¦
    strategy=â€™meanâ€™, # å¡«è¡¥ç­–ç•¥
    axis=0, # 0ä¸ºæŒ‰åˆ—ï¼Œ1ä¸ºæŒ‰è¡Œ
    verbose=0, # æ§åˆ¶imputerçš„è¯¦ç»†ç¨‹åº¦ 
    copy=True # Trueï¼Œå°†åˆ›å»ºXçš„å‰¯æœ¬ï¼›Falseï¼Œå¡«è¡¥å°†åœ¨Xä¸Šè¿›è¡Œ
)
# strategy:
# å¦‚æœæ˜¯â€œmeanâ€ï¼Œåˆ™ä½¿ç”¨æ²¿è½´çš„å¹³å‡å€¼æ›¿æ¢ç¼ºå¤±å€¼ã€‚
# å¦‚æœæ˜¯â€œmedianâ€ï¼Œåˆ™ä½¿ç”¨æ²¿è½´çš„ä¸­ä½æ•°æ›¿æ¢ç¼ºå¤±å€¼ã€‚
# å¦‚æœæ˜¯â€œmost_frequentâ€ï¼Œåˆ™ä½¿ç”¨æ²¿è½´çš„æœ€é¢‘ç¹å€¼æ›¿æ¢ç¼ºå¤±ã€‚

# Imputeråœ¨0.22ä¸­å·²ç»ä»preprocessingæ¨¡å—åˆ é™¤,ç¼ºå¤±å€¼å¡«è¡¥è½¬ç§»è‡³ Imputeæ¨¡å—
```

##### Imputer

| **æ–¹æ³•**               | **åŠŸèƒ½**                |
| ---------------------- | ----------------------- |
| .fit(X[, y])           | æ‹Ÿåˆç”¨äºXçš„å¡«è¡¥å‚æ•°ã€‚   |
| .fit_transform(X[, y]) | æ‹Ÿåˆæ•°æ®ï¼Œç„¶åè½¬æ¢ã€‚    |
| .transform(X)          | åœ¨Xä¸­è®¡ç®—æ‰€æœ‰ç¼ºå¤±çš„å€¼ã€‚ |



```python
# ç¤ºä¾‹ï¼šç”¨å‡å€¼å¯¹å¹´é¾„è¿›è¡Œå¡«è¡¥
from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
imp.fit(teenager_sns[["age"]])
teenager_sns["age_imputed"]=imp.transform(teenager_sns[["age"]])
# æ˜¾ç¤ºå¹´é¾„ç¼ºå¤±çš„è¡Œï¼Œå’Œæ’è¡¥åçš„åˆ—"age_imputed"
teenager_sns[teenager_sns['age'].isnull()]
```



### ç¦»ç¾¤å€¼æ£€æµ‹

ç¦»ç¾¤å€¼æŒ‡ä¸€ä¸ªæ•°æ®é›†ä¸­é‚£äº›æ˜æ˜¾åç¦»æ•°æ®é›†ä¸­çš„å…¶ä»–æ ·æœ¬ã€‚

ä¸€ç§å¸¦æœ‰ç»Ÿè®¡å­¦å‘³é“çš„å®šä¹‰æ˜¯ï¼šä¸€ä¸ªè§‚æµ‹ä¸å…¶ä»–è§‚æµ‹åç¦»å¤ªå¤šä»¥è‡´äºå€¼å¾—æ€€ç–‘å®ƒæ˜¯ç”±ä¸åŒçš„æœºåˆ¶æ‰€äº§ç”Ÿçš„ã€‚

äº§ç”ŸåŸå› ï¼šè‡ªç„¶å˜å¼‚ã€æ•°æ®æµ‹é‡å’Œæ”¶é›†çš„è¯¯å·®ä»¥åŠäººå·¥æ“ä½œå¤±è¯¯ç­‰ã€‚

ç¦»ç¾¤å€¼æ£€æµ‹å¯ä»¥ä½œä¸ºæ•°æ®é¢„å¤„ç†çš„ä¸€ä¸ªæ­¥éª¤ï¼Œä¸ºæ•°æ®åˆ†ææä¾›é«˜è´¨é‡çš„æ•°æ®ã€‚

ç¦»ç¾¤å€¼æ£€æµ‹ä¹Ÿå¯ä»¥ç›´æ¥ç”¨æ¥è§£å†³å¾ˆå¤šåº”ç”¨é—®é¢˜ï¼šä¿¡ç”¨æ¬ºè¯ˆæ£€æµ‹ã€ç”µä¿¡æ¬ºè¯ˆæ£€æµ‹ã€ç–¾ç—…åˆ†æã€è®¡ç®—æœºå®‰å…¨è¯Šæ–­ç­‰ã€‚

ä½†ç¦»ç¾¤å€¼æœªå¿…å°±æ˜¯â€œå¼‚å¸¸å€¼â€ã€‚å¤®è¡Œçš„ä¾‹å­ï¼šåˆæ­¥çš„æ•°æ®åˆ†æå‘ç°å­˜åœ¨æˆ¿è´·çš„é¢åº¦ä½äº1000çš„æƒ…å½¢ï¼Œç–‘ä¼¼æ•°æ®å¼‚å¸¸ã€‚ç»è¿‡çœŸå®éªŒè¯ï¼Œå‘ç°ç¡®å®å­˜åœ¨è¿™ç§æƒ…å†µï¼Œå†œæ‘è´·æ¬¾æ—¶å€™æŠŠä¿®æˆ¿æŠ¥é€ä¸ºæˆ¿è´·ã€‚

#### åŸºäºç»Ÿè®¡

åœ¨ä¸Šã€ä¸‹Î±åˆ†ä½ç‚¹ä¹‹å¤–çš„å€¼è®¤ä¸ºæ˜¯å¼‚å¸¸å€¼ã€‚ç›’å›¾è§‚å¯Ÿ

![](image/liqun1.png)

![](image/liqun2.png)

åˆ†ä½æ•°ï¼š

*   Kåˆ†ä½æ•°ï¼šä»¤xæ˜¯ä¸€ä¸ªå€¼ï¼Œå¦‚æœåœ¨æ•°æ®é›†ä¸­ï¼Œç™¾åˆ†ä¹‹Kçš„æ•°æ®çš„å€¼éƒ½ä¸å¤§äºxï¼Œåˆ™ç§°xä¸ºè¯¥æ•°æ®é›†çš„Kåˆ†ä½æ•°ã€‚
*   åˆ†ä½æ•°ï¼šQ1ï¼ˆ25th percentileï¼‰ï¼ŒQ3ï¼ˆ75th percentileï¼‰
*   ä¸­é—´åˆ†ä½æ•°èŒƒå›´ï¼šIQR = Q3 - Q1
*   æ–¹å·®ï¼ˆVarianceï¼‰å’Œæ ‡å‡†å·®ï¼ˆStandard deviationï¼‰
*   æ–¹å·® ï¼š$ \sigma^2 = \frac{1}{n-1} \overset{n}{\sum \limits_{i=1}} (x_i-\overline{x})^2 $ 

ç›’å›¾

![](image/hetu.png)

#### åŸºäºè¿‘é‚»

å±€éƒ¨å¼‚å¸¸å› å­ç®—æ³•ï¼ˆLOFç®—æ³•ï¼ŒLocal Outlier Factorï¼‰

åŸºæœ¬æƒ³æ³•ï¼š
é€šè¿‡æ¯”è¾ƒæ¯ä¸ªç‚¹ğ‘å’Œå…¶é‚»åŸŸç‚¹çš„å¯†åº¦æ¥åˆ¤æ–­è¯¥ç‚¹æ˜¯å¦ä¸ºå¼‚å¸¸ç‚¹ï¼Œç‚¹ğ‘çš„å¯†åº¦è¶Šä½ï¼Œè¶Šå¯èƒ½è¢«è®¤å®šæ˜¯å¼‚å¸¸ç‚¹
å¯†åº¦é€šè¿‡ç‚¹ä¹‹é—´çš„è·ç¦»æ¥è®¡ç®—ï¼Œç‚¹ä¹‹é—´è·ç¦»è¶Šè¿œï¼Œå¯†åº¦è¶Šä½ï¼Œè·ç¦»è¶Šè¿‘ï¼Œå¯†åº¦è¶Šé«˜



ç›¸å…³å®šä¹‰

*   d(A,B)ï¼šç‚¹Aä¸ç‚¹Bä¹‹é—´çš„è·ç¦»
*   d_k(A)ï¼šç‚¹Açš„ç¬¬ k è·ç¦»ï¼Œå³è·ç¦»ç‚¹Aç¬¬kè¿œçš„ç‚¹çš„è·ç¦»ï¼Œä¸åŒ…æ‹¬ç‚¹A 
*   N_k(A)ï¼šç‚¹Açš„ç¬¬ k è·ç¦»é‚»åŸŸï¼Œå³Açš„ç¬¬ k è·ç¦»ä»¥å†…çš„æ‰€æœ‰ç‚¹ï¼ŒåŒ…æ‹¬ç¬¬ k è·ç¦»çš„ç‚¹
*   rd_k(B,A)ï¼šç‚¹Aåˆ°ç‚¹Bçš„ç¬¬ k å¯è¾¾è·ç¦»ï¼Œè®¡ç®—å…¬å¼ä¸º : rd_k(B,A)=max{d_k(A),â…†(A,B)}

ç¦»ç¾¤å€¼æ£€æµ‹ï¼š

![](image/liqunjiance.png)

![](image/liqunjiancefangfa.png)



#### sklearn ç¦»ç¾¤å€¼æ£€æµ‹

```python
import sklearn
sklearn.neighbors.LocalOutlierFactor(
    n_neighbors=20, # ç”¨äºkneighborsæŸ¥è¯¢çš„é‚»åŸŸæ•°
    algorithm='auto', # {â€˜autoâ€™, â€˜ball_treeâ€™, â€˜kd_treeâ€™, â€˜bruteâ€™}, å¯é€‰ 
    leaf_size=30, # ä¼ é€’ç»™BallTreeæˆ–KDTreeçš„å¶å­å¤§å°
    metric='minkowski', # ç”¨äºè·ç¦»è®¡ç®—çš„åº¦é‡
    p=2, # p = 1æ—¶ï¼Œmanhattan_distanceï¼ˆl1ï¼‰ï¼›p = 2ï¼Œeuclidean_distanceï¼ˆl2ï¼‰
    metric_params=None, # åº¦é‡å‡½æ•°çš„å…¶ä»–å…³é”®å­—å‚æ•°
    contamination=0.1, # æ•°æ®é›†ä¸­å¼‚å¸¸å€¼çš„æ¯”ä¾‹
    n_jobs=1 # ä¸ºé‚»åŸŸæœç´¢è¿è¡Œçš„å¹¶è¡Œä½œä¸šæ•°
)
```



##### LocalOutlierFactor

| **æ–¹æ³•**                                       | **åŠŸèƒ½**                        |
| ---------------------------------------------- | ------------------------------- |
| .fit(X[, y])                                   | ä½¿ç”¨Xä½œä¸ºè®­ç»ƒæ•°æ®æ‹Ÿåˆæ¨¡å‹ã€‚     |
| .fit_predict(X[, y])                           | ä½¿æ¨¡å‹é€‚åˆè®­ç»ƒé›†Xå¹¶è¿”å›æ ‡ç­¾ã€‚   |
| .kneighbors([X, n_neighbors, return_distance]) | æ‰¾åˆ°ä¸€ä¸ªç‚¹çš„Ké‚»åŸŸã€‚             |
| .kneighbors_graph([X, n_neighbors, mode])      | è®¡ç®—Xä¸­ç‚¹çš„k-é‚»åŸŸçš„ï¼ˆåŠ æƒï¼‰å›¾ã€‚ |



```python
# ç¤ºä¾‹ï¼šå¯¹è¡Œé©¶é‡Œç¨‹å’Œä»·æ ¼è¿›è¡Œåˆ©ç¾¤æ£€æµ‹
import pandas as pd
from sklearn.neighbors import LocalOutlierFactor
test = pd.read_csv('accord_sedan_testing.csv')
data = test[['mileage','price']]
print(data.describe())
scaler = LocalOutlierFactor()
scaler.fit(data)
data['LOF'] = - scaler.negative_outlier_factor_
data[data.LOF>1.5] # æ˜¾ç¤ºå±€éƒ¨ç¦»ç¾¤å› å­å¤§äº1.5çš„æ ·æœ¬
```

#### ç¦»ç¾¤å€¼å¤„ç†

ç¦»ç¾¤å€¼çš„å¤„ç†æ–¹æ³•ä¸»è¦æ˜¯è¦çœ‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½æ˜¯å¦æœ‰æå‡ã€‚

æ­£å¸¸æƒ…å†µä¸‹å¦‚æœçš„ç¡®æ˜¯ç¦»ç¾¤å€¼ï¼Œåœ¨æœ‰ç¦»ç¾¤å€¼çš„ç‰¹å¾è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œå»æ‰ååœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ€§èƒ½æ˜¯ä¼šæœ‰æ˜¾è‘—æå‡çš„ã€‚

åœ¨å¯¹æ•°æ®åˆ†å¸ƒå½±å“è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥æŠŠç¦»ç¾¤å€¼å½“æˆç¼ºå¤±å€¼ï¼Œæˆ–ç”¨å‡å€¼æ›¿æ¢ã€‚

### æ ‡å‡†åŒ–

ä¸ºä»€ä¹ˆè¦è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–ï¼Ÿ

æ•°æ®åˆ†æåŠå»ºæ¨¡è¿‡ç¨‹ä¸­ï¼Œè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•éœ€è¦å…¶è¾“å…¥ç‰¹å¾ä¸ºæ ‡å‡†åŒ–å½¢å¼ã€‚ä¾‹å¦‚ï¼ŒSVMç®—æ³•ä¸­çš„RBFæ ¸å‡½æ•°ï¼Œçº¿æ€§æ¨¡å‹ä¸­çš„l1ã€l2æ­£åˆ™é¡¹ï¼Œç›®æ ‡å‡½æ•°å¾€å¾€å‡è®¾å…¶ç‰¹å¾å‡å€¼åœ¨0é™„è¿‘ä¸”æ–¹å·®é½æ¬¡ï¼›è‹¥æ ·æœ¬çš„ç‰¹å¾ä¹‹é—´çš„é‡çº²å·®å¼‚å¤ªå¤§ï¼Œæ ·æœ¬ä¹‹é—´ç›¸ä¼¼åº¦è¯„ä¼°ç»“æœå°†å­˜åœ¨åå·®ã€‚

å¸¸è§æ•°æ®æ ‡å‡†åŒ–æ–¹æ³•ï¼šZ-Scoreæ ‡å‡†åŒ–ã€Min-Maxæ ‡å‡†åŒ–ã€‚

![](image/z-score.png)

```python
# Scikit-learnä¸­Z-Scoreæ ‡å‡†åŒ–å‡½æ•°ç”¨æ³•ï¼š
import sklearn
sklearn.preprocessing.StandardScaler(
    copy=True, # å¦‚æœä¸ºFalseï¼Œå°è¯•é¿å…å¤åˆ¶å¹¶æ”¹ä¸ºç›´æ¥æ›¿æ¢
    with_mean=True, # å¦‚æœä¸ºTrueï¼Œåˆ™åœ¨ç¼©æ”¾ä¹‹å‰å°†æ•°æ®å±…ä¸­
    with_std=True # å¦‚æœä¸ºTrueï¼Œåˆ™å°†æ•°æ®ç¼©æ”¾ä¸ºå•ä½æ–¹å·®ï¼ˆæˆ–å•ä½æ ‡å‡†å·®ï¼‰
)
# å®ç°ç›¸åŒåŠŸèƒ½çš„å‡½æ•°scale
```

##### StandardScaler

| æ–¹æ³•                          | åŠŸèƒ½                                   |
| ----------------------------- | -------------------------------------- |
| .fit(X[, y])                  | è®¡ç®—å‡å€¼å’Œæ–¹å·®ç”¨äºåç»­æ ‡å‡†åŒ–ã€‚         |
| .fit_transform(X[, y])        | æ‹Ÿåˆæ•°æ®ï¼Œç„¶åè½¬æ¢ã€‚                   |
| .inverse_transform(X[, copy]) | å°†æ•°æ®ç¼©å‡ä¸ºåŸå§‹è¡¨ç¤ºã€‚                 |
| .partial_fit(X[, y])          | åœ¨çº¿è®¡ç®—Xä¸Šçš„meanå’Œstdä»¥ä¾¿åç»­æ ‡å‡†åŒ–ã€‚ |
| .transform(X[, y, copy])      | é€šè¿‡å±…ä¸­å’Œç¼©æ”¾æ‰§è¡Œæ ‡å‡†åŒ–ã€‚             |



```python
# ç¤ºä¾‹ï¼šå¯¹æœ‹å‹æ•°é‡è¿›è¡ŒZ-Scoreæ ‡å‡†åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler(copy=True)
teenager_sns_zscore = pd.DataFrame(scaler.fit_transform(teenager_sns[["friends"]]),columns =["friends_StandardScaled"] )
teenager_sns_zscore["friends"] = teenager_sns["friends"]
print("å‡å€¼ï¼š",teenager_sns_zscore["friends_StandardScaled"].mean(axis=0))
print("æ–¹å·®ï¼š",teenager_sns_zscore["friends_StandardScaled"].std(axis=0))
teenager_sns_zscore.head(5)

```



![](image/min-max.png)

```python
# Scikit-learnä¸­Min-Maxæ ‡å‡†åŒ–å‡½æ•°ç”¨æ³•ï¼š
import sklearn 
sklearn.preprocessing.MinMaxScaler(
    feature_range=(0, 1), #æœŸæœ›çš„è½¬æ¢æ•°æ®èŒƒå›´
    copy=True #è®¾ç½®ä¸ºFalseä»¥æ‰§è¡Œå°±åœ°æ ‡å‡†åŒ–å¹¶é¿å…å¤åˆ¶
)
# å®ç°ç›¸åŒåŠŸèƒ½çš„å‡½æ•°è¿˜æœ‰minmax_scale
```

##### MinMaxScaler

| **æ–¹æ³•**                      | **åŠŸèƒ½**                                    |
| ----------------------------- | ------------------------------------------- |
| .fit(X[, y])                  | è®¡ç®—æœ€å¤§å€¼å’Œæœ€å°å€¼ç”¨äºåç»­æ ‡å‡†åŒ–ã€‚          |
| .fit_transform(X[, y])        | æ‹Ÿåˆæ•°æ®ï¼Œç„¶åè½¬æ¢ã€‚                        |
| .inverse_transform(X[, copy]) | å°†æ•°æ®é‡ç½®ä¸ºåŸå§‹è¡¨ç¤ºã€‚                      |
| .partial_fit(X[, y])          | åœ¨çº¿è®¡ç®—Xä¸Šçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ä»¥ä¾¿åç»­æ ‡å‡†åŒ–ã€‚ |
| .transform(X[, y, copy])      | é€šè¿‡å±…ä¸­å’Œç¼©æ”¾æ‰§è¡Œæ ‡å‡†åŒ–ã€‚                  |

```python
# ç¤ºä¾‹ï¼šå¯¹æœ‹å‹æ•°é‡è¿›è¡ŒMin-Maxæ ‡å‡†åŒ–
from sklearn.preprocessing import MinMaxScaler
filtered_columns = ["friends"]
scaler = MinMaxScaler(copy=False)
teenager_sns_minmaxscore = pd.DataFrame(scaler.fit_transform(teenager_sns[["friends"]]),
columns = ["friends_MinMaxScaled"])
teenager_sns_minmaxscore["friends"] = teenager_sns["friends"]
teenager_sns_minmaxscore.head(5)
```



```python
# è€ƒè™‘ç¦»ç¾¤å€¼çš„æ ‡å‡†åŒ–
import sklearn
sklearn.preprocessing.RobustScaler(
    with_centering=True, # å¦‚æœä¸ºTrueï¼Œåˆ™åœ¨æ ‡å‡†åŒ–ä¹‹å‰å°†æ•°æ®å±…ä¸­
    with_scaling=True, # å¦‚æœä¸ºTrueï¼Œåˆ™å°†æ•°æ®ç¼©æ”¾åˆ°åˆ†ä½æ•°èŒƒå›´
    quantile_range=(25.0, 75.0), # ç”¨äºè®¡ç®—scale_çš„åˆ†ä½æ•°èŒƒå›´
    copy=True # å¦‚æœä¸ºFalseï¼Œè¯·å°è¯•é¿å…å¤åˆ¶å¹¶æ”¹ä¸ºç›´æ¥æ›¿æ¢
)
# æ ¹æ®åˆ†ä½æ•°èŒƒå›´ï¼ˆé»˜è®¤ä¸ºIQRï¼šInterquartile Rangeï¼‰å‡å»ä¸­ä½æ•°å¹¶ç¼©æ”¾æ•°æ®ã€‚
# IQRæ˜¯ç¬¬1ä¸ªå››åˆ†ä½æ•°ï¼ˆç¬¬25ä¸ªåˆ†ä½æ•°ï¼‰å’Œç¬¬3ä¸ªå››åˆ†ä½æ•°ï¼ˆç¬¬75ä¸ªåˆ†ä½æ•°ï¼‰ä¹‹é—´çš„èŒƒå›´ã€‚

# å®ç°ç›¸åŒåŠŸèƒ½çš„å‡½æ•° robust_scale
```

##### RobustScaler

| **æ–¹æ³•**               | **åŠŸèƒ½**                         |
| ---------------------- | -------------------------------- |
| .fit(X[, y])           | è®¡ç®—ä¸­å€¼å’Œåˆ†ä½æ•°ç”¨äºåç»­æ ‡å‡†åŒ–ã€‚ |
| .fit_transform(X[, y]) | æ‹Ÿåˆæ•°æ®ï¼Œç„¶åè½¬æ¢ã€‚             |
| .inverse_transform(X)  | å°†æ•°æ®ç¼©å‡ä¸ºåŸå§‹è¡¨ç¤ºã€‚           |
| .transform(X)          | é€šè¿‡å±…ä¸­å’Œç¼©æ”¾æ‰§è¡Œæ ‡å‡†åŒ–ã€‚       |



```python
# ç¤ºä¾‹ï¼šè€ƒè™‘ç¦»ç¾¤å€¼
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler(copy=True)
teenager_sns_robustscaled = pd.DataFrame(scaler.fit_transform(teenager_sns[["gradyear","friends"]]),columns = ['gradyear_RobustScaled', 'friends_RobustScaled'])
teenager_sns_robustscaled[["gradyear","friends"]]=teenager_sns[["gradyear","friends"]]
print(teenager_sns_robustscaled[['gradyear_RobustScaled', 'friends_RobustScaled']].describe())
teenager_sns_robustscaled.head(5)
```



### ç¦»æ•£åŒ–ä¸ç‰¹å¾ç¼–ç 

æ¨¡å‹è¾“å…¥çš„ç‰¹å¾é€šå¸¸éœ€è¦æ˜¯æ•°å€¼å‹çš„ï¼Œæ‰€ä»¥éœ€è¦å°†éæ•°å€¼å‹ç‰¹å¾è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾
å¦‚æ€§åˆ«ã€èŒä¸šã€æ”¶å…¥æ°´å¹³ã€å›½å®¶ã€æ±½è½¦ä½¿ç”¨å“ç‰Œã€‚åŒ…æ‹¬æ•°å­—ç¼–ç ã€One-Hotç¼–ç ã€å“‘å˜é‡ç¼–ç æ–¹æ³•ã€‚

**æ•°å­—ç¼–ç **

åŸç‰¹å¾ gender={â€œMâ€ï¼Œâ€œFâ€å’Œâ€œunknownâ€}
ç¼–ç å gender={0ï¼Œ1ï¼Œ2}
ç¼ºç‚¹ï¼šå¼•å…¥äº†æ¬¡åºå…³ç³»



##### LabelEncoder ï¼š

| **æ–¹æ³•**                           | **åŠŸèƒ½**                       |
| ---------------------------------- | ------------------------------ |
| .fit(y)                            | æ‹Ÿåˆæ ‡ç­¾ç¼–ç å™¨ã€‚               |
| .fit_transform(y)                  | æ‹Ÿåˆæ ‡ç­¾ç¼–ç å™¨å¹¶è¿”å›ç¼–ç æ ‡ç­¾ã€‚ |
| .inverse_transform(Y[, threshold]) | å°†æ ‡ç­¾è½¬æ¢å›åŸå§‹ç¼–ç ã€‚         |
| .transform(y)                      | å°†æ ‡ç­¾è½¬æ¢ä¸ºè§„èŒƒåŒ–ç¼–ç ã€‚       |

```python
# ç¤ºä¾‹ï¼šåˆ©ç”¨sklearnå¯¹æ€§åˆ«è¿›è¡Œæ ‡ç­¾ç¼–ç 
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
print(teenager_sns["gender"][:4]) # æ‰“å°å‰4ä¸ªäººçš„æ€§åˆ«
print(le.fit_transform(teenager_sns["gender"][:4])) # ç¼–ç 
print(le.classes_)
''' è¿è¡Œç»“æœ
0		M
1		F
2		M
3		F
Name: gender, dtype: object
[1 0 1 0]
['F' 'M']
'''
```

One-Hotç¼–ç 

å°†åŒ…å«ğ¾ä¸ªå–å€¼çš„ç¦»æ•£å‹ç‰¹å¾è½¬æ¢æˆğ¾ä¸ªäºŒå…ƒç‰¹å¾ï¼ˆå–å€¼ä¸º0æˆ–1ï¼‰

ä¼˜ç‚¹ï¼šç»è¿‡One-Hotç¼–ç ä¹‹åï¼Œä¸åŒçš„åŸå§‹ç‰¹å¾ä¹‹é—´æ‹¥æœ‰ç›¸åŒçš„è·ç¦»ï¼›One-Hotç¼–ç å¯¹åŒ…å«ç¦»æ•£å‹ç‰¹å¾çš„å›å½’æ¨¡å‹åŠåˆ†ç±»æ¨¡å‹çš„æ•ˆæœæœ‰å¾ˆå¥½çš„æå‡ã€‚

ç¼ºç‚¹ï¼šç‰¹å¾æ˜¾è‘—å¢å¤šï¼Œä¸”å¢åŠ äº†ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼š$f_1+f_2+ f_3+ f_4+ f_5=1$ 

```python
# ç¤ºä¾‹ï¼šç”¨Pandaså¯¹æ±½è½¦å“ç‰Œè¿›è¡ŒOne-Hotç¼–ç 
import pandas as pd
df = pd.read_csv("car.csv")
print(df)
df = pd.get_dummies(df, columns=['Brand'])
print(df)
# pd.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)
''' è¿è¡Œç»“æœ
  Brand
0    è·¯è™
1    å‰åˆ©
2    å¥¥è¿ª
3    å¤§ä¼—
4    å¥”é©°

Brand_å‰åˆ©  Brand_å¤§ä¼—  Brand_å¥”é©°  Brand_å¥¥è¿ª  Brand_è·¯è™
0         0         0         0         0         1
1         1         0         0         0         0
2         0         0         0         1         0
3         0         1         0         0         0
4         0         0         1         0         0
'''
```

```python
# sklearn One-Hotç¼–ç 
import sklearn
sklearn.preprocessing.OneHotEncoder(
    n_values='auto', # 'auto'ï¼Œintæˆ–intæ•°ç»„ï¼Œæ¯ä¸ªç‰¹å¾çš„å–å€¼ä¸ªæ•°ã€‚
    categorical_features='all', # æŒ‡å®šå°†å“ªäº›åŠŸèƒ½è§†ä¸ºåˆ†ç±»
    dtype=<class 'numpy.float64'>, # æœŸæœ›çš„è¾“å‡ºç±»å‹
    sparse=True, # å¦‚æœè®¾ç½®ä¸ºTrueå°†è¿”å›ç¨€ç–çŸ©é˜µï¼Œå¦åˆ™å°†è¿”å›ä¸€ä¸ªæ•°ç»„ã€‚
    handle_unknown='error' # è‹¥è½¬æ¢æœŸé—´å­˜åœ¨æœªçŸ¥çš„åˆ†ç±»ç‰¹å¾ï¼Œå¼•å‘é”™è¯¯è¿˜æ˜¯å¿½ç•¥
)
# è¾“å…¥ï¼šæ•´æ•°çŸ©é˜µï¼Œè¡¨ç¤ºåˆ†ç±»ï¼ˆç¦»æ•£ï¼‰ç‰¹å¾æ‰€é‡‡ç”¨çš„å€¼ã€‚
# è¾“å‡ºï¼šç¨€ç–çŸ©é˜µï¼Œå…¶ä¸­æ¯åˆ—å¯¹åº”äºä¸€ä¸ªç‰¹å¾çš„ä¸€ä¸ªå¯èƒ½å€¼ã€‚ 
# å‡è®¾è¾“å…¥ç‰¹å¾é‡‡ç”¨[0ï¼Œn_valuesï¼‰èŒƒå›´å†…çš„å€¼ã€‚

```

##### OneHotEncoder

| **æ–¹æ³•**               | **åŠŸèƒ½**                          |
| ---------------------- | --------------------------------- |
| .fit(X[, y])           | ç”¨OneHotEncoderæ‹ŸåˆXã€‚            |
| .fit_transform(X[, y]) | ç”¨OneHotEncoderæ‹ŸåˆXï¼Œç„¶åè½¬æ¢Xã€‚ |
| .transform(X)          | ä½¿ç”¨One-Hotç¼–ç è½¬æ¢Xã€‚            |

```python
# ç¤ºä¾‹ï¼šsklearnå¯¹æ€§åˆ«è¿›è¡ŒOne-Hotç¼–ç 
import numpy as np
from sklearn.preprocessing import OneHotEncoder
# å¯¹æ€§åˆ«è¿›è¡Œæ•°å­—ç¼–ç 
teenager_sns['gender']=teenager_sns['gender'].map({'M':1,'F':2,np.NaN:3})
enc = OneHotEncoder()
enc.fit(teenager_sns[['gender']]) # å¯¹æ€§åˆ«ç”¨OneHotEncoderè¿›è¡Œæ‹Ÿåˆ
print(enc.active_features_) # æ´»åŠ¨ç‰¹å¾çš„æŒ‡æ•°ï¼Œæ„å‘³ç€è®­ç»ƒé›†ä¸­å®é™…å‡ºç°çš„å€¼
print(enc.transform([[2]]).toarray()) # ä½¿ç”¨One-Hotç¼–ç è½¬æ¢
```

#### ç‰¹å¾ç¦»æ•£åŒ–

ä¸ºä»€ä¹ˆè¦å°†è¿ç»­å‹ç‰¹å¾è¿›è¡Œç¦»æ•£åŒ–å¤„ç†ï¼Ÿ

*   ç®—æ³•ç‰¹å¾ç±»å‹æœ‰è¦æ±‚ã€‚å¦‚å…³è”è§„åˆ™æŒ–æ˜ç®—æ³•ï¼ŒID3å†³ç­–æ ‘ç®—æ³•ã€‚ä¸ºæ›´å¥½åœ°æé«˜ç®—æ³•çš„ç²¾åº¦ã€‚æœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•çš„æ­£ç¡®ç‡æ¯”æ²¡æœ‰å¤„ç†çš„æƒ…å†µå¹³å‡é«˜å‡º10% ï¼›
*   ç¦»æ•£åŒ–å¤„ç†æœ¬è´¨æ˜¯å°†è¿ç»­å‹æ•°æ®åˆ†æ®µï¼Œå¢å¼ºäº†ä¹‹åæ¨¡å‹å¯¹äºæ•°æ®å¼‚å¸¸å€¼çš„é²æ£’æ€§ï¼›
*   ç¦»æ•£åŒ–åçš„ç‰¹å¾ï¼Œå…¶å–å€¼å‡è½¬åŒ–ä¸ºæœ‰æ˜ç¡®å«ä¹‰çš„åŒºé—´å·ï¼Œæ•°æ®çš„å¯è§£é‡Šæ€§æ›´å¼º
*   ç‰¹å¾çš„å–å€¼å¤§å¤§å‡å°‘:
    ä¸€æ¥å‡å°‘äº†æ•°æ®é›†å¯¹äºç³»ç»Ÿå­˜å‚¨ç©ºé—´çš„éœ€æ±‚ï¼Œ
    äºŒæ¥åœ¨ç®—æ³•å»ºæ¨¡ä¸­ä¹Ÿå¤§å¤§å‡å°‘äº†æ¨¡å‹çš„å®é™…è¿ç®—é‡ï¼Œä»è€Œå¯ä»¥æå‡æ¨¡å‹çš„è®¡ç®—æ•ˆç‡

ç‰¹å¾ç¦»æ•£åŒ–çš„åŸºæœ¬æ¦‚å¿µ

*   ç‰¹å¾çš„ç¦»æ•£åŒ–è¿‡ç¨‹æ˜¯å°†è¿ç»­å‹ç‰¹å¾çš„å–å€¼èŒƒå›´åˆ’åˆ†ä¸ºè‹¥å¹²åŒºé—´æ®µ(bin)ï¼Œç„¶åä½¿ç”¨åŒºé—´æ®µä»£æ›¿è½åœ¨è¯¥åŒºé—´æ®µçš„ç‰¹å¾å–å€¼ã€‚

*   åŒºé—´æ®µä¹‹é—´çš„åˆ†å‰²ç‚¹ç§°ä¹‹ä¸ºåˆ‡åˆ†ç‚¹(cut point)ã€‚

*   ç”±åˆ‡åˆ†ç‚¹åˆ†å‰²å‡ºæ¥çš„å­åŒºé—´æ®µçš„ä¸ªæ•°ï¼Œç§°ä¹‹ä¸ºå…ƒæ•° (arity)ã€‚

    

å‡è®¾éœ€è¦å°†â€œå¹´é¾„â€è¿™ä¸ªè¿ç»­å‹ç‰¹å¾åˆ‡åˆ†æˆğ‘˜ä¸ªåŒºé—´æ®µï¼Œåˆ™éœ€è¦(ğ‘˜âˆ’1)ä¸ªåˆ‡åˆ†ç‚¹ã€‚â€œå¹´é¾„â€ç‰¹å¾çš„å–å€¼èŒƒå›´åœ¨ [0,150] ä¹‹é—´ï¼Œé€šè¿‡4ä¸ªåˆ‡åˆ†ç‚¹10ã€25ã€40å’Œ60ï¼Œå°†å…¶è½¬åŒ–æˆä¸º5ä¸ªç¦»æ•£åŒºé—´æ®µ



ç‰¹å¾ç¦»æ•£åŒ–ç›®æ ‡ï¼šåœ¨æ•°æ®ä¿¡æ¯æŸå¤±å°½é‡å°‘çš„å‰æä¸‹ï¼Œå°½å¯èƒ½å‡å°‘å…ƒæ•°

æ–¹æ³•ï¼š

*   äºŒå€¼åŒ–

    *   æ ¹æ®é˜ˆå€¼å°†æ•°æ®äºŒå€¼åŒ–ï¼ˆå°†ç‰¹å¾å€¼è®¾ç½®ä¸º0æˆ–1ï¼‰ï¼šå¤§äºé˜ˆå€¼çš„å€¼æ˜ å°„ä¸º1ï¼Œè€Œå°äºæˆ–ç­‰äºé˜ˆå€¼çš„å€¼æ˜ å°„ä¸º0ã€‚é»˜è®¤é˜ˆå€¼ä¸º0æ—¶ï¼Œåªæœ‰æ­£å€¼æ˜ å°„åˆ°1ã€‚

    *   ```python
        # Scikit-learn ä¸­äºŒå€¼åŒ–å‡½æ•°
        sklearn.preprocessing.Binarizer(
            threshold=0.0, # ä½äºæˆ–ç­‰äºæ­¤å€¼çš„ç‰¹å¾å€¼å°†æ›¿æ¢ä¸º0ï¼Œé«˜äºæ­¤å€¼æ›¿æ¢ä¸º1
            copy=True # è®¾ç½®ä¸ºFalseä»¥æ‰§è¡Œå°±åœ°äºŒå€¼åŒ–å¹¶é¿å…å¤åˆ¶
        )
        # å®ç°ç›¸åŒåŠŸèƒ½çš„å‡½æ•°ï¼šbinarize
        ```

    *   Binarizer æ–¹æ³•

        *   | **æ–¹æ³•**                 | **åŠŸèƒ½**                           |
            | ------------------------ | ---------------------------------- |
            | .fit(X[, y])             | ä»€ä¹ˆéƒ½ä¸åšï¼Œè¿”å›ä¿æŒä¸å˜çš„ä¼°ç®—å™¨ã€‚ |
            | .fit_transform(X[, y])   | æ‹Ÿåˆæ•°æ®ï¼Œç„¶åè½¬æ¢ã€‚               |
            | .transform(X[, y, copy]) | äºŒå€¼åŒ–Xçš„æ¯ä¸ªå…ƒç´ ã€‚                |

    *   ```python
        # ç¤ºä¾‹ï¼šå¯¹æœ‹å‹æ•°é‡è¿›è¡ŒäºŒå€¼åŒ–
        from sklearn.preprocessing import Binarizer
        #ã€€äºŒå€¼åŒ–ï¼Œé˜ˆå€¼è®¾ç½®ä¸º3ï¼Œè¿”å›å€¼ä¸ºäºŒå€¼åŒ–åçš„æ•°æ®
        scaler = Binarizer(threshold=3)
        teenager_sns_binarizer = pd.DataFrame(scaler.fit_transform(teenager_sns[["friends"]]),columns = ["friends_Binarized"])
        teenager_sns_binarizer["friends"] = teenager_sns["friends"]
        ```

    *   

*   ç­‰è·ç¦»æ•£åŒ–

    *   æ ¹æ®è¿ç»­å‹ç‰¹å¾çš„å–å€¼ï¼Œå°†å…¶å‡åŒ€åœ°åˆ’åˆ†æˆ ğ‘˜ ä¸ªåŒºé—´ï¼Œæ¯ä¸ªåŒºé—´çš„å®½åº¦å‡ç›¸ç­‰ï¼Œç„¶åå°†ç‰¹å¾çš„å–å€¼åˆ’å…¥å¯¹åº”çš„åŒºé—´ä»è€Œå®Œæˆç‰¹å¾ç¦»æ•£åŒ–ã€‚
        å¦‚å¹´é¾„å–å€¼åº”åˆ†å¸ƒåœ¨[0,90]ï¼Œç¡®å®šç¦»æ•£åŒ–åçš„åŒºé—´æ®µä¸ªæ•°ä¸º5ã€‚ç­‰è·ç¦»æ•£åŒ–å¯¹è¾“å…¥æ•°æ®è´¨é‡è¦æ±‚é«˜ï¼Œæ— æ³•è§£å†³ç‰¹å¾å­˜åœ¨ç¦»ç¾¤å€¼çš„é—®é¢˜ã€‚è‹¥å­˜åœ¨ç¦»ç¾¤å€¼150ï¼Œåˆ™åˆ‡åˆ†ç‚¹å°†ä¸¥é‡åç§»ã€‚

    *   ```python
        # Pandas cutå®ç°ç­‰è·ç¦»æ•£åŒ–
        import pandas as pd
        pd.cut(
            x, # è¦ç¦»æ•£çš„è¾“å…¥æ•°ç»„ã€‚ å¿…é¡»æ˜¯ä¸€ç»´çš„ã€‚
            bins, # intï¼Œæ ‡é‡åºåˆ—æˆ–pandas.IntervalIndexï¼Œç¦»æ•£çš„æ ‡å‡†ã€‚
            right=True, # æ˜¯å¦åŒ…å«æœ€å³è¾¹ã€‚
            labels=None, # æŒ‡å®šè¿”å›çš„binçš„æ ‡ç­¾ã€‚ å¿…é¡»ä¸ç”Ÿæˆçš„biné•¿åº¦ç›¸åŒã€‚
            precision=3, # å­˜å‚¨å’Œæ˜¾ç¤ºbinæ ‡ç­¾çš„ç²¾åº¦ã€‚
            include_lowest=False # ç¬¬ä¸€ä¸ªé—´éš”æ˜¯å¦åº”è¯¥æ˜¯åŒ…å«åœ¨å†…çš„ã€‚
        )
        ```

    *   ```python
        # ç¤ºä¾‹ï¼šå¯¹æœ‹å‹æ•°é‡è¿›è¡Œç­‰è·ç¦»æ•£åŒ–
        import pandas as pd
        data = teenager_sns['friends'].copy()
        k = 4
        # ç­‰è·ç¦»æ•£åŒ–ï¼Œå„ä¸ªç±»æ¯”ä¾æ¬¡å‘½åä¸º0,1,2,3
        d1 = pd.cut(data, k, labels = range(k))
        ```

    *   

*   ç­‰é¢‘ç¦»æ•£åŒ–

    *   ä¸è¦æ±‚åŒºé—´æ®µçš„å®½åº¦å§‹ç»ˆä¿æŒä¸€è‡´ï¼Œè€Œæ˜¯å°½é‡ä½¿å¾—ç¦»æ•£åŒ–åæ¯ä¸€ä¸ªåŒºé—´å†…çš„æ ·æœ¬é‡å‡è¡¡

        æ ¹æ®è¿ç»­å‹ç‰¹å¾çš„æ€»ä¸ªæ•°ï¼Œå°†å…¶å‡åŒ€åœ°åˆ’åˆ†æˆ ğ‘˜ ä¸ªåŒºé—´æ®µï¼Œä½¿å¾—æ¯ä¸ªåŒºé—´æ®µä¸­çš„æ ·æœ¬æ•°ç›¸åŒï¼Œç„¶åæ¯ä¸€ä»½æ•°æ®çš„å–å€¼èŒƒå›´å³æ˜¯å¯¹åº”çš„ç‰¹å¾ç¦»æ•£åŒ–åŒºé—´

    *   ç¼ºç‚¹ï¼šæœ‰æ—¶ä¼šå°†åŒæ ·æˆ–æ¥è¿‘çš„æ ·æœ¬åˆ’åˆ†å…¥ä¸åŒçš„åŒºé—´ï¼Œå®¹æ˜“ä½¿å¾—ç›¸é‚»åŒºé—´æ®µå†…çš„æ•°æ®å…·æœ‰ç›¸ä¼¼çš„ç‰¹æ€§ã€‚

    *   ```python
        # Pandas qcutå®ç°ç­‰é¢‘ç‡ç¦»æ•£åŒ–
        pandas.qcut(
            x, # 1ç»´ ndarray æˆ– Series
            q, # åˆ†ä½æ•°ã€‚ 10ä¸ºååˆ†ä½æ•°ï¼Œ4ä¸ºå››åˆ†ä½æ•°ç­‰
            labels=None, #ã€€æŒ‡å®šè¿”å›çš„binçš„æ ‡ç­¾ã€‚ å¿…é¡»ä¸ç”Ÿæˆçš„biné•¿åº¦ç›¸åŒã€‚
            retbins=False, #ã€€æ˜¯å¦è¿”å›ï¼ˆbinsï¼Œæ ‡ç­¾ï¼‰ã€‚
            precision=3 # å­˜å‚¨å’Œæ˜¾ç¤ºbinæ ‡ç­¾çš„ç²¾åº¦ã€‚
        )
        
        import pandas as pd
        data = teenager_sns['friends'].copy()
        k = 4
        #ç­‰é¢‘ç‡ç¦»æ•£åŒ–ï¼Œå„ä¸ªç±»æ¯”ä¾æ¬¡å‘½åä¸º'A','B','C','D'
        d1 = pd.qcut(data, k, labels = ['A','B','C','D'])
        
        
        ```

    *   

*   èšç±»ç¦»æ•£åŒ–

    *   1.  å¯¹äºéœ€è¦ç¦»æ•£åŒ–çš„è¿ç»­å‹ç‰¹å¾ï¼Œé‡‡ç”¨èšç±»ç®—æ³•(å¦‚K-meansã€EMç®—æ³•ç­‰)ï¼ŒæŠŠæ ·æœ¬ä¾æ®è¯¥ç‰¹å¾çš„åˆ†å¸ƒåˆ’åˆ†æˆç›¸åº”çš„ç°‡æˆ–ç±»ï¼›
        2.  åœ¨èšç±»ç»“æœçš„åŸºç¡€ä¸Šï¼ŒåŸºäºç‰¹å®šçš„ç­–ç•¥ï¼Œå†³å®šæ˜¯å¦å¯¹ç°‡è¿›è¡Œè¿›ä¸€æ­¥åˆ†è£‚æˆ–åˆå¹¶ã€‚åˆ©ç”¨è‡ªé¡¶å‘ä¸‹çš„ç­–ç•¥å¯ä»¥é’ˆå¯¹æ¯ä¸€ä¸ªç°‡ç»§ç»­è¿è¡Œèšç±»ç®—æ³•ï¼Œå°†å…¶ç»†åˆ†ä¸ºæ›´å°çš„å­ç°‡ï¼›åˆ©ç”¨è‡ªåº•å‘ä¸Šçš„ç­–ç•¥ï¼Œåˆ™å¯ä»¥å¯¹é‚»è¿‘ç›¸ä¼¼çš„ç°‡è¿›è¡Œåˆå¹¶å¤„ç†å¾—åˆ°æ–°çš„ç°‡ï¼›
        3.  åœ¨æœ€ç»ˆç¡®å®šåˆ’åˆ†çš„ç°‡ä¹‹åï¼Œç¡®å®šåˆ‡åˆ†ç‚¹ä»¥åŠåŒºé—´ä¸ªæ•°ã€‚

    *   åœ¨æ•´ä¸ªèšç±»çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦äº‹å…ˆç¡®å®šç°‡çš„ä¸ªæ•°ä»¥åŠæè¿°æ ·æœ¬ä¹‹é—´çš„è·ç¦»è®¡ç®—æ–¹å¼ã€‚å¦‚ä½•é€‰å®šç°‡çš„ä¸ªæ•°ä¹Ÿä¼šå½±å“èšç±»ç®—æ³•çš„æ•ˆæœï¼Œä»è€Œå½±å“ç‰¹å¾çš„ç¦»æ•£åŒ–ã€‚

    *   ```python
        # ç¤ºä¾‹ï¼šå¯¹æœ‹å‹æ•°é‡è¿›è¡ŒK-meansèšç±»ç¦»æ•£åŒ–
        import pandas as pd
        data = teenager_sns['friends'].copy()
        k = 4
        # èšç±»ç¦»æ•£åŒ–ï¼Œå„ä¸ªç±»æ¯”ä¾æ¬¡å‘½åä¸º'A','B','C','D'
        from sklearn.cluster import KMeans # å¼•å…¥KMeans
        kmodel = KMeans(n_clusters = k) # å»ºç«‹æ¨¡å‹
        kmodel.fit(data.values.reshape((len(data), 1))) # è®­ç»ƒæ¨¡å‹
        c = pd.DataFrame(kmodel.cluster_centers_).sort_values(0)  # è¾“å‡ºèšç±»ä¸­å¿ƒï¼Œå¹¶ä¸”æ’åº
        w = c.rolling(2).mean().iloc[1:] # ç›¸é‚»ä¸¤é¡¹æ±‚ä¸­ç‚¹ï¼Œä½œä¸ºè¾¹ç•Œç‚¹
        w = [0] + list(w[0]) + [data.max()] # æŠŠé¦–æœ«è¾¹ç•Œç‚¹åŠ ä¸Šï¼Œw[0]ä¸­0ä¸ºåˆ—ç´¢å¼•
        d2 = pd.cut(data, w, labels =  ['A','B','C','D'])
        ```

    *   

*   ä¿¡æ¯å¢ç›Šç¦»æ•£åŒ–

*   å¡æ–¹ç¦»æ•£åŒ–ç­‰

    *   ![](image/datalisan.png)

### æ€»ç»“

| å¸¸è§æ•°æ®é¢„å¤„ç†æ–¹æ³• | å¯¹åº”çš„sklearnå’Œpandasçš„ç±»                                    |
| ------------------ | ------------------------------------------------------------ |
| ç¼ºå¤±å€¼å¤„ç†         | sklearn.preprocessing.Imputer                                |
| ç¦»ç¾¤å€¼æ£€æµ‹å’Œå¤„ç†   | sklearn.neighbors.LocalOutlierFactor                         |
| æ ‡å‡†åŒ–             | sklearn.preprocessing.StandardScaler, sklearn.preprocessing.MinMaxScaler, sklearn.preprocessing.RobustScaler |
| ç‰¹å¾ç¼–ç            | sklearn.preprocessing.OneHotEncoder, pandas.get_dummiesï¼Œsklearn.preprocessing.LabelEncoder |
| ç¦»æ•£åŒ–             | pandas.cutpandas.qcut sklearn.preprocessing.Binarizer        |



## **å›å½’æ¨¡å‹**

### å›å½’é—®é¢˜å’ŒScikit-learnå›å½’æ¨¡å—ä»‹ç»

å›å½’(Regression)ç”¨ä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡æ¥é¢„æµ‹å› å˜é‡çš„æ•°å­¦æ–¹æ³•ã€‚

åœ¨ä¸€ä¸ªå›å½’æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å…³æ³¨æˆ–é¢„æµ‹çš„å˜é‡å«åšå› å˜é‡ï¼Œæˆ‘ä»¬é€‰å–çš„ç”¨æ¥è§£é‡Šå› å˜é‡å˜åŒ–çš„å˜é‡å«åšè‡ªå˜é‡ã€‚

Scikit-learnå›å½’çš„ä¸»è¦ç®—æ³•

| **ç±»**                         | **è¯´æ˜**                                      |
| ------------------------------ | --------------------------------------------- |
| linear_model.LinearRegression  | æ™®é€šæœ€å°äºŒä¹˜æ³•                                |
| linear_model.Ridge             | å²­å›å½’                                        |
| linear_model.Lasso             | ç”¨L1ä½œä¸ºæ­£åˆ™é¡¹çš„çº¿æ€§æ¨¡å‹                      |
| linear_model.ElasticNet        | å°†L1å’ŒL2ç»„åˆä½œä¸ºæ­£åˆ™é¡¹çš„çº¿æ€§å›å½’              |
| linear_model.BayesianRidge     | è´å¶æ–¯å²­å›å½’                                  |
| linear_model.TheilSenRegressor | æ³°å°”æ£®å›å½’ï¼šç¨³å¥çš„å¤šå…ƒå›å½’æ¨¡å‹                |
| linear_model.HuberRegressor    | Huberå›å½’ï¼ˆå¯¹å¼‚å¸¸å€¼å…·æœ‰ç¨³å¥æ€§çš„çº¿æ€§å›å½’æ¨¡å‹ï¼‰ |
| linear_model.RANSACRegressor   | RANSACï¼ˆéšæœºé‡‡æ ·ä¸€è‡´æ€§ç®—æ³•ï¼‰                  |
| neighbors.KNeighborsRegressor  | Kè¿‘é‚»å›å½’                                     |
| svm.SVR                        | æ”¯æŒå‘é‡å›å½’                                  |
| tree.DecisionTreeRegressor     | å†³ç­–æ ‘å›å½’                                    |

### çº¿æ€§å›å½’

#### ä¸€å…ƒçº¿æ€§å›å½’

#### å¤šå…ƒçº¿æ€§å›å½’

#### æœ€å°äºŒä¹˜ä¼°è®¡

```python
# Scikit-learnä¸­æ™®é€šæœ€å°äºŒä¹˜çº¿æ€§å›å½’å‡½æ•°ç”¨æ³•
import sklearn
sklearn.linear_model.LinearRegression(
    fit_intercept=True, # æ˜¯å¦è®¡ç®—æ­¤æ¨¡å‹çš„æˆªè·ã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™ä¸ä¼šåœ¨è®¡ç®—ä¸­ä½¿ç”¨æˆªè·
    normalize=False # å½“fit_interceptè®¾ç½®ä¸ºFalseæ—¶ï¼Œå°†å¿½ç•¥æ­¤å‚æ•°ã€‚ å¦‚æœä¸ºçœŸï¼Œåˆ™å›å½’é‡Xå°†åœ¨å›å½’ä¹‹å‰é€šè¿‡å‡å»å¹³å‡å€¼å¹¶é™¤ä»¥L2èŒƒæ•°æ¥å½’ä¸€åŒ–ã€‚
)
```

##### LinearRegression

| æ–¹æ³•         | åŠŸèƒ½                 |
| ------------ | -------------------- |
| .fit(X, y)   | æ‹Ÿåˆæ¨¡å‹             |
| .predict(X)  | ä½¿ç”¨æ¨¡å‹é¢„æµ‹         |
| .score(X, y) | è¿”å›é¢„æµ‹çš„å†³å®šç³»æ•°R2 |

| å±æ€§        | è¯´æ˜          |
| ----------- | ------------- |
| .coef_      | å›å½’ç³»æ•°ï¼ˆwï¼‰ |
| .intercept_ | æˆªè·          |

```python
#æ„å»ºçº¿æ€§å›å½’æ¨¡å‹
from sklearn.linear_model import LinearRegression
from sklearn import metrics
lr = LinearRegression()
lr.fit(train_x, train_y)
```



è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½

çº¿æ€§å›å½’æ¨¡å‹ï¼š

`charges = 258.79*age+370.97*bmi+461.80*children-82.56*sex_male+23779.80*smoker_yes-564.06*region_northwest
-1243.80*region_southeast-1011.00*region_southwest-12881.86`

```python
#å›å½’æ•ˆæœè¯„ä¼°
pred_y_test = lr.predict(test_x)
pred_y_train = lr.predict(train_x)

print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°: ", round(metrics.r2_score(train_y, pred_y_train),3))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°: ", round(metrics.r2_score(test_y, pred_y_test),3))

''' è¿è¡Œç»“æœï¼š 
è®­ç»ƒé›†çš„å†³å®šç³»æ•°:  0.751
æµ‹è¯•é›†çš„å†³å®šç³»æ•°:  0.75
'''
```

### çº¿æ€§å›å½’æ­£åˆ™åŒ–

```python
# sklearn å²­å›å½’
import sklearn
sklearn.linear_model.Ridge(
    alpha=1.0, # æ­£åˆ™åŒ–å¼ºåº¦,å³æ­£åˆ™åŒ–å‚æ•°Î»
    fit_intercept=True # æ˜¯å¦è®¡ç®—æ­¤æ¨¡å‹çš„æˆªè·ã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™ä¸ä¼šåœ¨è®¡ç®—ä¸­ä½¿ç”¨æˆªè·
)

```

##### Ridge

| æ–¹æ³•         | åŠŸèƒ½                 |
| ------------ | -------------------- |
| .fit(X, y)   | æ‹Ÿåˆæ¨¡å‹             |
| .predict(X)  | ä½¿ç”¨æ¨¡å‹é¢„æµ‹         |
| .score(X, y) | è¿”å›é¢„æµ‹çš„å†³å®šç³»æ•°R2 |

| å±æ€§        | è¯´æ˜          |
| ----------- | ------------- |
| .coef_      | å›å½’ç³»æ•°ï¼ˆwï¼‰ |
| .intercept_ | æˆªè·          |

```python
# sklearn LASSO å›å½’
import sklearn
sklearn.linear_model.Lasso(
    alpha=1.0, # å³æ­£åˆ™åŒ–å‚æ•°Î»ï¼Œå¸¸æ•°ä¹˜ä»¥L1é¡¹ã€‚é»˜è®¤ä¸º1.0ã€‚
    fit_intercept=True # æ˜¯å¦è®¡ç®—æ­¤æ¨¡å‹çš„æˆªè·ã€‚
)
```

##### Lasso

| æ–¹æ³•         | åŠŸèƒ½                 |
| ------------ | -------------------- |
| .fit(X, y)   | æ‹Ÿåˆæ¨¡å‹             |
| .predict(X)  | ä½¿ç”¨æ¨¡å‹é¢„æµ‹         |
| .score(X, y) | è¿”å›é¢„æµ‹çš„å†³å®šç³»æ•°R2 |

| å±æ€§        | è¯´æ˜          |
| ----------- | ------------- |
| .coef_      | å›å½’ç³»æ•°ï¼ˆwï¼‰ |
| .intercept_ | æˆªè·          |

```python
# è‡ªè¡Œè½¦åˆ†äº«æ•°æ®é›†
import pandas as pd
from sklearn import model_selection
data = pd.read_csv('./input/Bike.csv')
Y = data['cnt']
X = data.copy().drop(['cnt'], axis=1)
# å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸¤éƒ¨åˆ†
train_x, test_x, train_y, test_y = model_selection.train_test_split(X, Y, test_size = 0.3, random_state = 14)

from sklearn.linear_model import LinearRegression
from sklearn import metrics
lr = LinearRegression()
lr.fit(train_x, train_y)

# å›å½’æ•ˆæœè¯„ä¼°
pred_y_test = lr.predict(test_x)
pred_y_train = lr.predict(train_x)
print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(train_y, pred_y_train),4))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(test_y, pred_y_test),4))
''' è¿è¡Œç»“æœ 
è®­ç»ƒé›†çš„å†³å®šç³»æ•°: 0.7986
æµ‹è¯•é›†çš„å†³å®šç³»æ•°: 0.8
'''
```

```python
# å²­å›å½’æ¨¡å‹
from sklearn.linear_model import Ridge
from sklearn import metrics
clf = Ridge(alpha=1.0)
clf.fit(train_x, train_y)

# å›å½’æ•ˆæœè¯„ä¼°
pred_y_test = clf.predict(test_x)
pred_y_train = clf.predict(train_x)

print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(train_y, pred_y_train),4))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(test_y, pred_y_test),4))
''' è¿è¡Œç»“æœ
è®­ç»ƒé›†çš„å†³å®šç³»æ•°: 0.7974
æµ‹è¯•é›†çš„å†³å®šç³»æ•°: 0.802
'''

# å²­å›å½’æ­£åˆ™åŒ–è·¯å¾„
from sklearn.linear_model import Ridge
from sklearn import metrics
import numpy as np
import math

lambdaList=np.arange(math.exp(0),math.exp(10), 10)
coef = pd.DataFrame() # åˆ›å»ºä¸€ä¸ªç©ºçš„dataframe

for lambda_ in lambdaList:
    index = lambda_ 
    columns = X.columns
    clf = Ridge(alpha = lambda_)
    clf.fit(train_x, train_y)
    df = pd.DataFrame([clf.coef_],columns=columns)
    df['lambda']=math.log(index)
    coef = coef.append(df, ignore_index = True) 

import matplotlib.pyplot as plt
# %matplotlib inline # åœ¨jupyterä¸­å¯ç”¨
# ç»˜å¤šæ¡çº¿å›¾
for feature in X.columns:
    plt.plot( 'lambda', feature, data=coef)

plt.legend( bbox_to_anchor=(1.3, 1),loc='upper right')
plt.xlabel(r'log($\lambda$)', fontsize=16)
plt.ylabel(r'Coefficient', fontsize=16)
plt.show()
```

```python
# LASSO æ¨¡å‹
from sklearn import linear_model
from sklearn import metrics
clf = linear_model.Lasso(alpha=0.1)
clf.fit(train_x, train_y)

# LASSOå›å½’æ•ˆæœè¯„ä¼°
pred_y_test = clf.predict(test_x)
pred_y_train = clf.predict(train_x)

print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(train_y, pred_y_train),4))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(test_y, pred_y_test),4))
''' è¿è¡Œç»“æœ
è®­ç»ƒé›†çš„å†³å®šç³»æ•°: 0.7986
æµ‹è¯•é›†çš„å†³å®šç³»æ•°: 0.8001
'''
```



### ç¨³å¥æ€§å›å½’ï¼šç¦»ç¾¤ç‚¹ä¸å»ºæ¨¡è¯¯å·®

åœ¨å­˜åœ¨æœ‰æŸçš„æ•°æ®ï¼ˆå¼‚å¸¸å€¼æˆ–æ¨¡å‹ä¸­çš„é”™è¯¯ï¼‰çš„æƒ…å†µä¸‹æ‹Ÿåˆå›å½’æ¨¡å‹ã€‚

Scikit-learnæä¾›äº†3ç§ç¨³å¥æ€§å›å½’ä¼°è®¡æ–¹æ³•ï¼š

*   éšæœºæŠ½æ ·ä¸€è‡´æ€§ç®—æ³•ï¼ˆRANSACï¼‰
*   æ³°å°”æ£®å›å½’ï¼ˆTheil Senï¼‰ 
*   Huberå›å½’ 



#### éšæœºæŠ½æ ·ä¸€è‡´æ€§ç®—æ³•ï¼ˆRANSACï¼‰

RANSAC (RANdom SAmple Consensus) ç®—æ³•

ç®—æ³•æ­¥éª¤ï¼š
1ï¼‰ä»åŸå§‹æ•°æ®ä¸­éšæœºé€‰æ‹©ä¸€äº›æ ·æœ¬ä½œä¸ºâ€œå†…ç‚¹â€ã€‚
2ï¼‰ç”¨1ä¸­é€‰æ‹©çš„æ ·æœ¬æ‹Ÿåˆæ¨¡å‹ã€‚
3ï¼‰åˆ©ç”¨æ¨¡å‹è®¡ç®—å…¶å®ƒæ ·æœ¬çš„æ®‹å·®ï¼Œè‹¥æŸä¸ªæ ·æœ¬çš„æ®‹å·®å°äºé¢„å…ˆè®¾ç½®çš„é˜ˆå€¼tï¼Œåˆ™å°†å…¶åŠ å…¥å†…ç‚¹ï¼Œå°†å†…ç‚¹ä¸­çš„æ ·æœ¬é‡æ‰©å……ã€‚
4ï¼‰ç”¨æ‰©å……åçš„å†…ç‚¹æ‹Ÿåˆæ¨¡å‹ï¼Œè®¡ç®—å‡æ–¹è¯¯å·®ã€‚
5ï¼‰é‡å¤1-4æ­¥ï¼Œæœ€ç»ˆé€‰å–å‡æ–¹è¯¯å·®æœ€å°çš„æ¨¡å‹ã€‚

```python
# sklearn å®ç°RANSAC
import sklearn
sklearn.linear_model.RANSACRegressor(
    base_estimator=None, # ä¼°ç®—å™¨ï¼Œå®ç°fit(X,y)å’Œscore(X,y)å’Œpredict(X)
    min_samples=None, # ä»åŸå§‹æ•°æ®ä¸­éšæœºé€‰æ‹©çš„æœ€å°æ ·æœ¬æ•°
    residual_threshold=None, # è¢«å½’ç±»ä¸ºå†…ç‚¹çš„æ•°æ®æ ·æœ¬çš„æœ€å¤§æ®‹å·®ã€‚
    max_trials=100, # éšæœºæ ·æœ¬é€‰æ‹©çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚
    stop_n_inliers=inf, # åœæ­¢è¿­ä»£çš„æœ€å°æ•°é‡çš„å†…ç‚¹ã€‚
    stop_score=inf, # å¦‚æœå¾—åˆ†å¤§äºæ­¤é˜ˆå€¼ï¼Œåˆ™åœæ­¢è¿­ä»£ã€‚
    loss='absolute_loss' # {absolute_loss, squared_loss}åˆ†åˆ«æ‰¾å‡ºæ¯ä¸ªæ ·æœ¬çš„ç»å¯¹æŸå¤±å’Œå¹³æ–¹æŸå¤±ã€‚ 
)
```

##### RANSACRegressor

| æ–¹æ³•         | åŠŸèƒ½                 |
| ------------ | -------------------- |
| .fit(X, y)   | æ‹Ÿåˆæ¨¡å‹             |
| .predict(X)  | ä½¿ç”¨ä¼°è®¡çš„æ¨¡å‹é¢„æµ‹   |
| .score(X, y) | è¿”å›é¢„æµ‹çš„å†³å®šç³»æ•°R2 |

| å±æ€§        | è¯´æ˜         |
| ----------- | ------------ |
| .estimator_ | æœ€ä½³æ‹Ÿåˆæ¨¡å‹ |



é¢„æµ‹çš„åˆ†æ•° ï¼š é»˜è®¤æ˜¯R^2

Score(X,y)ï¼šè¿”å›ç»™å®šæµ‹è¯•æ•°æ®çš„å¹³å‡ç²¾åº¦ï¼Œç”¨äºstop_scoreå®šä¹‰çš„åœæ­¢æ ‡å‡†ã€‚ å¦å¤–ï¼Œè¯¥åˆ†æ•°ç”¨äºå†³å®šé€‰æ‹©ä¸¤ä¸ªéšåçš„å¤§å…±è¯†é›†ä¸­çš„å“ªä¸€ä¸ªä½œä¸ºæ›´å¥½çš„ä¸€è‡´ã€‚

#### æ³°å°”æ£®å›å½’ï¼ˆTheil Senï¼‰

ä¸€èˆ¬çš„å›å½’æ¨¡å‹å…¬å¼ä¸ºï¼š$y = w_0 + w_1 x + \varepsilon$ ã€‚å…¶ä¸­$w_0,w_1$æ˜¯æ¨¡å‹çš„å‚æ•°ï¼Œ$\varepsilon$ä¸ºæ¨¡å‹çš„éšæœºè¯¯å·®ã€‚

åœ¨æ³°å°”æ£®å›å½’ä¸­ï¼Œ

$\displaystyle w_1 = Median \{\frac{y_i-y_j}{x_i-x_j} : x_i \neq x_j,i<j=1,...,n \}$

æ³°å°”æ£®å›å½’é€šè¿‡é€‰æ‹©é€šè¿‡æˆå¯¹ç‚¹çš„æ‰€æœ‰çº¿çš„æ–œç‡çš„ä¸­ä½æ•°æ¥ç¨³å¥åœ°å°†çº¿æ‹Ÿåˆåˆ°å¹³é¢ä¸­çš„é‡‡æ ·ç‚¹ï¼ˆç®€å•çº¿æ€§å›å½’ï¼‰ã€‚

```python
# æ³°å°”æ£®å›å½’ï¼šç¨³å¥çš„å¤šå…ƒå›å½’æ¨¡å‹
import sklearn
sklearn.linear_model.TheilSenRegressor(
    fit_intercept=True, # æ˜¯å¦è®¡ç®—æ­¤æ¨¡å‹çš„æˆªè·ã€‚
    n_subsamples=None, # è®¡ç®—å‚æ•°çš„æ ·æœ¬æ•°ã€‚
    max_iter=300 # è®¡ç®—ä¸­ä½æ•°çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚
)
# æ³°å°”æ£®å›å½’ä½¿ç”¨å¤šç»´åº¦çš„ä¸­ä½æ•°è¿›è¡Œä¼°è®¡ã€‚å› æ­¤ï¼Œå¯¹å¤šå˜é‡å¼‚å¸¸å€¼å…·æœ‰ç¨³å¥æ€§ã€‚ä¼°è®¡é‡çš„ç¨³å¥æ€§éšç€æ•°æ®ç‰¹å¾ç»´æ•°çš„æå‡è€Œè¿…é€Ÿé™ä½ã€‚åœ¨é«˜ç»´æ•°æ®ä¸­ï¼Œæ³°å°”æ£®å›å½’çš„æ•ˆæœæœ‰æ—¶è¿˜ä¸å¦‚æœ€å°äºŒä¹˜æ³•ã€‚
```

##### TheilSenRegressor

| æ–¹æ³•         | åŠŸèƒ½                 |
| ------------ | -------------------- |
| .fit(X, y)   | æ‹Ÿåˆæ¨¡å‹             |
| .predict(X)  | ä½¿ç”¨ä¼°è®¡çš„æ¨¡å‹é¢„æµ‹   |
| .score(X, y) | è¿”å›é¢„æµ‹çš„å†³å®šç³»æ•°R2 |

| å±æ€§        | è¯´æ˜     |
| ----------- | -------- |
| .coef_      | å›å½’ç³»æ•° |
| .intercept_ | æˆªè·     |



#### Huberå›å½’

![](image/huber.png)

```python
# sklearn Huber å›å½’
sklearn.linear_model.HuberRegressor(
    epsilon=1.35, # å½’ç±»ä¸ºå¼‚å¸¸å€¼çš„æ ·æœ¬æ•°ã€‚epsilonè¶Šå°ï¼Œç¨³å¥æ€§è¶Šé«˜ã€‚
    alpha=0.0001, # æ­£åˆ™åŒ–å‚æ•°ã€‚
    warm_start=False, # å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™æ¯æ¬¡è°ƒç”¨éƒ½ä¼šé‡å†™ç³»æ•°ã€‚
    fit_intercept=True # æ˜¯å¦æ‹Ÿåˆæˆªè·ã€‚
)
```

##### HuberRegressor

| æ–¹æ³•         | åŠŸèƒ½                 |
| ------------ | -------------------- |
| .fit(X, y)   | æ‹Ÿåˆæ¨¡å‹             |
| .predict(X)  | ä½¿ç”¨æ¨¡å‹é¢„æµ‹         |
| .score(X, y) | è¿”å›é¢„æµ‹çš„å†³å®šç³»æ•°R2 |

| å±æ€§        | è¯´æ˜     |
| ----------- | -------- |
| .coef_      | å›å½’ç³»æ•° |
| .intercept_ | æˆªè·     |

```python
import pandas as pd
from sklearn import model_selection
data = pd.read_csv('./input/ElectricalLength.csv')
Y = data['Electrical length']
X = data.copy().drop(['Electrical length'], axis=1)
# å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸¤éƒ¨åˆ†
train_x, test_x, train_y, test_y = model_selection.train_test_split(X, Y, test_size = 0.2)

# çº¿æ€§å›å½’æ¨¡å‹
from sklearn.linear_model import LinearRegression
from sklearn import metrics
lr = LinearRegression()
lr.fit(train_x, train_y)

#å›å½’æ•ˆæœè¯„ä¼°
pred_y_test = lr.predict(test_x)
pred_y_train = lr.predict(train_x)

print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(train_y, pred_y_train),4))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(test_y, pred_y_test),4))
''' è¿è¡Œç»“æœ
è®­ç»ƒé›†çš„å†³å®šç³»æ•°: 0.7063
æµ‹è¯•é›†çš„å†³å®šç³»æ•°: 0.6667
'''
```

```python
# RANSACå›å½’æ¨¡å‹
from sklearn import linear_model
from sklearn import metrics
ransac = linear_model.RANSACRegressor()
ransac.fit(train_x, train_y)

#å›å½’æ•ˆæœè¯„ä¼°
pred_y_test = ransac.predict(test_x)
pred_y_train = ransac.predict(train_x)

print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(train_y, pred_y_train),4))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(test_y, pred_y_test),4))
''' è¿è¡Œç»“æœ
è®­ç»ƒé›†çš„å†³å®šç³»æ•°: 0.681
æµ‹è¯•é›†çš„å†³å®šç³»æ•°: 0.6851
'''
```

```python
# æ³°å°”æ£®å›å½’æ¨¡å‹
from sklearn.linear_model import TheilSenRegressor
from sklearn import metrics
theil_sen = TheilSenRegressor()
theil_sen.fit(train_x, train_y)

#å›å½’æ•ˆæœè¯„ä¼°
pred_y_test = theil_sen.predict(test_x)
pred_y_train = theil_sen.predict(train_x)

print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(train_y, pred_y_train),4))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(test_y, pred_y_test),4))
''' è¿è¡Œç»“æœ
è®­ç»ƒé›†çš„å†³å®šç³»æ•°: 0.7
æµ‹è¯•é›†çš„å†³å®šç³»æ•°: 0.6742
'''
```

```python
# Huberå›å½’æ¨¡å‹
from sklearn.linear_model import HuberRegressor
from sklearn import metrics
huberRegressor = HuberRegressor()
huberRegressor.fit(train_x, train_y)

#å›å½’æ•ˆæœè¯„ä¼°
pred_y_test = huberRegressor.predict(test_x)
pred_y_train = huberRegressor.predict(train_x)

print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(train_y, pred_y_train),4))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°:", round(metrics.r2_score(test_y, pred_y_test),4))
''' è¿è¡Œç»“æœ
è®­ç»ƒé›†çš„å†³å®šç³»æ•°: 0.7044
æµ‹è¯•é›†çš„å†³å®šç³»æ•°: 0.6722
'''
```

**ç¨³å¥æ€§å›å½’æ–¹æ³•çš„é€‰å–**

Huberå›å½’ä¸€èˆ¬æ¯”RANSACå’Œæ³°å°”æ£®å›å½’å¿«ï¼Œé™¤éæ ·æœ¬æ•°å¾ˆå¤§ï¼Œå³n_samples >>n_featuresã€‚è¿™æ˜¯å› ä¸ºRANSACå’Œæ³°å°”æ£®å›å½’é€‚åˆè¾ƒå°çš„æ•°æ®å­é›†ã€‚ä½†æ˜¯ï¼Œå¯¹äºé»˜è®¤å‚æ•°ï¼ŒHuberå›å½’æ¯”RANSACå’Œæ³°å°”æ£®å›å½’ç¨³å¥ã€‚ 
RANSACæ¯”æ³°å°”æ£®å›å½’å¿«ï¼Œå¹¶ä¸”åœ¨æ ·æœ¬æ•°é‡ä¸Šçš„ä¼¸ç¼©æ€§ï¼ˆé€‚åº”æ€§ï¼‰æ›´å¥½ã€‚
RANSACå¯ä»¥æ›´å¥½åœ°å¤„ç†yæ–¹å‘çš„å¤§å€¼å¼‚å¸¸ç‚¹ï¼ˆä¸€èˆ¬æƒ…å†µä¸‹ï¼‰ã€‚
æ³°å°”æ£®å›å½’å¯ä»¥æ›´å¥½åœ°åº”å¯¹Xæ–¹å‘çš„ä¸­ç­‰å¤§å°çš„å¼‚å¸¸å€¼ï¼Œä½†æ˜¯è¿™ä¸ªå±æ€§å°†åœ¨é«˜ç»´æƒ…å†µä¸‹æ¶ˆå¤±ã€‚
ä¸ç¡®å®šé€‰å“ªä¸ªç®—æ³•çš„æ—¶å€™ï¼Œè¯·ä½¿ç”¨RANSAC ã€‚

### å…¶ä»–å›å½’æ–¹æ³•

éçº¿æ€§å›å½’æ–¹æ³•ï¼š

*   Kè¿‘é‚»å›å½’
*   æ”¯æŒå‘é‡å›å½’ï¼ˆSVRï¼‰
*   å†³ç­–æ ‘å›å½’
*   æ ·æ¡å›å½’  
*   å¾„å‘åŸºç½‘ç»œ 
*   é«˜æ–¯è¿‡ç¨‹
*    â€¦â€¦

#### Kè¿‘é‚»å›å½’

å½“å¯¹æµ‹è¯•æ ·æœ¬è¿›è¡Œå›å½’é¢„æµ‹æ—¶ï¼Œé€šè¿‡æŒ‰è·ç¦»æ’åºæ‰¾å‡ºä¸€ä¸ªæ ·æœ¬çš„kä¸ªæœ€è¿‘é‚»å±…ï¼Œå°†è¿™äº›é‚»å±…çš„æ ‡ç­¾çš„å¹³å‡å€¼æˆ–åŠ æƒå¹³å‡å€¼ï¼ˆè·ç¦»çš„å€’æ•°ä¹˜ä»¥æƒé‡ï¼‰èµ‹ç»™è¯¥æ ·æœ¬ï¼Œå°±å¯ä»¥å¾—åˆ°è¯¥æ ·æœ¬çš„é¢„æµ‹å€¼ã€‚

```python
# sklearn Kè¿‘é‚»å›å½’
import sklearn
sklearn.neighbors.KNeighborsRegressor(
    n_neighbors=5, # ç”¨äºæŸ¥è¯¢çš„é‚»å±…æ•°ï¼ˆkï¼‰ã€‚
    weights=â€˜uniformâ€™, # æ˜¯å¦å¯¹æ ·æœ¬è¿›è¡ŒåŠ æƒï¼Œé»˜è®¤æ‰€æœ‰ç‚¹çš„æƒé‡ç›¸åŒã€‚
    algorithm=â€˜autoâ€™, # è®¡ç®—æœ€è¿‘é‚»å±…çš„ç®—æ³•ã€‚
    p=2, # MinkowskiæŒ‡æ ‡çš„å¹‚å‚æ•°ï¼Œp=1æ˜¯æ›¼å“ˆé¡¿è·ç¦»ï¼Œp=2æ˜¯æ¬§æ°è·ç¦»
    metric=â€˜minkowskiâ€™ # è·ç¦»åº¦é‡æŒ‡æ ‡ã€‚
)
```

##### KNeighborsRegressor

| æ–¹æ³•         | åŠŸèƒ½                 |
| ------------ | -------------------- |
| .fit(X, y)   | æ‹Ÿåˆæ¨¡å‹             |
| .predict(X)  | æ ¹æ®æ¨¡å‹é¢„æµ‹         |
| .score(X, y) | è¿”å›é¢„æµ‹çš„å†³å®šç³»æ•°R2 |



#### å†³ç­–æ ‘å›å½’

```python
# sklearn å†³ç­–æ ‘å›å½’
import sklearn
sklearn.tree.DecisionTreeRegressor(
    criterion='mse', #ã€€æ¯ä¸€æ¬¡åˆ†è£‚çš„è¯„ä»·æ ‡å‡†ï¼Œé»˜è®¤ä½¿ç”¨å‡æ–¹è¯¯å·®(MSE)
    splitter='best', #ã€€ç”¨äºåœ¨æ¯ä¸ªèŠ‚ç‚¹å¤„é€‰æ‹©æ‹†åˆ†çš„ç­–ç•¥ã€‚
    max_depth=None, #ã€€æ ‘çš„æœ€å¤§æ·±åº¦ã€‚
    min_samples_split=2, #ã€€æ‹†åˆ†å†…éƒ¨èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°.
    min_impurity_decrease=0.0, #ã€€å¦‚æœè¯¥åˆ†è£‚å¯¼è‡´æ‚è´¨çš„å‡å°‘å¤§äºæˆ–ç­‰äºè¯¥å€¼ï¼Œåˆ™å°†åˆ†è£‚èŠ‚ç‚¹ã€‚
    min_impurity_split=None #ã€€æ ‘æœ¨å¢é•¿æ—©æœŸåœæ­¢çš„é—¨æ§›ã€‚
)
```

##### DecisionTreeRegressor

| æ–¹æ³•         | åŠŸèƒ½                 |
| ------------ | -------------------- |
| .fit(X, y)   | æ‹Ÿåˆæ¨¡å‹             |
| .predict(X)  | æ ¹æ®æ¨¡å‹é¢„æµ‹         |
| .score(X, y) | è¿”å›é¢„æµ‹çš„å†³å®šç³»æ•°R2 |

| å±æ€§                  | è¯´æ˜       |
| --------------------- | ---------- |
| .feature_importances_ | ç‰¹å¾é‡è¦æ€§ |
| .tree_                | å†³ç­–æ ‘å¯¹è±¡ |

## **åˆ†ç±»æ¨¡å‹**

### åˆ†ç±»é—®é¢˜æ¦‚è¿°å’ŒScikit-learnåˆ†ç±»æ¨¡å—

åˆ†ç±»æ˜¯ä¸€ç§å…¸å‹çš„æœ‰ç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œå…¶åº”ç”¨åœºæ™¯å¦‚ï¼šä¿¡ç”¨é£é™©è¯„ä¼°ã€åŒ»å­¦è¯Šæ–­å’Œç”µå­é‚®ä»¶åˆ†ç±»ç­‰ã€‚

ä¿¡ç”¨é£é™©è¯„ä¼°ï¼šæ ¹æ®ç”¨æˆ·å†å²è¿˜æ¬¾ä¿¡æ¯é¢„æµ‹å…¶æœªæ¥æ˜¯å¦ä¼šè¿çº¦;
åŒ»å­¦è¯Šæ–­ï¼šæ ¹æ®è‚¿ç˜¤ç»†èƒçš„ç‰¹å¾è¿›è¡Œè‰¯æ€§å’Œæ¶æ€§çš„åˆ†ç±»;
ç”µå­é‚®ä»¶ï¼šæ ¹æ®é‚®ä»¶å†…å®¹å°†é‚®ä»¶å½’ç±»ï¼ˆæ­£å¸¸é‚®ä»¶/åƒåœ¾é‚®ä»¶ï¼‰

åˆ†ç±»çš„ä¸¤ä¸ªé˜¶æ®µï¼š
åˆ†ç±»å™¨è®­ç»ƒï¼šå³é€šè¿‡è®­ç»ƒæ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾æ¥å»ºç«‹åˆ†ç±»æ¨¡å‹
é¢„æµ‹ï¼šåˆ©ç”¨åˆ†ç±»æ¨¡å‹å¯¹æ²¡æœ‰åˆ†ç±»æ ‡ç­¾çš„æ•°æ®è¿›è¡Œé¢„æµ‹åˆ†ç±»

**äºŒåˆ†ç±»é—®é¢˜è¯„ä»·æŒ‡æ ‡**

è®¸å¤šäºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œç”±äºç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡ï¼Œæ­£ç¡®ç‡æ— æ³•æœ‰æ•ˆè¯„ä»·åˆ†ç±»æ•ˆæœï¼Œéœ€è¦å€ŸåŠ©ä¸€äº›ç‰¹å®šçš„æŒ‡æ ‡æ¥è¯„ä»·æ¨¡å‹ã€‚

| æ··æ·†çŸ©é˜µ<br />(Confuslon Matrix) | é¢„æµ‹æ ‡ç­¾<br />1(æ­£ä¾‹) | é¢„æµ‹æ ‡ç­¾<br />0(è´Ÿä¾‹) |
| -------------------------------- | --------------------- | --------------------- |
| çœŸå®æ ‡ç­¾<br />1(æ­£ä¾‹)            | TPï¼ˆçœŸæ­£æ ·æœ¬æ•°é‡ï¼‰    | FNï¼ˆå‡è´Ÿæ ·æœ¬æ•°é‡ï¼‰    |
| çœŸå®æ ‡ç­¾<br />0(è´Ÿä¾‹)            | FPï¼ˆå‡æ­£æ ·æœ¬æ•°é‡ï¼‰    | TNï¼ˆçœŸè´Ÿæ ·æœ¬æ•°é‡ï¼‰    |

$$
æ­£ç¡®ç‡(accuracy) = \frac{TP+TN}{TN+FN+FP+TP}  \\
å¬å›ç‡(recall) = \frac{TP}{TP+FN} \\
ç²¾ç¡®ç‡(precision) = \frac{TP}{TP+FP} \\
F_1 = \frac{ï¼’\times ç²¾ç¡®ç‡ \times å¬å›ç‡ }{ç²¾ç¡®ç‡+å¬å›ç‡}
$$


**ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜è§£å†³æ–¹æ³•**

åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œä¸åŒçš„åˆ†ç±»é”™è¯¯ä¼šå¯¼è‡´ä¸åŒçš„ä»£ä»·ï¼Œå¦‚ï¼šç—…äººè¯¯è¯Šã€‚

åœ¨ç±»åˆ«ä¸å¹³è¡¡æ—¶å¯ä»¥ä¸ºä¸åŒç±»åˆ«çš„æ ·æœ¬è®¾ç½®ä¸åŒçš„æƒé‡ï¼ˆä»£ä»·ï¼‰ï¼Œè¯¥æ–¹æ³•ä¸ºä»£ä»·æ•æ„Ÿå­¦ä¹ çš„åŸºç¡€ã€‚

é€šå¸¸è®¾ç½®æ¯ä¸ªç±»åˆ«çš„æƒé‡ä¸è¯¥åˆ†ç±»åœ¨æ ·æœ¬ä¸­å‡ºç°çš„é¢‘ç‡æˆåæ¯”

**å¤šåˆ†ç±»ä»»åŠ¡**

â€œæ‹†åˆ†æ³•â€ å¯ä»¥è§£å†³å¤šåˆ†ç±»é—®é¢˜ï¼Œå³å°†å¤šåˆ†ç±»ä»»åŠ¡æ‹†ä¸ºè‹¥å¹²ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ï¼š

*   ä¸€å¯¹ä¸€(OvO)
    *   å°†Nä¸ªç±»åˆ«ä¸¤ä¸¤é…å¯¹ï¼Œäº§ç”ŸN(Nâˆ’1)/2ä¸ªåˆ†ç±»ç»“æœï¼Œæœ€ç»ˆç»“æœé€šè¿‡æŠ•ç¥¨äº§ç”Ÿã€‚
*   ä¸€å¯¹å…¶ä½™(OvR)
    *   æ¯æ¬¡å°†ä¸€ä¸ªç±»çš„æ ·ä¾‹ä½œä¸ºæ­£ä¾‹ã€æ‰€æœ‰å…¶ä»–ç±»çš„æ ·ä¾‹ä½œä¸ºåä¾‹è®­ç»ƒN ä¸ªåˆ†ç±»ã€‚è‹¥ä»…æœ‰ä¸€ä¸ªåˆ†ç±»å™¨é¢„æµ‹ä¸ºæ­£ç±»ï¼Œåˆ™å¯¹åº”çš„ç±»åˆ«æ ‡è®°ä½œä¸ºåˆ†ç±»ç»“æœï¼›è‹¥æœ‰å¤šä¸ªåˆ†ç±»å™¨é¢„æµ‹ä¸ºæ­£ç±»ï¼Œåˆ™é€‰æ‹©é¢„æµ‹ç½®ä¿¡åº¦æœ€å¤§çš„ç±»åˆ«æ ‡è®°ä½œä¸ºåˆ†ç±»ç»“æœã€‚
*   å¤šå¯¹å¤š(MvM)
    *   æ¯æ¬¡å°†è‹¥å¹²ä¸ªç±»ä½œä¸ºæ­£ç±»ï¼Œè‹¥å¹²ä¸ªå…¶ä»–ç±»ä½œä¸ºåç±»ï¼Œæœ€ç»ˆç»“æœé€šè¿‡æŠ•ç¥¨äº§ç”Ÿã€‚

#### **Scikit-learnæ¨¡å—ä¸­çš„ä¸»è¦åˆ†ç±»ç®—æ³•**

| **ç±»**                          | **è¯´æ˜**           |
| ------------------------------- | ------------------ |
| linear_model.LogisticRegression | é€»è¾‘å›å½’           |
| neighbors.KNeighborsClassifier  | Kè¿‘é‚»              |
| tree.DecisionTreeClassifier     | å†³ç­–æ ‘             |
| naive_bayes.BernoulliNB         | Bernoulli è´å¶æ–¯   |
| naive_bayes.GaussianNB          | Gaussian è´å¶æ–¯    |
| naive_bayes.MultinomialNB       | å¤šé¡¹å¼è´å¶æ–¯       |
| svm.LinearSVC                   | çº¿æ€§æ”¯æŒå‘é‡åˆ†ç±»å™¨ |
| svm.SVC                         | æ”¯æŒå‘é‡åˆ†ç±»å™¨     |

**é€šç”¨æ–¹æ³•**

| **æ–¹æ³•**             | **è¯´æ˜**                                                 |
| -------------------- | -------------------------------------------------------- |
| fit(X, y)            | è®­ç»ƒæ¨¡å‹                                                 |
| predict(X)           | è¿”å›Xä¸­æ ·æœ¬çš„é¢„æµ‹ç±»æ ‡ç­¾                                  |
| predict_log_proba(X) | è¿”å›Xä¸­æ ·æœ¬çš„å¯¹æ•°é¢„æµ‹ç±»åˆ«æ¦‚ç‡                            |
| predict_proba(X)     | è¿”å›Xä¸­æ ·æœ¬çš„é¢„æµ‹ç±»åˆ«æ¦‚ç‡                                |
| score(X, y)          | å¯¹Xè¿›è¡Œé¢„æµ‹å¾—åˆ°é¢„æµ‹æ ‡ç­¾ï¼Œå†ä¸çœŸå®æ ‡ç­¾yä½œæ¯”å¯¹ï¼Œè¿”å›æ­£ç¡®ç‡ |



### é€»è¾‘å›å½’

é€»è¾‘å›å½’æ˜¯é‡‡ç”¨å›å½’åˆ†æçš„æ€æƒ³æ¥è§£å†³åˆ†ç±»é—®é¢˜çš„æ¨¡å‹ï¼Œé€šå¸¸è§£å†³çš„æ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªLogisticå‡½æ•°ï¼Œå°†è¿ç»­å‹çš„è¾“å‡ºæ˜ å°„åˆ°(0,1)ä¹‹é—´ã€‚å³åœ¨çº¿æ€§å›å½’çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œå»ºç«‹äº†äºŒå…ƒé¢„æµ‹ç›®æ ‡ä¸åŸå§‹è¾“å…¥ä¹‹é—´çš„å…³ç³»ã€‚

è‹¥é¢„æµ‹ç›®æ ‡ $y_i \in \{1,âˆ’1\}$ ï¼Œåˆ™é€»è¾‘å›å½’æ¨¡å‹çš„åŸºæœ¬å½¢å¼ä¸ºï¼š $ P(y_iâ”‚x_i)=\frac{1}{1+e^{âˆ’y_iw^Tx_i}} $ã€‚

|                   Logistic å‡½æ•°                    |
| :------------------------------------------------: |
| <img src="image/Logistic.png" style="zoom:50%;" /> |



é€»è¾‘å›å½’çš„æ­£åˆ™åŒ–å¯ç¼“è§£æ¨¡å‹è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œé€šå¸¸åœ¨æŸå¤±å‡½æ•°ä¸Šæ·»åŠ æƒ©ç½šé¡¹L1èŒƒæ•° $||w||_1$ æˆ–æ·»åŠ L2èŒƒæ•° $||w||_2^2$ ã€‚Sklearnä¸­ï¼Œæ·»åŠ L1å’ŒL2èŒƒæ•°çš„é€»è¾‘å›å½’æ¨¡å‹ç›®æ ‡å‡½æ•°ï¼š
$$
\overset{}{\min \limits_{w}}||w||_1 + C \overset{n}{\sum \limits_{i=1}}logâ¡(expâ¡(âˆ’y_i(w^Tx_i))+1) \\
\overset{}{\min \limits_{w}} \frac{1}{2}||w||_2^2 + C \overset{n}{\sum \limits_{i=1}}logâ¡(expâ¡(âˆ’y_i(w^Tx_i))+1) 
$$

é€»è¾‘å›å½’ç®—æ³•çš„å¸¸ç”¨ä¼˜åŒ–æ–¹æ³•æœ‰ï¼šéšæœºå¹³å‡æ¢¯åº¦ä¸‹é™æ³•ã€åæ ‡è½´ä¸‹é™æ³•ã€ç‰›é¡¿æ³•å’Œæ‹Ÿç‰›é¡¿æ³•ç­‰ã€‚

##### LogisticRegressionç±»

| **å‚æ•°**     | **è¯´æ˜**                                                     |
| ------------ | ------------------------------------------------------------ |
| penalty      | æ·»åŠ æ­£åˆ™åŒ–é¡¹ï¼Œé»˜è®¤ä¸ºâ€œl2â€                                     |
| C            | æ­£åˆ™åŒ–å¼ºåº¦ï¼Œæ­£çš„æµ®ç‚¹æ•°ï¼Œå€¼è¶Šå°å¼ºåº¦è¶Šå¤§ï¼Œé»˜è®¤ä¸º1.0            |
| class_weight | è®¾ç½®å„ä¸ªç±»åˆ«çš„æƒé‡ï¼Œé»˜è®¤ä¸ºâ€œNoneâ€ï¼ˆæƒé‡ç›¸ç­‰ï¼‰                 |
| slover       | é€‰æ‹©ä¼˜åŒ–æ–¹æ³•ï¼Œé»˜è®¤ä¸ºâ€œliblinearâ€ï¼ˆåæ ‡è½´ä¸‹é™æ³•ï¼‰<br />è¿˜å¯é€‰æ‹©ï¼šâ€œlbfgsâ€ï¼ˆæ‹Ÿç‰›é¡¿æ³•çš„ä¸€ç§ï¼‰ã€â€œnewton-cgâ€ï¼ˆç‰›é¡¿æ³•çš„ä¸€ç§ï¼‰ã€â€œsagâ€ï¼ˆéšæœºå¹³å‡æ¢¯åº¦ä¸‹é™ï¼‰ |
| multi_class  | å¤šåˆ†ç±»ä»»åŠ¡æ‹†åˆ†ç­–ç•¥ï¼Œé»˜è®¤ä¸ºâ€œovrâ€                              |

å‚æ•°class_weight

*   æ ·æœ¬ç±»åˆ«ä¸å¹³è¡¡ä¸”è¯¯åˆ†ç±»ä»£ä»·è¾ƒé«˜ï¼Œå¹³è¡¡ç±»åˆ«æƒé‡ï¼Œæå‡å°‘æ•°ç±»åˆ†ç±»æ•ˆæœ
*   è®¾ç½®ä¸ºâ€œbalancedâ€ï¼Œè‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡ï¼š$ ç¬¬iç±»æƒé‡ = \frac{æ ·æœ¬æ€»æ•°}{ç±»çš„ä¸ªæ•° Ã— ç¬¬iç±»æ ·æœ¬çš„ä¸ªæ•°} $ 
*   ä¾‹ï¼šæ ·æœ¬æ€»æ•°ä¸º100ï¼Œ1ç±»æœ‰20ä¸ªï¼Œ0ç±»æœ‰80ä¸ªï¼Œåˆ™è°ƒæ•´å1ç±»ä¸0ç±»çš„æƒé‡æ¯”ä¸º
                         100/2 Ã—20 :100/(2Ã—80)=2.5 :0.625=4 :1ï¼ˆåŸå§‹æƒé‡æ¯”ä¸º1ï¼š4ï¼‰
*   å¯è‡ªè¡Œè®¾ç½®ç±»åˆ«æƒé‡ï¼Œä»¥å­—å…¸å½¢å¼ä¼ å…¥ï¼Œå½¢å¦‚ï¼š{1 : 4, 0 : 1}

å»ºç«‹å¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹

```python
class sklearn.linear_model.LogisticRegression
LogisticRegression(penalty='l2', dual=False, tol=0.0001, 
                   C=1.0, fit_intercept=True, intercept_scaling=1, 
                   class_weight=None, random_state=None, solver='lbfgs', 
                   max_iter=100, multi_class='auto', verbose=0, 
                   warm_start=False, n_jobs=None, l1_ratio=None)

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=10)  ## æ¨¡å‹æ„å»º
# å½“ä¼˜åŒ–ç®—æ³•ä¸ºâ€liblinearâ€æ—¶ï¼Œéœ€è¦è®¾ç½®éšæœºç§å­
clf.fit(train_x, train_y)  ## æ¨¡å‹è®­ç»ƒ

```

è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½

```python
# æ¨¡å‹é¢„æµ‹
y_pred = clf.predict(test_x)
print("åˆ†ç±»æ­£ç¡®ç‡ï¼š",round(clf.score(test_x, test_y),4))
from sklearn.metrics import classification_report
# è¾“å‡ºä¸»è¦åˆ†ç±»æŒ‡æ ‡çš„æ–‡æœ¬æŠ¥å‘Š
print(classification_report(test_y, y_pred))
''' è¿è¡Œç»“æœï¼š
åˆ†ç±»æ­£ç¡®ç‡ï¼š0.8555
	precision	recall	f1-score	support
0	0.82	0.72	0.77	114
1	0.87	0.92	0.90	232
avg/total	0.85	0.86	0.85	346
'''
```

ç»˜åˆ¶æ··æ·†çŸ©é˜µçš„çƒ­åŠ›å›¾

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

## è®¾ç½®æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡
sns.set(font='SimHei')


## ç»˜åˆ¶çƒ­åŠ›å›¾
ax = sns.heatmap(confusion_matrix(test_y, y_pred), annot=True, fmt='d', 
                 xticklabels=["æ»¡æ„(0)","ä¸æ»¡æ„(1)"],
                 yticklabels=["æ»¡æ„(0)","ä¸æ»¡æ„(1)"])
                     
ax.set_ylabel('çœŸå®')
ax.set_xlabel('é¢„æµ‹')
ax.set_title('æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾')
# æ³¨æ„ï¼šè¿™é‡Œ0ä»£è¡¨æ»¡æ„ï¼Œ1ä»£è¡¨ä¸æ»¡æ„ï¼Œä¸å‰é¢ä»‹ç»æ··æ·†çŸ©é˜µçš„0ã€1ç›¸åã€‚
```

![](image/æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾.png)

å¹³è¡¡æ ·æœ¬ç±»åˆ«æƒé‡

```python
# æ¨¡å‹æ„å»ºã€æ‹Ÿåˆå¹¶é¢„æµ‹ï¼Œè®¾ç½®ç±»åˆ«æƒé‡ä¸ºâ€œbalancedâ€ 
clf = LogisticRegression(random_state=10, class_weight ='balanced')
clf.fit(train_x, train_y)
y_pred = clf.predict(test_x)
# åˆ†ç±»æ­£ç¡®ç‡
print("åˆ†ç±»æ­£ç¡®ç‡ï¼š",round(clf.score(test_x, test_y),4))
# è¾“å‡ºä¸»è¦åˆ†ç±»æŒ‡æ ‡çš„æ–‡æœ¬æŠ¥å‘Š
print(classification_report(test_y, y_pred))
''' è¿è¡Œç»“æœ:
åˆ†ç±»æ­£ç¡®ç‡ï¼š0.8468
	precision	recall	f1-score	support
0	0.73	0.86	0.79	114
1	0.92	0.84	0.88	232
avg/total	0.86	0.85	0.85	346
'''
```

è§‚å¯Ÿå¹¶åˆ†æç»“æœ

*   æ›´å¤šçš„æ»¡æ„å®¢æˆ·è¢«åˆ†ç±»æ­£ç¡®ï¼Œä½†ä½œä¸ºä»£ä»·ï¼Œä¸æ»¡æ„å®¢æˆ·çš„åˆ†ç±»æ­£ç¡®ç‡é™ä½
*   è°ƒç”¨å±æ€§ coef\_ å’Œ intercep\_ å¯ä»¥æŸ¥çœ‹æ¨¡å‹ç³»æ•°å’Œæˆªè·é¡¹

### Kè¿‘é‚»

ç®—æ³•æµç¨‹

1.  ç¡®å®šKçš„å¤§å°å’Œè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆè·ç¦»ï¼‰çš„æ–¹æ³•
2.  ä»è®­ç»ƒæ ·æœ¬ä¸­æ‰¾åˆ°Kä¸ªä¸æµ‹è¯•æ ·æœ¬â€œæœ€è¿‘â€ çš„æ ·æœ¬
    *  é€šè¿‡ä½•ç§æ–¹æ³•å¯»æ‰¾æµ‹è¯•æ ·æœ¬çš„è¿‘é‚»ï¼Œå³å¦‚ä½•è®¡ç®—æ ·æœ¬ä¹‹é—´çš„è·ç¦»æˆ–ç›¸ä¼¼åº¦ï¼Ÿå¦‚ä½•é€‰æ‹©ğ‘˜çš„å¤§å°æ‰èƒ½è¾¾åˆ°æœ€å¥½çš„é¢„æµ‹æ•ˆæœï¼Ÿ
3.  æ ¹æ®è¿™Kä¸ªè®­ç»ƒæ ·æœ¬çš„ç±»åˆ«ï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨çš„æ–¹å¼æ¥ç¡®å®šæµ‹è¯•æ ·æœ¬çš„ç±»åˆ«

å¸¸è§è·ç¦»

![](image/å¸¸è§è·ç¦».png)

å³å›¾ä¸­ï¼Œå½“K=3å’ŒK=6æ—¶ï¼Œæ ·æœ¬bä¼šè¢«åˆ†åˆ°ä¸åŒçš„ç±»ä¸­

Kè¶Šå°ï¼Œæ¨¡å‹åå·®å°ï¼Œæ–¹å·®å¤§
Kè¶Šå¤§ï¼Œæ¨¡å‹åå·®å¤§ï¼Œæ–¹å·®å°

åŠ æƒKè¿‘é‚»

*   å¯¹æ‰¾å‡ºçš„Kä¸ªâ€œæœ€è¿‘â€ æ ·æœ¬ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬åˆ†é…ä¸€ä¸ªæƒé‡ï¼Œä½¿è·ç¦»è¾ƒè¿‘çš„æ ·æœ¬çš„æƒé‡è¾ƒå¤§ã€‚
*   åå‡½æ•°ï¼šæƒé‡ä¸ºè·ç¦»çš„å€’æ•°ï¼Œè®¾Kä¸ªè¿‘é‚»æ ·æœ¬çš„æƒé‡ä¸ºw_1, w_2, â€¦,w_Kï¼Œä¸å¾…æµ‹æ ·æœ¬é—´çš„è·ç¦»ä¸ºd_1, d_2, â€¦,d_Kï¼Œåˆ™ï¼š$w_i=1/d_i+c     i=1,2,â€¦,K$
*   é«˜æ–¯æ ¸å‡½æ•°ï¼šæƒé‡é€šè¿‡é«˜æ–¯æ ¸æ¥è®¡ç®—ï¼š$w_i=k(x_i,x)=expâ¡{âˆ’||x_i âˆ’x||^2/2Ïƒ^2}$
*   è®¾æœ‰Mä¸ªç±»åˆ«ï¼Œè®°Kä¸ªè¿‘é‚»æ ·æœ¬ä¸­å±äºç±»mçš„æ ·æœ¬é›†åˆä¸º$m_jï¼Œj=1, 2, â€¦, M$ï¼Œåˆ™å¾…æµ‹æ ·æœ¬çš„é¢„æµ‹ç±»åˆ«ä¸ºæœ€å¤§çš„$P_j$æ‰€å¯¹åº”çš„ç±»åˆ«ï¼š$P_j = \frac{\overset{}{\sum \limits_{m_i \in m_j}} w_i}{\overset{K}{\sum \limits_{i=1}} w_i}$ 

##### KNeighborsClassifierç±»

| **å‚æ•°**    | **è¯´æ˜**                                                     |
| ----------- | ------------------------------------------------------------ |
| n_neighbors | è®¾å®šKçš„å–å€¼ï¼Œé»˜è®¤ä¸º5                                         |
| weights     | è®¾ç½®Kè¿‘é‚»æ ·æœ¬çš„æƒé‡ï¼Œé»˜è®¤ä¸ºâ€uniformâ€ï¼ˆæƒé‡ç›¸ç­‰ï¼‰ï¼Œå¯è¾“å…¥å‡½æ•°è‡ªè¡Œå®šä¹‰æƒé‡ |
| metric      | å®šä¹‰è·ç¦»åº¦é‡æ–¹å¼ï¼Œé»˜è®¤ä¸ºâ€minkowskiâ€ï¼ˆé—µå¯å¤«æ–¯åŸºè·ç¦»ï¼‰        |
| p           | é—µå¯å¤«æ–¯åŸºè·ç¦»çš„è¶…å‚æ•°ï¼Œé»˜è®¤ä¸º2ï¼ˆæ¬§å¼è·ç¦»ï¼‰                  |

Kè¿‘é‚»åˆ†ç±»æ¨¡å‹çš„æ„å»ºä¸æ‹Ÿåˆ

```python
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)

from sklearn.neighbors import KNeighborsClassifier

# æ¨¡å‹æ„å»ºä¸æ‹Ÿåˆ
neigh = KNeighborsClassifier()
neigh.fit(train_x, train_y)

```

ä¸åŒKå€¼å¯¹åˆ†ç±»æ­£ç¡®ç‡çš„å½±å“

```python
# è®¡ç®—ä¸åŒKå€¼ä¸‹çš„åˆ†ç±»æ­£ç¡®ç‡
K_scores = [KNeighborsClassifier(n_neighbors=k).fit(train_x,train_y)
           .score(test_x, test_y) for k in range(1,10)]

# ç»˜åˆ¶æŠ˜çº¿å›¾	
import matplotlib.pyplot as plt
plt.plot(range(1, 10), K_scores, 'r-.', linewidth=3)
plt.xlabel('K')
plt.ylabel('æ­£ç¡®ç‡')
plt.title('ä¸åŒKå€¼ä¸‹æ¨¡å‹é¢„æµ‹æ­£ç¡®ç‡')
```

![](image/ä¸åŒKå€¼.png)

åŠ æƒKè¿‘é‚»

```python
# è·ç¦»åŠ æƒä¸‹ä¸åŒKå€¼å¯¹åˆ†ç±»æ­£ç¡®ç‡çš„å½±å“
K_scores_weights = [KNeighborsClassifier(n_neighbors=k,weights='distance').fit(train_x, train_y).score(test_x, test_y) for k in range(1,10)]	
# æŒ‰è·ç¦»çš„å€’æ•°åˆ†é…æƒé‡
```



### å†³ç­–æ ‘

å†³ç­–æ ‘æ¨¡å‹å¯ä»¥ç”±æ ‘å½¢ç»“æ„è¡¨ç¤ºï¼Œç”±æ ¹ç»“ç‚¹ã€å†…ç»“ç‚¹å’Œå¶ç»“ç‚¹ç»„æˆï¼Œä¹Ÿå¯ä»¥åŸºäºè´ªå©ªç®—æ³•å¯¹ç‰¹å¾ç©ºé—´è¿›è¡Œå‚ç›´åˆ’åˆ†ã€‚

å¸¸è§çš„å†³ç­–æ ‘æ¨¡å‹

| æ¨¡å‹ | å¯å¤„ç†çš„ç‰¹å¾ç±»å‹ | ä¸çº¯åº¦åº¦é‡æ–¹å¼ | åˆ†å‰²åå­ç»“ç‚¹æ•°é‡ | ç›®æ ‡ç‰¹å¾ç±»å‹   |
| ---- | ---------------- | -------------- | ---------------- | -------------- |
| ID3  | ç¦»æ•£å‹           | ä¿¡æ¯å¢ç›Š       | å¤§äºç­‰äº2        | ç¦»æ•£å‹         |
| C4.5 | ç¦»æ•£å‹ã€è¿ç»­å‹   | ä¿¡æ¯å¢ç›Šç‡     | å¤§äºç­‰äº2        | ç¦»æ•£å‹         |
| CART | ç¦»æ•£å‹ã€è¿ç»­å‹   | Giniç³»æ•°       | ç­‰äº2            | ç¦»æ•£å‹ã€è¿ç»­å‹ |

å†³ç­–æ ‘çš„å‰ªæ

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå†³ç­–æ ‘å®Œå…¨ç”Ÿé•¿ï¼Œå¾ˆå®¹æ˜“é€ æˆè¿‡æ‹Ÿåˆï¼Œéœ€è¦è¿›è¡Œå‰ªææé«˜æ³›åŒ–èƒ½åŠ›ï¼Œå‰ªæåˆ†ä¸ºé¢„å‰ªæå’Œåå‰ªæ

*   é¢„å‰ªæï¼šåœ¨å†³ç­–æ ‘ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªç»“ç‚¹åœ¨åˆ’åˆ†å‰å…ˆè¿›è¡Œè¯„ä»·ï¼Œè‹¥å½“å‰ç»“ç‚¹åˆ’åˆ†ä¸èƒ½å¸¦æ¥æ³›åŒ–èƒ½åŠ›çš„æå‡ï¼Œåˆ™åœæ­¢åˆ’åˆ†å¹¶å°†å½“å‰ç»“ç‚¹è®°ä¸ºå¶ç»“ç‚¹ã€‚
*   åå‰ªæï¼šåœ¨å†³ç­–æ ‘ç”Ÿæˆåï¼Œè‡ªåº•å‘ä¸Šå¯¹å¶ç»“ç‚¹è¿›è¡Œè€ƒå¯Ÿï¼Œè‹¥è¯¥ç»“ç‚¹å¯¹åº”çš„å­æ ‘æ›¿æ¢ä¸ºå¶ç»“ç‚¹å¯ä»¥æé«˜å†³ç­–æ ‘çš„æ³›åŒ–èƒ½åŠ›ï¼Œåˆ™å°†è¯¥å­æ ‘æ›¿æ¢ä¸ºå¶ç»“ç‚¹ã€‚

å†³ç­–æ ‘æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§

æ ¹æ®Giniç³»æ•°çš„å‡å°é‡è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼šè®¾æœ‰X_1ï¼ŒX_2ï¼Œâ€¦â€¦ï¼ŒX_då…±dä¸ªç‰¹å¾ï¼Œç¬¬jä¸ªç‰¹å¾X_jåœ¨å†³ç­–æ ‘ä¸­å‡ºç°çš„ç»“ç‚¹é›†åˆä¸ºMï¼Œåˆ™X_jçš„é‡è¦æ€§ä¸ºï¼š
$$
VIM_j = \sum \limits_{m \in M} VIM_{jm} \\
VIM_{jm} = Gini_m - Gini_j - Gini_r
$$
å…¶ä¸­Gini_lå’ŒGini_ræ˜¯ç»“ç‚¹måˆ†è£‚åå·¦å³ç»“ç‚¹çš„Giniç³»æ•°ï¼Œæœ€ååšå½’ä¸€åŒ–å¤„ç†å³å¯å¾—åˆ°X_jçš„é‡è¦æ€§è¯„åˆ† $VIM_j = \frac{VIM_j}{\overset{d}{\sum \limits_{c=1}} VIM_c}$ ã€‚

##### DecisionTreeClassifierç±»

| **å‚æ•°**           | **è¯´æ˜**                                                     |
| ------------------ | ------------------------------------------------------------ |
| criterion          | ç»“ç‚¹ä¸çº¯åº¦çš„åº¦é‡æ–¹æ³•ï¼Œé»˜è®¤ä¸ºâ€œginiâ€ï¼ˆCARTï¼‰ï¼Œå¯è®¾ç½®ä¸ºâ€œentropyâ€ |
| max_depth          | è®¾ç½®æ ‘ç”Ÿé•¿çš„æœ€å¤§æ·±åº¦ï¼Œé»˜è®¤ä¸ºâ€œNoneâ€ï¼ˆä¸é™åˆ¶ï¼‰                 |
| min_samples_split  | å†…ç»“ç‚¹åˆ†è£‚æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º2                          |
| min_samples_leaf   | å¶ç»“ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º1                              |
| max_features       | å¯»æ‰¾æœ€ä¼˜åˆ†è£‚æ—¶æ‰€è€ƒè™‘çš„ç‰¹å¾ä¸ªæ•°ï¼Œé»˜è®¤ä¸ºâ€œNoneâ€ï¼ˆæ‰€æœ‰ç‰¹å¾ï¼‰     |
| min_impurity_split | æ ‘åœæ­¢ç”Ÿé•¿çš„é˜ˆå€¼ï¼Œå³å½“å‰ç»“ç‚¹çš„ä¸çº¯åº¦è¶…è¿‡é˜ˆå€¼æ—¶è¿›è¡Œåˆ†è£‚ï¼Œå¦åˆ™å˜ä¸ºå¶ç»“ç‚¹ |
| class_weight       | è®¾ç½®å„ä¸ªç±»åˆ«çš„æƒé‡ï¼Œé»˜è®¤ä¸ºâ€œNoneâ€ï¼ˆæƒé‡ä¸€è‡´ï¼‰                 |

å†³ç­–æ ‘æ¨¡å‹çš„å»ºç«‹å’Œè®­ç»ƒ

```python
class sklearn.tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)

from sklearn.tree import DecisionTreeClassifier

# æ¨¡å‹å»ºç«‹ä¸è®­ç»ƒ
DF_model = DecisionTreeClassifier(random_state=10)
DF_model.fit(train_x, train_y)	
```

ç‰¹å¾é‡è¦æ€§

```python
# è®­ç»ƒæ¨¡å‹åï¼Œè°ƒç”¨å±æ€§feature_importances_å¯ä»¥æŸ¥çœ‹å„ç‰¹å¾çš„é‡è¦æ€§
pd.Series(DF_model.feature_importances_, index=train_x.columns)
 .sort_values().plot(kind='barh', title='ç‰¹å¾é‡è¦æ€§')	
```

ä¸åŒæ ‘æ·±åº¦ä¸‹æ¨¡å‹çš„åˆ†ç±»æ­£ç¡®ç‡

```python
# é™åˆ¶æ ‘æ·±åº¦å¯¹åˆ†ç±»æ­£ç¡®ç‡çš„å½±å“
depth_grid = [1, 3, 5, 7, 9, None]
depth_scores = [DecisionTreeClassifier(random_state=10, max_depth=item).fit(train_x, train_y).score(test_x, test_y) for item in depth_grid]	
# ç»˜åˆ¶å›¾ç‰‡å¦‚ä¸‹
```

![](image/ä¸åŒæ·±åº¦.png)

å†³ç­–æ ‘æ¨¡å‹å¯è§†åŒ–

```python
## å¯¼å…¥å¿…è¦åº“
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO
from IPython.display import Image 
import pydotplus

## è¾“å‡ºå›¾ç‰‡åˆ°dotæ–‡ä»¶
export_graphviz(DF_model_3, out_file='tree.dot', feature_names=train_x.columns,rounded=True, filled=True,class_names=['acc', 'unacc'])

## ä½¿ç”¨dotæ–‡ä»¶æ„é€ å›¾
graph= pydotplus.graph_from_dot_file('tree.dot')
Image(graph.create_png())	
# æ¯ä¸ªç»“ç‚¹ä¸­æ˜¾ç¤ºä¸‹åˆ—ä¿¡æ¯ï¼šç‰¹å¾åˆ’åˆ†æ¡ä»¶ã€åŸºå°¼ç³»æ•°çš„å€¼ã€æ€»æ ·æœ¬é‡ã€æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬é‡
```

![](image/å†³ç­–æ ‘å¯è§†åŒ–.png)



### æœ´ç´ è´å¶æ–¯

æœ´ç´ è´å¶æ–¯ç®—æ³•æ˜¯åŸºäºè´å¶æ–¯å®šç†å’Œç‰¹å¾æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾çš„åˆ†ç±»æ–¹æ³•

*   è®¾ç‰¹å¾å‘é‡X={X_1,X_2,â€¦, X_d}æ˜¯dç»´éšæœºå‘é‡ï¼Œç±»æ ‡ç­¾Yâˆˆ{1,2,â€¦,c}ï¼š
    1.   å…ˆéªŒæ¦‚ç‡ $P(Y=k)ï¼Œk=1, 2,â€¦,c$
    2.   ç±»æ¡ä»¶æ¦‚ç‡ $P(Xâ”‚Y=k)=P(X_1,â€¦,X_dâ”‚Y=k)=P(X_1â”‚Y=k)P(X_2â”‚Y=k)â‹¯P(X_dâ”‚Y=k)ï¼Œk=1,2,â€¦,c $  
    3.   åéªŒæ¦‚ç‡ $P(Y=k|X)=\frac{P(X|Y=k)âˆ™P(Y=k)}{P(X)}$ 
    4.   æå¤§åŒ–åéªŒæ¦‚ç‡ $arg\max \limits_{k}P(Y=k|X) = arg \max \limits_{k}{P(Xâ”‚Y=k)P(Y=k)}$

æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘

*   ä¸ºé¿å…å…¶ä»–å±æ€§æºå¸¦çš„ä¿¡æ¯è¢«è®­ç»ƒé›†ä¸­æœªå‡ºç°çš„å±æ€§å€¼â€œæŠ¹å»â€ï¼Œåœ¨ä¼°è®¡æ¦‚ç‡å€¼æ—¶é€šå¸¸è¦è¿›è¡Œâ€œå¹³æ»‘â€ï¼ˆsmoothingï¼‰
    1.   è®¾$\alpha$ä¸ºæ‹‰æ™®æ‹‰æ–¯ä¿®æ­£ç³»æ•°ï¼ˆå¸¸è®¾ä¸º1ï¼‰ï¼Œå…ˆéªŒæ¦‚ç‡P(Y=k)çš„ä¼°è®¡ä¿®æ­£ä¸º
        $P(Y=k)=\frac{\overset{n}{\sum \limits_{i=1}}I(y_i=k)+\alpha}{n+\alpha c},k=1,2,â‹¯,c$ 
    2.   è®¾$X_j \in {1,2,â€¦, s,â€¦,m}$, ç±»æ¡ä»¶æ¦‚ç‡$P(X_j = s|Y=k)$çš„ä¼°è®¡ä¿®æ­£ä¸º$P(X_j=sâ”‚Y=k)=\frac{\overset{n}{\sum \limits_{i=1}}I(X_j=s,y_i=k)+\alpha}{\overset{n}{\sum \limits_{i=1}}I(y_i=k)+\alpha m}$

è¿ç»­å‹ç‰¹å¾çš„å¤„ç†

å½“ç‰¹å¾ä¸ºè¿ç»­å‹æ—¶ï¼Œæœ‰ä»¥ä¸‹ä¸¤ç§å¤„ç†æ–¹æ³•ï¼š

*   å¯ä»¥å°†è¿ç»­ç‰¹å¾ç¦»æ•£åŒ–ï¼Œäººä¸ºè®¾å®šåˆ†ç»„åŒºé—´ï¼Œç¦»æ•£åŒ–åå¯åˆ©ç”¨ä¹‹å‰çš„æ–¹æ³•ä¼°è®¡ç±»æ¡ä»¶æ¦‚ç‡
*   å‡å®šè¿ç»­å˜é‡æœä»æŸç§åˆ†å¸ƒï¼Œç”±åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦æ¥ä¼°è®¡ç±»æ¡ä»¶æ¦‚ç‡
    *   é€šå¸¸å‡å®šè¿ç»­å‹ç‰¹å¾æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå‚æ•°Î¼å’ŒÏƒ^2çš„å€¼åˆ†åˆ«ç”±è¿™ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®è®¡ç®—å¾—åˆ°ï¼›å¯¹äºæŸç±»Y=kï¼Œç‰¹å¾X_j=sæ—¶ç±»æ¡ä»¶æ¦‚ç‡çš„ä¼°è®¡ä¸ºï¼š$P(X_j=sâ”‚Y=k)=1/\sqrt{2Ï€Ïƒ}^{e^(sâˆ’Î¼)^2/2Ïƒ^2}$

##### naive_bayesæ¨¡å—

| **ç±»**          | åç§°             | è¯´æ˜                                                         |
| --------------- | ---------------- | ------------------------------------------------------------ |
| GaussianNB()    | é«˜æ–¯æœ´ç´ è´å¶æ–¯   | ä¸»è¦é¢å¯¹è¾“å…¥ä¸ºè¿ç»­å‹çš„æƒ…å†µï¼Œé€šè¿‡å‡è®¾é«˜æ–¯åˆ†å¸ƒè®¡ç®—ç±»æ¡ä»¶æ¦‚ç‡   |
| BernoulliNB()   | ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ | ä¸»è¦é¢å¯¹è¾“å…¥ä¸ºç¦»æ•£å‹çš„æƒ…å†µï¼Œå°†ç‰¹å¾äºŒå€¼åŒ–ï¼Œä¾‹å¦‚ä¸€ä¸ªè¯åœ¨æ–‡æœ¬ä¸­æ˜¯å¦å‡ºç°ï¼ˆ1ä¸ºæ˜¯ï¼Œ0ä¸ºå¦ï¼‰ |
| MultinomialNB() | å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ | ä¸»è¦é¢å¯¹è¾“å…¥ä¸ºç¦»æ•£å‹çš„æƒ…å†µï¼Œä¾‹å¦‚ä¸€ä¸ªè¯åœ¨ä¸åŒæ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•° |

å‚æ•°åˆ—è¡¨åŠè¯´æ˜

##### BernoulliNBç±»å’ŒMultinomialNBç±»ï¼š

| **å‚æ•°**                | **è¯´æ˜**                                                     |
| ----------------------- | ------------------------------------------------------------ |
| alpha                   | æ·»åŠ æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ç³»æ•°ï¼Œé»˜è®¤ä¸º1.0                              |
| binarizeï¼ˆBernoulliNBï¼‰ | äºŒå€¼åŒ–çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.0                                      |
| fit_prior               | æ˜¯å¦å­¦ä¹ å…ˆéªŒæ¦‚ç‡ï¼Œé»˜è®¤ä¸ºTrueï¼Œè‹¥ä¸ºFalseï¼Œåˆ™æ‰€æœ‰ç±»åˆ«å…ˆéªŒæ¦‚ç‡ä¸€è‡´ |
| class_prior             | è‡ªè¡Œè®¾å®šç±»åˆ«å…ˆéªŒæ¦‚ç‡ï¼Œé»˜è®¤ä¸ºNoneï¼ˆå­¦ä¹ å…ˆéªŒæ¦‚ç‡ï¼‰             |

##### GaussianNBç±»

| **å‚æ•°** | **è¯´æ˜**                                         |
| -------- | ------------------------------------------------ |
| priors   | è‡ªè¡Œè®¾å®šç±»åˆ«å…ˆéªŒæ¦‚ç‡ï¼Œé»˜è®¤ä¸ºNoneï¼ˆå­¦ä¹ å…ˆéªŒæ¦‚ç‡ï¼‰ |

æ„å»ºé«˜æ–¯æœ´ç´ è´å¶æ–¯æ¨¡å‹

```python
# æ‰€æœ‰ç‰¹å¾è™½ä¸ºç¦»æ•£å‹ï¼Œä½†å­˜åœ¨æ¬¡åºå…³ç³»ï¼Œå¯ä»¥ä½œä¸ºè¿ç»­å‹æ¥å¤„ç†ï¼Œé¦–å…ˆå»ºç«‹é«˜æ–¯æœ´ç´ è´å¶æ–¯æ¨¡å‹ã€‚
## è½½å…¥ç®—æ³•ç±»
from sklearn.naive_bayes import GaussianNB

# æ¨¡å‹æ„å»ºä¸è®­ç»ƒ
GNB= GaussianNB()
GNB.fit(train_x, train_y)

# è®­ç»ƒæ¨¡å‹åï¼Œè°ƒç”¨å±æ€§class_prior_å¯ä»¥æŸ¥çœ‹å…ˆéªŒæ¦‚ç‡
# è°ƒç”¨å±æ€§theta_å’Œsigma_å¯ä»¥æŸ¥çœ‹æ¯ä¸ªç±»åˆ«ä¸‹å„ç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®
```

å±æ€§çš„æŸ¥çœ‹

è®­ç»ƒæ¨¡å‹åï¼ŒMultinomialNBå’ŒBernoulliNBç±»å¯ä»¥æŸ¥çœ‹ä¸‹åˆ—å±æ€§ï¼š

| **å±æ€§**          | **è¯´æ˜**                         |
| ----------------- | -------------------------------- |
| class_log_prior_  | æ¯ä¸ªç±»åˆ«çš„å¯¹æ•°å¹³æ»‘å…ˆéªŒæ¦‚ç‡       |
| feature_log_prob_ | ä¸åŒç±»åˆ«ä¸‹å„ç‰¹å¾çš„å¯¹æ•°ç±»æ¡ä»¶æ¦‚ç‡ |
| class_count_      | ä¸åŒç±»åˆ«ä¸‹æ ·æœ¬çš„ä¸ªæ•°             |
| feature_count_    | ä¸åŒç±»åˆ«ä¸‹ç‰¹å¾å–å€¼çš„åŠ å’Œ         |



### æ”¯æŒå‘é‡æœº

[ç¬¬å››ç«  56é¡µ]([ç¬¬å››ç« ]åˆ†ç±»æ¨¡å‹.pptx)



##### LinearSVCç±»

| **å‚æ•°**     | **è¯´æ˜**                                                    |
| ------------ | ----------------------------------------------------------- |
| penalty      | æŒ‡å®šä¼˜åŒ–æ—¶ä½¿ç”¨çš„èŒƒæ•°ç±»å‹ï¼Œé»˜è®¤ä¸ºâ€l2â€ï¼Œå¯è®¾ç½®ä¸ºâ€l1â€          |
| loss         | æŒ‡å®šä¼˜åŒ–æ—¶çš„æŸå¤±å‡½æ•°ï¼Œé»˜è®¤ä¸ºâ€squared_hingeâ€ï¼ˆå¹³æ–¹åˆé¡µæŸå¤±ï¼‰ |
| C            | è®¾ç½®æƒ©ç½šç³»æ•°ï¼Œé»˜è®¤ä¸º1.0                                     |
| multi_class  | å¤„ç†å¤šåˆ†ç±»é—®é¢˜çš„ç­–ç•¥ï¼Œé»˜è®¤ä¸ºâ€ovrâ€ï¼ˆä¸€å¯¹å¤šï¼‰                 |
| class_weight | è®¾ç½®ä¸åŒç±»åˆ«çš„åˆ†ç±»æƒé‡                                      |

##### SVCç±»

| **å‚æ•°**     | **è¯´æ˜**                                                     |
| ------------ | ------------------------------------------------------------ |
| C            | è®¾ç½®æƒ©ç½šç³»æ•°ï¼Œé»˜è®¤ä¸º1.0                                      |
| kernel       | æ ¸å‡½æ•°çš„é€‰æ‹©ï¼Œé»˜è®¤ä¸ºâ€rbfâ€ï¼ˆé«˜æ–¯æ ¸ï¼‰è¿˜å¯é€‰æ‹©ï¼šâ€linearâ€ï¼ˆçº¿æ€§æ ¸ï¼‰â€polyâ€ï¼ˆå¤šé¡¹å¼æ ¸ï¼‰â€sigmoidâ€ï¼ˆSigmoidæ ¸ï¼‰ |
| degree       | è®¾ç½®å¤šé¡¹å¼æ ¸ä¸­çš„æ¬¡æ•°ï¼Œé»˜è®¤ä¸º3ï¼Œé€‰å…¶å®ƒæ ¸ä¼šè¢«å¿½ç•¥              |
| gamma        | è®¾ç½®é«˜æ–¯æ ¸ã€å¤šé¡¹å¼æ ¸å’ŒSigmoidæ ¸ä¸­çš„å‚æ•°ï¼Œé»˜è®¤ä¸ºâ€autoâ€ï¼Œå³ç‰¹å¾æ•°çš„å€’æ•° |
| probability  | æ˜¯å¦é‡‡ç”¨é¢„æµ‹æ¦‚ç‡æ¥ä¼°è®¡ï¼Œé»˜è®¤ä¸ºFalse                          |
| class_weight | è®¾ç½®ä¸åŒç±»åˆ«çš„åˆ†ç±»æƒé‡                                       |

æ„å»ºçº¿æ€§æ”¯æŒå‘é‡æœºæ¨¡å‹

```python
class sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)

# è½½å…¥ç®—æ³•ç±»
from sklearn.svm import LinearSVC

# æ¨¡å‹æ„å»ºä¸æ‹Ÿåˆ
lsvm = LinearSVC()
lsvm.fit(train_x, train_y)	
```

ä¸åŒæƒ©ç½šç³»æ•°ä¸‹æ¨¡å‹åˆ†ç±»æ­£ç¡®ç‡

```python
# è°ƒæ•´æƒ©ç½šç³»æ•°çš„å–å€¼
C_grid = [0.01, 0.1, 0.5, 1, 2, 5, 10]
C_scores = [LinearSVC(C=c, random_state=10).fit(train_x, train_y).score(test_x, test_y) for c in C_grid]
# Cè¶Šå¤§ï¼Œå¯¹é”™è¯¯çš„å®¹å¿ç¨‹åº¦å°±è¶Šå°ï¼Œåˆ†ç±»æ­£ç¡®ç‡ä¼šé™ä½
# è®­ç»ƒæ¨¡å‹åï¼Œè°ƒç”¨å±æ€§ coef_ å’Œ intercept_ å¯ä»¥æŸ¥çœ‹æ¨¡å‹ç³»æ•°å’Œæˆªè·é¡¹
```

æ„å»ºéçº¿æ€§æ”¯æŒå‘é‡æœºæ¨¡å‹

```python
class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)

# è½½å…¥ç®—æ³•ç±»
from sklearn.svm import SVC

# æ¨¡å‹æ„å»ºä¸æ‹Ÿåˆ
ksvm = SVC(random_state=10)
ksvm.fit(train_x, train_y)	
```

è°ƒæ•´æ ¸å‡½æ•°ä¸æ ¸å‡½æ•°çš„å‚æ•°

```python
# è°ƒæ•´æ ¸å‡½æ•°
kernel_grid = ['poly','sigmoid']
kernel_scores = [SVC(kernel=k, random_state=10).fit(train_x, train_y).score(test_x, test_y) for k in kernel_grid]

# è°ƒæ•´å¾„å‘åŸºæ ¸å‡½æ•°ä¸­çª—å®½å‚æ•°çš„å–å€¼
gamma_grid = [0.001, 0.1, 0.5, 1, 2, 5, 10]
gamma_scores = [SVC(gamma=g, random_state=10)
           .fit(train_x, train_y)
           .score(test_x, test_y) 
           for g in gamma_grid]
# è®­ç»ƒæ¨¡å‹åï¼Œè°ƒç”¨å±æ€§ support_vectors_ å¯ä»¥æŸ¥çœ‹æ‰€æœ‰çš„æ”¯æŒå‘é‡
```



## **èšç±»æ¨¡å‹**

### èšç±»é—®é¢˜å’Œclusteræ¨¡å—ä»‹ç»

â€œç‰©ä»¥ç±»èšï¼Œäººä»¥ç¾¤åˆ†â€ï¼Œå°†æ•°æ®é›†ä¸­ç›¸ä¼¼çš„æ ·æœ¬åˆ†åˆ°ä¸€ç»„ï¼Œæ¯ä¸ªç»„ç§°ä¸ºä¸€ä¸ªç°‡(cluster)ï¼Œç›¸åŒç°‡çš„æ ·æœ¬ä¹‹é—´ç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œä¸åŒç°‡çš„æ ·æœ¬ä¹‹é—´ç›¸ä¼¼åº¦è¾ƒä½ã€‚æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦é€šå¸¸æ˜¯é€šè¿‡è·ç¦»å®šä¹‰çš„ï¼Œè·ç¦»è¶Šè¿œï¼Œç›¸ä¼¼åº¦è¶Šä½ã€‚

åº”ç”¨ï¼šæ ¹æ®å…¨çƒå„åœ°è§‚æµ‹åˆ°çš„æ°”å€™ç‰¹å¾ï¼Œå°†å…¨çƒåˆ’åˆ†ä¸ºä¸åŒçš„æ°”å€™åŒºåŸŸï¼›æ ¹æ®å®¢æˆ·çš„è´­ç‰©è®°å½•å°†å®¢æˆ·åˆ†æˆä¸åŒçš„æ¶ˆè´¹ç¾¤ä½“ã€‚

å¸¸è§çš„èšç±»æ¨¡å‹

*   K-Means
*   å±‚æ¬¡èšç±»
*   DBSCAN

Scikit-learnèšç±»æ¨¡å—clusterçš„ä¸»è¦ç±»

| **ç±»**                          | **è¯´æ˜**     |
| ------------------------------- | ------------ |
| cluster.KMeans                  | K-Meansèšç±»  |
| cluster.AgglomerativeClustering | å±‚æ¬¡èšç±»     |
| cluster.DBSCAN                  | DBSCANèšç±»   |
| cluster.MeanShift               | å‡å€¼æ¼‚ç§»èšç±» |
| cluster.SpectralClustering      | è°±èšç±»       |



#### K-Meansèšç±»

ç®—æ³•åŸç†

1.  K-Meansçš„ç›®æ ‡æ˜¯è¦å°†æ ·æœ¬ç‚¹åˆ’åˆ†ä¸º K ä¸ªç°‡ï¼Œæ‰¾åˆ°æ¯ä¸ªç°‡çš„è´¨å¿ƒ $u_k (1â‰¤kâ‰¤K)$ ï¼Œå¹¶ä¸”æœ€å°åŒ–æ‰€æœ‰æ ·æœ¬ç‚¹åˆ°æ‰€å±ç°‡è´¨å¿ƒè·ç¦»çš„å¹³æ–¹å’Œ $J=\overset{n}{\sum \limits_{i=1}}\overset{K}{\sum \limits_{k=1}} r_{ik}||x_i-u_k||^2$ 
2.  å…¶ä¸­ $r_{ik} \in \{0,1\}$ï¼Œè‹¥æ ·æœ¬ $x_i$ è¢«åˆ’åˆ†åˆ°ç°‡ kä¸­ï¼Œé‚£ä¹ˆ$r_{ik}=1$ï¼Œå¯¹äºjâ‰ kï¼Œæœ‰ r_ij=0ï¼Œå³$\overset{K}{\sum \limits_{k=1}}r_{ik}=1$   

K-Meansç®—æ³•æµç¨‹

1.  è®¾ç½®åˆå§‹è´¨å¿ƒ
2.  Repeatï¼š
    å°†æ¯ä¸ªæ ·æœ¬æŒ‡æ´¾åˆ°æœ€è¿‘çš„è´¨å¿ƒ
    ç”¨æ¯ä¸ªç°‡çš„æ ·æœ¬å‡å€¼æ¥æ›´æ–°è´¨å¿ƒ

3.  ç›´åˆ°ç›®æ ‡å‡½æ•°å˜åŒ–ç¨‹åº¦å°äºé¢„è®¾çš„é˜ˆå€¼ï¼Œ  æˆ–è¿­ä»£æ¬¡æ•°å¤§äºé¢„è®¾çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œåœæ­¢è¿­ä»£

åˆå§‹è´¨å¿ƒçš„é€‰æ‹©

*   äººå·¥ç»™å‡ºåˆå§‹è´¨å¿ƒ
*   éšæœºé€‰æ‹©åˆå§‹è´¨å¿ƒ
    è¯¥æ–¹æ³•å®¹æ˜“é™·è¿›å±€éƒ¨æœ€ä¼˜è§£ï¼Œä¸å®¹æ˜“å¾—åˆ°æœ€ä¼˜çš„èšç±»ç»“æœ
*   æ”¹è¿›ä¸Šè¿°æƒ…å†µå¯é‡‡å–ä¸‹åˆ—æ–¹æ¡ˆï¼š
    å¤šæ¬¡è¿è¡ŒK-Meansç®—æ³•ï¼Œæ¯æ¬¡éšæœºé€‰æ‹©ä¸åŒçš„åˆå§‹è´¨å¿ƒï¼Œä»è¿™äº›ç»“æœä¸­é€‰æ‹©æœ€ä¼˜ï¼ˆç›®æ ‡å‡½æ•°æœ€å°ï¼‰çš„èšç±»ç»“æœ
    ä½¿ç”¨K-Means++ç®—æ³•æ¥åˆå§‹åŒ–è´¨å¿ƒï¼Œä¸»è¦æ€è·¯æ˜¯é€‰æ‹©çš„åˆå§‹è´¨å¿ƒå°½å¯èƒ½çš„äº’ç›¸è¿œç¦»

##### KMeansç±»

| **å‚æ•°**     | **è¯´æ˜**                                                     |
| ------------ | ------------------------------------------------------------ |
| n_clusters   | å¸Œæœ›åˆ†æˆçš„ç°‡æ•°ï¼Œé»˜è®¤ä¸º8                                      |
| init         | è®¾ç½®åˆå§‹è´¨å¿ƒçš„æ–¹æ³•{â€˜k-means++â€™ï¼ˆé»˜è®¤ï¼‰ï¼Œâ€˜randomâ€™æˆ–ndarrayï¼ˆè‡ªè¡Œè®¾å®šï¼‰ |
| tol          | ç›®æ ‡å‡½æ•°å˜åŒ–ç¨‹åº¦çš„é˜ˆå€¼ï¼Œå°äºé˜ˆå€¼åˆ™åœæ­¢è¿­ä»£ï¼Œé»˜è®¤ä¸º1e-4       |
| max_iter     | æœ€å¤§è¿­ä»£æ¬¡æ•°é˜ˆå€¼ï¼Œå¤§äºé˜ˆå€¼åˆ™åœæ­¢è¿­ä»£ï¼Œé»˜è®¤ä¸º300              |
| n_init       | éšæœºä½¿ç”¨ä¸åŒè´¨å¿ƒæ¥è¿è¡Œk-meansç®—æ³•çš„æ¬¡æ•°ï¼Œé»˜è®¤ä¸º10            |
| random_state | è®¾ç½®éšæœºç§å­                                                 |

æ„å»ºK-Meansèšç±»æ¨¡å‹

```python
class sklearn.cluster.KMeans(n_clusters=8, init=â€™k-means++â€™, n_init=10, max_iter=300, tol=0.0001, precompute_distances=â€™autoâ€™, verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm=â€™autoâ€™)

from sklearn.cluster import KMeans
# è®­ç»ƒæ¨¡å‹
model = KMeans(n_clusters=3,random_state=111).fit(auto_scaled)
# æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾
auto_label = model.labels_
# æ¯ä¸ªç°‡çš„è´¨å¿ƒ
auto_cluster = model.cluster_centers_

```

**è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½**

å¯ä»¥ä½¿ç”¨è®¸å¤šæ–¹æ³•æ¥è¯„ä¼°èšç±»çš„æ€§èƒ½ï¼Œè¿™é‡Œé‡‡ç”¨è½®å»“ç³»æ•°ï¼ˆSilhouette Coefficientï¼‰æ¥è¯„ä¼°

å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè½®å»“ç³»æ•°éœ€è¦è®¡ç®—ä¸¤ä¸ªå€¼a, b ï¼š
a :è¿™ä¸ªæ ·æœ¬å’ŒåŒç°‡å†…å…¶å®ƒæ ·æœ¬é—´çš„å¹³å‡è·ç¦»
b :è¿™ä¸ªæ ·æœ¬å’Œè·ç¦»ç¬¬äºŒè¿‘çš„ç°‡å†…æ‰€æœ‰æ ·æœ¬é—´çš„å¹³å‡è·ç¦»
å•ä¸ªæ ·æœ¬çš„è½®å»“ç³»æ•°è®¡ç®—å¦‚ä¸‹ï¼š$s= \frac{bâˆ’a}{maxâ¡(a,b)}$
æ‰€æœ‰æ ·æœ¬çš„è½®å»“ç³»æ•°åˆ™ç”±æ¯ä¸ªæ ·æœ¬çš„è½®å»“ç³»æ•°æ±‚å¹³å‡å¾—åˆ°

è½®å»“ç³»æ•°å–å€¼åœ¨-1åˆ°1ä¹‹é—´ï¼Œè¶Šæ¥è¿‘1è¯´æ˜èšç±»æ•ˆæœè¶Šå¥½

```python
from sklearn import metrics
labels = model.labels_

print("è½®å»“ç³»æ•°(Silhouette Coefficient): %0.4f"
      % metrics.silhouette_score(auto_scaled, labels))
'''
è½®å»“ç³»æ•°(Silhouette Coefficient): 0.3183
'''
```



#### MeanShiftèšç±»

ä¸»è¦æ€æƒ³æ˜¯è®¤ä¸ºä¸€ä¸ªç°‡çš„è´¨å¿ƒåº”è¯¥åœ¨ç°‡å†…ç‚¹æœ€å¯†é›†çš„åœ°æ–¹ï¼Œå¯»æ‰¾è´¨å¿ƒçš„æ–¹æ³•æ˜¯éšæœºé€‰æ‹©ä¸€ä¸ªç‚¹(seed)ï¼Œä»è¿™ä¸ªç‚¹å¼€å§‹ä¸æ–­çš„å°†è¿™ä¸ªç‚¹æ›´æ–°ä¸ºè¿™ä¸ªç‚¹é‚»åŸŸå†…ç‚¹çš„å‡å€¼ï¼Œè¿™æ ·ï¼Œè¿™ä¸ªç‚¹å°±ä¼šä¸æ–­çš„å‘é«˜å¯†åº¦åŒºåŸŸç§»åŠ¨ï¼Œç›´è‡³åˆ°è¾¾æœ€å¯†é›†å¤„

MeanShiftç®—æ³•åŸç†

1.  ä¸Šè¿°è¿‡ç¨‹æ˜¯ä»å¤šä¸ªèµ·å§‹ç‚¹(seed)å¼€å§‹çš„ï¼Œå¦‚æœå‡ ä¸ªä¸åŒçš„èµ·å§‹ç‚¹ç»è¿‡ç§»åŠ¨æœ€ç»ˆèšé›†åœ¨ä¸€èµ·ï¼Œåˆ™è®¤ä¸ºä»–ä»¬æ‰¾äº†åŒä¸€ä¸ªç°‡çš„è´¨å¿ƒï¼Œå°†ä»–ä»¬åˆå¹¶ï¼›
2.  è¿›ä¸€æ­¥ï¼Œåœ¨æ›´æ–°seedç‚¹çš„è¿‡ç¨‹ä¸­ï¼Œæ±‚å‡å€¼æ—¶è€ƒè™‘ä¸´è¿‘ç‚¹ä¸å½“å‰seedç‚¹çš„è·ç¦»ï¼Œä½¿ç”¨é«˜æ–¯æ ¸å‡½æ•°è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œå³è·ç¦»è¶Šè¿‘çš„ç‚¹æƒé‡è¶Šå¤§ï¼›
3.  é«˜æ–¯æ ¸éœ€è¦è¾“å…¥ä¸€ä¸ªå‚æ•°bandwidthï¼ˆçª—å®½ï¼‰ï¼Œç”¨æ¥æ§åˆ¶seedç‚¹é‚»åŸŸèŒƒå›´çš„å¤§å°ã€‚ï¼ˆç”±äºæ­£æ€åˆ†å¸ƒçš„3Ïƒå‡†åˆ™ï¼ˆå°†bandwidthä½œä¸ºÏƒï¼‰ï¼Œå‡ºäºè®¡ç®—ä¸Šçš„è€ƒè™‘ï¼Œå°†å’Œseedè·ç¦»è¶…è¿‡ä¸‰å€çš„bandwidthçš„ç‚¹cut offï¼Œç›¸å½“äºç”¨çª—å®½å‚æ•°bandwidthå¯ä»¥æ§åˆ¶é‚»åŸŸçš„å¤§å°ï¼‰

é€‰æ‹©èµ·å§‹ç‚¹ï¼ˆseedï¼‰æ—¶ï¼Œå‡ºäºè®¡ç®—ä¸Šçš„è€ƒè™‘ï¼Œé€šå¸¸ä¸ä½¿ç”¨å…¨éƒ¨æ ·æœ¬ç‚¹ä½œä¸ºseedï¼Œè€Œæ˜¯æ ¹æ®çª—å®½bandwidthä½œä¸ºç½‘æ ¼é—´éš”ï¼Œé€‰æ‹©èµ·å§‹ç‚¹ã€‚
é€‰å¥½ç°‡è´¨å¿ƒåï¼Œå°†æ ·æœ¬ç‚¹åˆ†åˆ°æœ€è¿‘çš„ç°‡

##### MeanShiftç±»

| **å‚æ•°**    | **åŠŸèƒ½**                           |
| ----------- | ---------------------------------- |
| bandwidth   | è®¾ç½®çª—å®½                           |
| seeds       | è®¾ç½®èµ·å§‹ç‚¹                         |
| bin_seeding | æ˜¯å¦é€‰æ‹©å…¨éƒ¨æ ·æœ¬ç‚¹ä½œä¸ºèµ·å§‹ç‚¹       |
| cluster_all | æ˜¯å¦å¯¹å…¨éƒ¨æ ·æœ¬è¿›è¡Œèšç±»ï¼Œé»˜è®¤ä¸ºTrue |

æ„å»ºMeanShiftæ¨¡å‹

```python
class sklearn.cluster.MeanShift(bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300)

from sklearn.cluster import MeanShift
#æ‹Ÿåˆæ¨¡å‹
model = MeanShift(bandwidth=2).fit(auto_scaled)
#æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾
auto_label = model.labels_
#æ¯ä¸ªç°‡çš„è´¨å¿ƒ
auto_cluster = model.cluster_centers_

# è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½

# æ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾
labels = model.labels_
print("è½®å»“ç³»æ•°(Silhouette Coefficient): %0.4f"
      % metrics.silhouette_score(df_trainx_new, labels))
'''
è½®å»“ç³»æ•°(Silhouette Coefficient):  0.2487
'''
```

ä¸åŒçª—å®½ä¸‹ç°‡çš„ä¸ªæ•°å’Œè½®å»“ç³»æ•°

```python
# ä¸åŒçª—å®½ä¸‹ç°‡çš„ä¸ªæ•°å’Œè½®å»“ç³»æ•°
bandwidth_grid = np.arange(1, 2.5, 0.2)
cluster_number = []
slt_score = []

for i in bandwidth_grid:
    model = MeanShift(bandwidth=i).fit(auto_scaled)
    cluster_number.append(len(np.unique(model.labels_)))
    slt_score.append(round(metrics.silhouette_score(auto_scaled, model.labels_), 4))
''' ä¸¾ä¾‹
çª—å®½	ç°‡çš„ä¸ªæ•°	è½®å»“ç³»æ•°	çª—å®½	ç°‡çš„ä¸ªæ•°	è½®å»“ç³»æ•°
1.0	141	0.3628	1.8	13	0.1567
1.2	118	0.3437	2.0	6	0.2487
1.4	108	0.3404	2.2	2	0.3705
1.6	22	0.1363	2.4	2	0.3710
'''
# éšç€çª—å®½çš„å¢åŠ ï¼Œç°‡çš„ä¸ªæ•°åœ¨ä¸æ–­é™ä½
```

#### å±‚æ¬¡èšç±»

å±‚æ¬¡èšç±»(hierarchical clustering)æœ‰ä¸¤ç§å¸¸ç”¨çš„å½¢å¼ï¼Œè‡ªé¡¶å‘ä¸‹å’Œè‡ªåº•å‘ä¸Š
å…¶ä¸­ï¼Œè‡ªåº•å‘ä¸Šçš„ä¸»è¦åšæ³•æ˜¯ï¼Œåœ¨å¼€å§‹æ—¶ï¼Œå°†æ¯ä¸ªæ ·æœ¬è§†ä¸ºä¸€ä¸ªç°‡ï¼Œé‡å¤çš„åˆå¹¶æœ€è¿‘çš„ä¸¤ä¸ªç°‡ï¼Œç›´åˆ°ç°‡çš„ä¸ªæ•°è¾¾åˆ°ç»™å®šå€¼
è°±ç³»å›¾å¯ä»¥ç”¨æ¥æè¿°ç°‡åˆå¹¶çš„è¿‡ç¨‹

![](image/è°±ç³»å›¾.png)

ç°‡é—´è·ç¦»çš„è®¡ç®—

å½“ä¸€ä¸ªç°‡å†…æ ·æœ¬æ•°å¤šäºä¸€ä¸ªæ—¶ï¼Œå¸¸ç”¨çš„åº¦é‡ç°‡é—´è·ç¦»çš„æ–¹å¼å¦‚ä¸‹ï¼š
completeï¼ˆå®Œæ•´è¿æ¥æ³•ï¼‰ï¼šä¸¤ç°‡ä¹‹é—´æœ€è¿œçš„æ ·æœ¬ä¹‹é—´çš„è·ç¦»
averageï¼ˆå¹³å‡è¿æ¥æ³•ï¼‰ï¼š ä¸¤ç°‡é—´æ‰€æœ‰æ ·æœ¬å¯¹çš„è·ç¦»çš„å¹³å‡å€¼
singleï¼ˆå•è¿æ¥æ³•ï¼‰ï¼šä¸¤ç°‡ä¹‹é—´æœ€è¿‘çš„æ ·æœ¬ä¹‹é—´çš„è·ç¦»
wardï¼ˆç¦»å·®å¹³æ–¹å’Œæ³•ï¼‰ï¼šä¸¤ç°‡çš„ç¦»å·®å¹³æ–¹å’Œä¹‹å’Œï¼ˆåˆå¹¶åçš„n*æ–¹å·®å¢é‡æœ€å°ï¼‰

![](image/è·ç¦».png)

##### AgglomerativeClusteringç±»

| **å‚æ•°**   | **è¯´æ˜**                                                     |
| ---------- | ------------------------------------------------------------ |
| n_clusters | å¸Œæœ›åˆ†æˆçš„ç°‡æ•°ï¼Œé»˜è®¤ä¸º2                                      |
| affinity   | æ ·æœ¬é—´çš„è·ç¦»è®¡ç®—æ–¹å¼ï¼Œé»˜è®¤ä¸ºâ€œeuclideanâ€ï¼ˆæ¬§å¼è·ç¦»ï¼‰ï¼Œè¿˜å¯é€‰æ‹©â€œmanhattanâ€ï¼ˆç»å¯¹å€¼è·ç¦»ï¼‰ã€â€œcosineâ€ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ |
| linkage    | ç°‡é—´çš„è·ç¦»è®¡ç®—æ–¹å¼ï¼Œé»˜è®¤ä¸ºâ€œwardâ€ï¼ˆç¦»å·®å¹³æ–¹å’Œï¼‰ï¼Œè¿˜å¯é€‰æ‹©â€œsingleâ€ï¼ˆå•è¿æ¥ï¼‰ã€â€œcompleteâ€ï¼ˆå®Œå…¨è¿æ¥ï¼‰å’Œâ€œaverageâ€ï¼ˆå¹³å‡è¿æ¥ï¼‰ |

https://sklearn.org/modules/generated/sklearn.cluster.AgglomerativeClustering.html

æ„å»ºå±‚æ¬¡èšç±»æ¨¡å‹

```python
class sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity=â€™euclideanâ€™, memory=None, connectivity=None, compute_full_tree=â€™autoâ€™, linkage=â€™wardâ€™, pooling_func=<function mean>)

from sklearn.cluster import AgglomerativeClustering
# è®­ç»ƒæ¨¡å‹
model = AgglomerativeClustering(n_clusters=3,linkage=â€˜averageâ€™).fit(auto_scaled)
# è¾“å‡ºæ¨¡å‹ç»“æœ  
auto_label = model.labels_

# ç»˜åˆ¶è°±ç³»å›¾
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']

#åˆ©ç”¨scipyä¸­pdist,linkage,dendrogramå‡½æ•°ç»˜åˆ¶è°±ç³»å›¾
#pdistå‡½æ•°è¿”å›è·ç¦»çŸ©é˜µï¼Œlinkageå‡½æ•°è¿”å›ä¸€ä¸ªndarrayå¯¹è±¡ï¼Œæè¿°äº†ç°‡åˆå¹¶çš„è¿‡ç¨‹
#dendrogramå‡½æ•°ç”¨æ¥ç»˜åˆ¶è°±ç³»å›¾
row_clusters = linkage(pdist(auto_scaled,metric='euclidean'),method='ward')
fig = plt.figure(figsize=(12,10))
#å‚æ•°på’Œå‚æ•°truncate_modeç”¨æ¥å°†è°±ç³»å›¾æˆªæ–­ï¼Œéƒ¨åˆ†ç»“ç‚¹çš„å­æ ‘è¢«å‰ªæï¼Œæ¨ªè½´æ˜¾ç¤ºçš„æ˜¯è¯¥ç»“ç‚¹åŒ…å«çš„æ ·æœ¬æ•°
row_dendr = dendrogram(row_clusters, p=50, truncate_mode='lastp',color_threshold=5)
plt.tight_layout()
plt.title('è°±ç³»å›¾', fontsize=15)
```

![](image/è°±ç³»å›¾å±•ç¤º.png)

#### DBSCAN

DBSCANç®—æ³•æ€æƒ³

1.  DBSCANä¸»è¦æ€æƒ³æ˜¯æŠŠæ ·æœ¬ç©ºé—´ä¸­ä¸åŒçš„é«˜å¯†åº¦åŒºåŸŸåˆ’åˆ†ä¸ºä¸åŒç°‡
2.  å¯¹äºä¸€ä¸ªæ ·æœ¬ç‚¹ï¼Œå¦‚æœåœ¨å®ƒçš„åŠå¾„ä¸ºÎµçš„è¶…çƒä½“ï¼ˆé‚»åŸŸï¼‰å†…çš„æ ·æœ¬ç‚¹ä¸ªæ•°å¤§äºN_minï¼Œåˆ™è®¤ä¸ºè¿™ä¸ªæ ·æœ¬ç‚¹åœ¨ä¸€ä¸ªé«˜å¯†åº¦åŒºåŸŸå†…ï¼Œç§°è¿™ä¸ªç‚¹ä¸ºæ ¸å¿ƒå¯¹è±¡
3.  å¯¹äºä¸¤ä¸ªéƒ½å¤„åœ¨é«˜å¯†åº¦åŒºåŸŸçš„æ ·æœ¬ç‚¹ï¼Œå¦‚æœä»–ä»¬ä¹‹é—´çš„è·ç¦»æ¯”è¾ƒæ¥è¿‘ï¼ˆå³äº’ç›¸åœ¨å¯¹æ–¹çš„é‚»åŸŸå†…ï¼‰ï¼Œåˆ™è®¤ä¸ºå®ƒä»¬åŒå±äºä¸€ä¸ªç°‡

1.ç›´æ¥å¯†åº¦å¯è¾¾(directly density-reachable)ã€‚ å¯¹äºæ•°æ®é›†X, å¦‚æœæ ·æœ¬ç‚¹xj åœ¨xi çš„Îµ-é‚»åŸŸå†…, å¹¶ä¸”xiä¸ºæ ¸å¿ƒå¯¹è±¡, é‚£ä¹ˆå¯¹è±¡xjä»å¯¹è±¡xiç›´æ¥å¯†åº¦å¯è¾¾;  
2.å¯†åº¦å¯è¾¾(density-reachable)ã€‚ ç»™å®šæ ·æœ¬åºåˆ—p1,p2,â‹¯,pm, ä»¤xi=p1ï¼Œxj = pm, å¦‚æœæ ·æœ¬pi ä»p_iâˆ’1 ç›´æ¥å¯†åº¦å¯è¾¾, é‚£ä¹ˆæ ·æœ¬xj ä»æ ·æœ¬xi å¯†åº¦å¯è¾¾. æ³¨æ„: å¯†åº¦å¯è¾¾æ˜¯å•å‘çš„, å¯†åº¦å¯è¾¾å³å¯å®¹çº³åœ¨åŒä¸€ç°‡ä¸­
3.å¯†åº¦ç›¸è¿(density-connected)ã€‚å¦‚æœå­˜åœ¨ä¸€ä¸ªæ ·æœ¬p, ä½¿å¾—p åˆ°xi å’Œxjéƒ½æ˜¯å¯†åº¦å¯è¾¾çš„, é‚£ä¹ˆxiå’Œxj å¯†åº¦ç›¸è¿ã€‚

![](image/DBSCAN.png)

DBSCANç®—æ³•æµç¨‹

1.  ä»ä¸€ä¸ªç‚¹å¼€å§‹ï¼Œå¯¹å…¶åŠå¾„ä¸ºÎµçš„é‚»åŸŸå†…çš„ç‚¹èµ‹äºˆåŒä¸€ä¸ªç°‡æ ‡è®°ï¼Œå¹¶å¯»æ‰¾é‚»åŸŸå†…çš„å…¶å®ƒæ ¸å¿ƒå¯¹è±¡
2.  å¯¹äºæ‰¾åˆ°çš„æ ¸å¿ƒå¯¹è±¡ï¼Œé‡å¤æ­¥éª¤1
3.  é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´åˆ°æ— æ³•æ‰¾åˆ°æ–°çš„é«˜å¯†åº¦ç‚¹ä¸ºæ­¢ï¼Œè®¤ä¸ºä¸€ä¸ªç°‡è¢«å®Œæ•´æ‰¾å‡º
4.  ä»æœªè¢«è®¿é—®è¿‡çš„ç‚¹å¼€å§‹ï¼Œå¯»æ‰¾ä¸‹ä¸€ä¸ªç°‡ï¼Œæœ€ç»ˆæ²¡æœ‰å‡ºç°åœ¨ä»»ä½•ç°‡å†…çš„ç‚¹è¢«æ ‡è®°ä¸ºå­¤ç«‹ç‚¹

![](image/image19.gif)

##### DBSCANç±»

| **å‚æ•°**    | **åŠŸèƒ½**                                            |
| ----------- | --------------------------------------------------- |
| eps         | é‚»åŸŸçš„åŠå¾„ï¼Œé»˜è®¤ä¸º0.5                               |
| min_samples | ç¡®å®šæ ¸å¿ƒå¯¹è±¡çš„é‚»åŸŸå†…çš„æœ€å°æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º5           |
| metric      | ç‚¹ä¹‹é—´çš„è·ç¦»åº¦é‡æ–¹å¼ï¼Œé»˜è®¤ä¸ºâ€œeuclideanâ€ï¼ˆæ¬§å¼è·ç¦»ï¼‰ |

æ„å»ºDBSCANèšç±»æ¨¡å‹

```python
class sklearn.cluster.DBSCAN(X, eps=0.5, min_samples=5, metric='minkowski', metric_params=None, algorithm='auto', leaf_size=30, p=2, sample_weight=None, n_jobs=None)

from sklearn.cluster import DBSCAN

# è®­ç»ƒæ¨¡å‹
model = DBSCAN(eps=1.5,
   min_samples=4).fit(auto_scaled)
# è¾“å‡ºæ¨¡å‹ç»“æœ
auto_label = model.labels_

# æ ¸å¿ƒå¯¹è±¡çš„ç´¢å¼•
model.core_sample_indices_

# è¾“å‡ºæ ¸å¿ƒå¯¹è±¡
model.components_
```

è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½

```python
# æ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾
labels = model.labels_

# æ ‡ç­¾ä¸­çš„ç°‡æ•°ï¼Œå¿½ç•¥å™ªå£°ç‚¹
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)

print('ç°‡æ•°: %d' % n_clusters_)
print("è½®å»“ç³»æ•°(Silhouette Coefficient): %0.4f"
      % metrics.silhouette_score(auto_scaled, labels))
'''
ç°‡æ•°ï¼š3
è½®å»“ç³»æ•°(Silhouette Coefficient): 0.1476
'''
```

### æ€»ç»“ï¼š

| **ç®—æ³•**  | **ä¼˜ç‚¹**                                                     | **ç¼ºç‚¹**                                                     |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| K-Means   | å®ç°ç®€å•ï¼Œç›´è§‚ï¼Œæ”¯æŒå¤šç§è·ç¦»è®¡ç®—æ–¹å¼                         | éœ€è¦æŒ‡å®šç°‡æ•°Kï¼Œä¸”æ•ˆæœä¾èµ–äºåˆå§‹è´¨å¿ƒçš„é€‰æ‹©ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚åŒæ—¶ä¸æ˜“å¤„ç†éâ€˜çƒâ€™çŠ¶æ•°æ®ï¼Œæ˜“å—ç¦»ç¾¤å€¼å½±å“ |
| MeanShift | ä¸éœ€è¦æŒ‡å®šç°‡çš„ä¸ªæ•°ï¼Œå¯ä»¥å¤„ç†éâ€˜çƒâ€™çŠ¶æ•°æ®                     | æ•ˆæœä¾èµ–äºå¸¦å®½çš„é€‰æ‹©                                         |
| å±‚æ¬¡èšç±»  | èƒ½äº§ç”Ÿä¸åŒå±‚çº§çš„èšç±»ç»“æœï¼Œæ›´åŠ çµæ´»                           | ç©ºé—´å’Œæ—¶é—´å¤æ‚åº¦è¦é«˜äºK-Meansèšç±»                            |
| DBSCAN    | ä¸éœ€è¦æŒ‡å®šç°‡çš„ä¸ªæ•°ï¼Œå¯ä»¥æ‰¾å‡ºä»»ä½•å½¢çŠ¶çš„ç°‡ï¼Œèƒ½æœ‰æ•ˆåˆ†è¾¨å™ªéŸ³æ ·æœ¬ | å¯¹å‚æ•°*ğ‘’ğ‘ğ‘ *å’Œmin*â¡_**ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ *çš„è®¾ç½®å¾ˆæ•æ„Ÿ                     |

## **æ¨¡å‹ä¼˜åŒ–ä¸å‚æ•°ä¼˜åŒ–**

### æ¨¡å‹è¯„ä»·æŒ‡æ ‡ä¸metricsæ¨¡å—

```python
# æ¨¡å‹æ„å»ºä¸æ¨¡å‹è¯„ä¼°
from sklearn.linear_model import LogisticRegression
# æ¨¡å‹æ„å»ºã€æ‹Ÿåˆå¹¶é¢„æµ‹ï¼Œè®¾ç½®ç±»åˆ«æƒé‡ä¸ºâ€œbalancedâ€ 
clf = LogisticRegression(random_state=10, class_weight ='balanced')
clf.fit(train_x, train_y)
y_pred = clf.predict(test_x)
# åˆ†ç±»æ­£ç¡®ç‡
print("åˆ†ç±»æ­£ç¡®ç‡ï¼š",round(clf.score(test_x, test_y),4))
'''
åˆ†ç±»æ­£ç¡®ç‡ï¼š0.8468
'''
```

**æ€è€ƒï¼šåœ¨sklearnä¸­é™¤äº†.scoreæ–¹æ³•ï¼Œè¿˜æœ‰å“ªäº›æ–¹æ³•å¯ä»¥è¯„ä¼°æ¨¡å‹å‘¢ï¼Ÿ**



#### åˆ†ç±»æŒ‡æ ‡

| **å‡½æ•°**                       | **è¯´æ˜**                                            |
| ------------------------------ | --------------------------------------------------- |
| metrics.accuracy_score         | è®¡ç®—æ­£ç¡®ç‡                                          |
| metrics.f1_score               | è®¡ç®— $F_1$ å€¼                                       |
| metrics.confusion_matrix       | è®¡ç®—æ··æ·†çŸ©é˜µ                                        |
| metrics.classification_report  | è¾“å‡ºåˆ†ç±»æŒ‡æ ‡çš„æ–‡æœ¬æŠ¥å‘Š                              |
| metrics.recall_score           | è®¡ç®—å¬å›ç‡                                          |
| metrics.precision_score        | è®¡ç®—ç²¾ç¡®ç‡                                          |
| metrics.roc_auc_score          | ç»™å®šé¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾ï¼Œç›´æ¥è¿”å›AUCå€¼               |
| metrics.roc_curve              | ç»™å®šé¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾ï¼Œè¿”å›FPRã€TPRå’Œé˜ˆå€¼          |
| metrics.auc                    | ç»™å®šæ¨ªçºµåæ ‡è½´å€¼ï¼Œå¦‚ï¼šFPRå’ŒTPRï¼Œè¿”å›AUCå€¼           |
| metrics.precision_recall_curve | ç»™å®šé¢„æµ‹æ¦‚ç‡å’ŒçœŸå®æ ‡ç­¾ï¼Œè¿”å›precisionã€recallå’Œé˜ˆå€¼ |
| metrics.hinge_loss             | è®¡ç®—åˆé¡µæŸå¤±                                        |
| metrics.log_loss               | è®¡ç®—å¯¹æ•°æŸå¤±                                        |

##### metrics.accuracy_score ï¼š

| **å‚æ•°**  | **è¯´æ˜**                                                     |
| --------- | ------------------------------------------------------------ |
| y_true    | çœŸå®æ ‡ç­¾                                                     |
| y_pred    | é¢„æµ‹æ ‡ç­¾                                                     |
| normalize | å¦‚æœä¸ºTrueï¼ˆé»˜è®¤ï¼‰ï¼Œåˆ™è¿”å›åˆ†ç±»æ­£ç¡®çš„æ¯”ä¾‹ï¼ˆæ­£ç¡®ç‡ï¼‰ï¼Œè‹¥ä¸ºFalseï¼Œåˆ™è¿”å›åˆ†ç±»æ­£ç¡®çš„æ ·æœ¬æ•°é‡ |

```python
# metrics.accuracy_scoreä¸.scoreæ–¹æ³•ç»“æœä¸€è‡´ï¼Œéƒ½å¯ä»¥è®¡ç®—æ­£ç¡®ç‡ï¼ˆaccuracyï¼‰
from sklearn.metrics import accuracy_score

print("åˆ†ç±»æ­£ç¡®ç‡ï¼š",round(accuracy_score(test_y ,y_pred),4))
'''
åˆ†ç±»æ­£ç¡®ç‡ï¼š 0.8468
'''
print("åˆ†ç±»æ­£ç¡®çš„æ ·æœ¬æ•°é‡ï¼š",round(accuracy_score(test_y ,y_pred, normalize=False),4))
'''
åˆ†ç±»æ­£ç¡®çš„æ ·æœ¬æ•°é‡ï¼š 293
'''
```

**äºŒåˆ†ç±»è¯„ä»·æŒ‡æ ‡**

è®¸å¤šäºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œç”±äºç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡ï¼Œéœ€è¦å€ŸåŠ©ä¸€äº›ç‰¹å®šçš„æŒ‡æ ‡æ¥è¯„ä»·æ¨¡å‹ã€‚

![](image/äºŒåˆ†ç±».png)

##### **precisionã€recallå’ŒF_1å€¼çš„å‚æ•°**

| **å‚æ•°**  | **è¯´æ˜**                   |
| --------- | -------------------------- |
| y_true    | çœŸå®æ ‡ç­¾                   |
| y_pred    | é¢„æµ‹æ ‡ç­¾                   |
| pos_label | æ­£ç±»çš„ç±»åˆ«æ ‡è®°ï¼Œé»˜è®¤ä¸º1    |
| average   | ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨äºå¤šåˆ†ç±»é—®é¢˜ |



|                   averageå‚æ•°â€”â€”å¤šåˆ†ç±»é—®é¢˜                    |||
| :----------------------------------------------------------: ||:-:|
| å¤šåˆ†ç±»é—®é¢˜å¯ä»¥å°†æ•°æ®çœ‹æˆå¤šä¸ªäºŒåˆ†ç±»é—®é¢˜çš„é›†åˆï¼Œ å‚æ•°averageæœ‰å¦‚ä¸‹é€‰é¡¹ï¼š<br />
â€˜binaryâ€™ï¼ˆé»˜è®¤ï¼‰ï¼šäºŒåˆ†ç±»é—®é¢˜<br />
â€˜microâ€™ï¼šå…ˆè®¡ç®—å„æ··æ·†çŸ©é˜µå¯¹åº”å…ƒç´ çš„å¹³å‡å€¼ï¼Œå³(TP) Ì…ã€ (FP) Ì…ã€ (TN) Ì…å’Œ(FN) Ì…ï¼Œå†è®¡ç®—è¯„ä»·æŒ‡æ ‡<br />
â€˜macroâ€™ï¼šå…ˆåœ¨å„æ··æ·†çŸ©é˜µä¸Šåˆ†åˆ«è®¡ç®—å‡ºè¯„ä»·æŒ‡æ ‡ï¼Œå†è®¡ç®—å…¶å¹³å‡å€¼<br />
â€˜weightedâ€™ï¼šåœ¨â€˜macroâ€™åŸºç¡€ä¸Šè€ƒè™‘ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼ŒåŠ ä¸Šæƒé‡è®¡ç®—è¯„ä»·æŒ‡æ ‡ |||



```python
# precisionã€recallå’ŒF_1å€¼çš„è®¡ç®—
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

print("åˆ†ç±»ç²¾ç¡®ç‡ï¼š",round(precision_score(test_y ,y_pred),4))
print("åˆ†ç±»å¬å›ç‡ï¼š",round(recall_score(test_y ,y_pred),4))
print("åˆ†ç±»F1å€¼ï¼š",round(f1_score(test_y ,y_pred),4))
'''
åˆ†ç±»ç²¾ç¡®ç‡ï¼š 0.9242
åˆ†ç±»å¬å›ç‡ï¼š 0.8405
åˆ†ç±»F1å€¼ï¼š 0.8804
'''
print("åˆ†ç±»F1å€¼ï¼š",round(f1_score(test_y ,y_pred, pos_label=0),4))
'''
åˆ†ç±»F1å€¼ï¼š 0.7871
'''
```

##### metrics.confusion_matrix

| **å‚æ•°** | **è¯´æ˜**                   |
| -------- | -------------------------- |
| y_true   | çœŸå®æ ‡ç­¾                   |
| y_pred   | é¢„æµ‹æ ‡ç­¾                   |
| labels   | æŒ‡å®šæ··æ·†çŸ©é˜µä¸­å‡ºç°å“ªäº›ç±»åˆ« |

```python
# ç»˜åˆ¶æ··æ·†çŸ©é˜µçš„çƒ­åŠ›å›¾
from sklearn.metrics import confusion_matrix
import seaborn as sns
## è®¾ç½®æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡
sns.set(font='SimHei')
## ç»˜åˆ¶çƒ­åŠ›å›¾
ax = sns.heatmap(confusion_matrix(test_y, y_pred), 
                 annot=True, fmt='d', 
                 xticklabels=["æ»¡æ„(0)","ä¸æ»¡æ„(1)"],
                 yticklabels=["æ»¡æ„(0)","ä¸æ»¡æ„(1)"])
                     
ax.set_ylabel('çœŸå®')
ax.set_xlabel('é¢„æµ‹')
ax.set_title('æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾')
```

##### metrics.classification_report

| **å‚æ•°**     | **è¯´æ˜**                 |
| ------------ | ------------------------ |
| y_true       | çœŸå®æ ‡ç­¾                 |
| y_pred       | é¢„æµ‹æ ‡ç­¾                 |
| labels       | æŒ‡å®šæŠ¥å‘Šä¸­å‡ºç°å“ªäº›ç±»åˆ«   |
| target_names | æŒ‡å®šæŠ¥å‘Šä¸­ç±»åˆ«æ˜¾ç¤ºçš„åç§° |



```python
# åˆ†ç±»æŒ‡æ ‡æ–‡æœ¬æŠ¥å‘Š
from sklearn.metrics import classification_report
print(classification_report(test_y, y_pred))
'''
	precision	recall	f1-score	support
0	0.73	0.86	0.79	114
1	0.92	0.84	0.88	232
avg/total	0.86	0.85	0.85	346
'''
print(classification_report(test_y, y_pred, labels=[0], target_names=['æ»¡æ„']))
'''
	precision	recall	f1-score	support
æ»¡æ„	0.73	0.86	0.79	114
avg/total	0.73	0.86	0.79	114
'''
```

##### P-Ræ›²çº¿

![](image/Pï¼R.png)

ç®—æ³•å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç±»æ—¶ï¼Œéƒ½ä¼šæœ‰ç½®ä¿¡åº¦ï¼Œå³è¡¨ç¤ºè¯¥æ ·æœ¬æ˜¯æ­£æ ·æœ¬çš„æ¦‚ç‡ï¼Œæ¯”å¦‚99%çš„æ¦‚ç‡è®¤ä¸ºæ ·æœ¬ï¼¡æ˜¯æ­£ä¾‹ï¼Œï¼‘ï¼…çš„æ¦‚ç‡è®¤ä¸ºæ ·æœ¬Bæ˜¯æ­£ä¾‹ã€‚é€šè¿‡é€‰æ‹©åˆé€‚çš„é˜ˆå€¼ï¼Œæ¯”å¦‚50%ï¼Œå¯¹æ ·æœ¬è¿›è¡Œåˆ’åˆ†ï¼Œæ¦‚ç‡å¤§äº50%çš„å°±è®¤ä¸ºæ˜¯æ­£ä¾‹ï¼Œå°äº50%çš„å°±æ˜¯è´Ÿä¾‹ã€‚
é€šè¿‡ç½®ä¿¡åº¦å°±å¯ä»¥å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œæ’åºï¼Œå†é€ä¸ªæ ·æœ¬çš„é€‰æ‹©é˜ˆå€¼ï¼Œåœ¨è¯¥æ ·æœ¬ä¹‹å‰çš„éƒ½å±äºæ­£ä¾‹ï¼Œè¯¥æ ·æœ¬ä¹‹åçš„éƒ½å±äºè´Ÿä¾‹ã€‚æ¯ä¸€ä¸ªæ ·æœ¬ä½œä¸ºåˆ’åˆ†é˜ˆå€¼æ—¶ï¼Œéƒ½å¯ä»¥è®¡ç®—å¯¹åº”çš„precisionå’Œrecallï¼Œé‚£ä¹ˆå°±å¯ä»¥ä»¥æ­¤ç»˜åˆ¶æ›²çº¿ã€‚

å°†æ ·æœ¬æŒ‰ç…§é¢„æµ‹ä¸ºæ­£ç±»çš„æ¦‚ç‡ä»å¤§åˆ°å°è¿›è¡Œæ’åˆ—ï¼Œé€ä¸ªæŠŠæ ·æœ¬ä½œä¸ºæ­£ä¾‹è¿›è¡Œé¢„æµ‹ï¼Œè®¡ç®—å½“å‰çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ã€‚

å¹³è¡¡ç‚¹  (Break-even Pointï¼Œç®€ç§° BEP) æ˜¯â€œæŸ¥å‡†ç‡=æŸ¥å…¨ç‡â€æ—¶çš„å–å€¼ï¼Œæ ¹æ®å›¾å¯çŸ¥ï¼Œæ¨¡å‹Açš„æ€§èƒ½æœ€å¥½ã€‚

##### metrics.precision_recall_curve

| **å‚æ•°**    | **è¯´æ˜**                     |
| ----------- | ---------------------------- |
| y_true      | çœŸå®æ ‡ç­¾                     |
| probas_pred | ä¾æ¬¡æŒ‡å®šæ¯ä¸ªæ ·æœ¬ä¸ºæ­£ç±»çš„æ¦‚ç‡ |
| pos_label   | æ­£ç±»çš„ç±»åˆ«æ ‡è®°               |

```python
# ç»˜åˆ¶P-Ræ›²çº¿
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

probs = clf.predict_proba(test_x)[:,0]
precision,recall,thresholds = 
precision_recall_curve(test_y,probs,pos_label=0)
#ç»˜åˆ¶P-Ræ›²çº¿
plt.plot(recall, precision)
plt.title('P-Ræ›²çº¿')
plt.xlabel('å¬å›ç‡')
plt.ylabel('ç²¾ç¡®ç‡')
plt.show()
'''ç»“æœå¦‚ä¸‹å›¾'''
```

![](image/PR.png)

##### ROCæ›²çº¿ä¸AUC

![](image/ROC&AUC.png)

##### ROCæ›²çº¿

 å…¨ç§°æ˜¯"å—è¯•è€…å·¥ä½œç‰¹å¾" (Receiver Operating Characteristic)
ç»˜åˆ¶è¿‡ç¨‹ï¼š

1.  é¦–å…ˆï¼Œå°†æµ‹è¯•é›†æ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡ä»å¤§åˆ°å°æ’åˆ—
2.  ç„¶åï¼Œå°†æ¯ä¸ªé¢„æµ‹æ¦‚ç‡ä½œä¸ºåˆ†ç±»é˜ˆå€¼ï¼Œè®¡ç®—åœ¨æ­¤é˜ˆå€¼ä¸‹çš„ä¸€ç»„TPRå’ŒFPR
3.  æœ€åï¼Œä½¿FPRä¸ºæ¨ªè½´ï¼Œ TPRä¸ºçºµè½´ï¼ŒæŒ‰é˜ˆå€¼æ’åˆ—é¡ºåºï¼Œåˆ©ç”¨æç‚¹æ³•ç»˜åˆ¶ROCæ›²çº¿

##### metrics.roc_curve

| **å‚æ•°**  | **è¯´æ˜**                     |
| --------- | ---------------------------- |
| y_true    | çœŸå®æ ‡ç­¾                     |
| y_score   | ä¾æ¬¡æŒ‡å®šæ¯ä¸ªæ ·æœ¬ä¸ºæ­£ç±»çš„æ¦‚ç‡ |
| pos_label | æ­£ç±»çš„ç±»åˆ«æ ‡è®°               |

```python
#ã€€ç»˜åˆ¶ROCæ›²çº¿
from sklearn.metrics import roc_curve

probs = clf.predict_proba(test_x)[:,1]
fpr,tpr,thresholds = roc_curve(test_y,probs)
	# è¿”å›å€¼ä¸ºä¸€ä¸ªå…ƒç»„ï¼Œå…¶å…ƒç´ åˆ†åˆ«ä¸ºROCæ›²çº¿çš„FPRåºåˆ—ã€TPRåºåˆ—å’Œé˜ˆå€¼åºåˆ—

plt.plot(fpr, tpr)
plt.xlabel('å‡æ­£ç‡')
plt.ylabel('çœŸæ­£ç‡')
plt.title('ROCæ›²çº¿')
plt.show()
'''ç»“æœå¦‚ä¸‹å›¾'''
```

![](image/ROC.png)

##### AUCæ›²çº¿

##### metrics.roc_auc_score

| **å‚æ•°** | **è¯´æ˜**                     |
| -------- | ---------------------------- |
| y_true   | çœŸå®æ ‡ç­¾                     |
| y_score  | ä¾æ¬¡æŒ‡å®šæ¯ä¸ªæ ·æœ¬ä¸ºæ­£ç±»çš„æ¦‚ç‡ |
| average  | ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨äºå¤šåˆ†ç±»é—®é¢˜   |

##### metrics.auc

| **å‚æ•°** | **è¯´æ˜**                |
| -------- | ----------------------- |
| x        | æŠ˜çº¿ä¸Šç‚¹çš„æ¨ªåæ ‡ï¼Œå¦‚FPR |
| y        | æŠ˜çº¿ä¸Šç‚¹çš„çºµåæ ‡ï¼Œå¦‚TPR |

metrics.auc: é€šç”¨æ–¹æ³•ï¼Œåˆ©ç”¨æ¢¯å½¢æ³•åˆ™è®¡ç®—æ›²çº¿ä¸‹çš„é¢ç§¯(AUCï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶ä»–)ã€‚
åŒºåˆ«ï¼šroc_auc_score æ˜¯é¢„æµ‹ROCæ›²çº¿ä¸‹çš„ AUCï¼Œåœ¨è®¡ç®—çš„æ—¶å€™è°ƒç”¨äº† AUCã€‚

```python
# è®¡ç®—AUCå€¼
## ä¸¤ç§æ–¹æ³•ç»“æœç›¸åŒï¼Œæ³¨æ„è¾“å…¥å‚æ•°çš„åŒºåˆ«
from sklearn.metrics import roc_auc_score
print(roc_auc_score(test_y,probs))
'''
0.9296355111917726
'''
from sklearn import metrics
print(metrics.auc(fpr,tpr))
'''
0.9296355111917726
'''
```

##### å¯¹æ•°æŸå¤±å‡½æ•°

å¯¹æ•°æŸå¤±(log loss)ï¼Œåˆç§°é€»è¾‘æŸå¤±(logistic loss)æˆ–äº¤å‰ç†µæŸå¤±(cross-entropy lossï¼‰ï¼Œå¸¸ç”¨äºè¯„ä¼°åˆ†ç±»æ¨¡å‹çš„æ¦‚ç‡è¾“å‡º

äºŒåˆ†ç±»é—®é¢˜å¯¹æ•°æŸå¤±å‡½æ•°å…¬å¼ä¸ºï¼š$-\frac{1}{n} \sum (y_i log p_i + (1-y_i)log(1-p_i))$ 

*   çœŸå®æ ‡ç­¾å–å€¼ä¸º{0ï¼Œ1}
*   y_iä¸ºç¬¬iä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«
*   p_iè¡¨ç¤ºç¬¬iä¸ªæ ·æœ¬ç±»åˆ«ä¸º1çš„æ¦‚ç‡

##### metrics.log_loss

| **å‚æ•°**  | **è¯´æ˜**                                                     |
| --------- | ------------------------------------------------------------ |
| y_true    | çœŸå®æ ‡ç­¾                                                     |
| y_pred    | é¢„æµ‹æ¦‚ç‡                                                     |
| normalize | é»˜è®¤ä¸ºTrueï¼Œè¿”å›æ ·æœ¬å¯¹æ•°æŸå¤±çš„å‡å€¼ï¼Œè‹¥ä¸ºFalseï¼Œè¿”å›å¯¹æ•°æŸå¤±çš„æ€»å’Œ |

```python
# è®¡ç®—å¯¹æ•°æŸå¤±
from sklearn.metrics import log_loss
print('æŸå¤±å‡½æ•°:',log_loss(test_y,probs))
'''
æŸå¤±å‡½æ•°: 0.3549618366673331
'''# æŸå¤±å‡½æ•°è¶Šå°ï¼Œæ¨¡å‹å°±è¶Šå¥½
print('æŸå¤±å‡½æ•°çš„æ€»å’Œ:',log_loss(test_y,probs,normalize=False))
'''
æŸå¤±å‡½æ•°çš„æ€»å’Œ: 122.81679548689726
'''

```

##### åˆé¡µæŸå¤±å‡½æ•°

åˆé¡µæŸå¤±ï¼ˆhinge lossï¼‰å¸¸è¢«ç”¨äºæœ€å¤§é—´éš”ç®—æ³•ä½œä¸ºSVMçš„æŸå¤±å‡½æ•°

æ ‡ç­¾å€¼y=Â±1ï¼Œé¢„æµ‹å€¼ $\hat{y} \in R$ ï¼Œå…¬å¼ä¸ºï¼š$â„inge(z)=maxâ¡\{0, 1âˆ’z\}ï¼Œ z=y \times \hat{y}$ 

##### metrics.hinge_loss

| **å‚æ•°**      | **è¯´æ˜**                             |
| ------------- | ------------------------------------ |
| y_true        | çœŸå®æ ‡ç­¾                             |
| pred_decision | ä½¿ç”¨decision_function æ–¹æ³•çš„é¢„æµ‹ç»“æœ |

```python
# è®¡ç®—åˆé¡µæŸå¤±
from sklearn.svm import SVC
from sklearn.metrics import hinge_loss
# SVMæ¨¡å‹æ„å»ºä¸æ‹Ÿåˆ # æ„å»ºéçº¿æ€§æ”¯æŒå‘é‡æœºæ¨¡å‹
ksvm = SVC(random_state=10)
ksvm.fit(train_x, train_y)
# è®¡ç®—åˆé¡µæŸå¤±
pred_decision = ksvm.decision_function(test_x)
print('åˆé¡µæŸå¤±ï¼š',hinge_loss(test_y,pred_decision))
'''
åˆé¡µæŸå¤±ï¼š 0.10316668105886984
''' 

```



#### å›å½’æŒ‡æ ‡

```python
# æ„å»ºçº¿æ€§å›å½’æ¨¡å‹
from sklearn.linear_model import LinearRegression
from sklearn import metrics
lr = LinearRegression()
lr.fit(train_x, train_y)
## LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
```

**æ€è€ƒï¼šå¦‚ä½•è¯„ä»·å›å½’æ¨¡å‹ï¼Ÿ**

| **æ–¹æ³•**                       | **è¯´æ˜**       |
| ------------------------------ | -------------- |
| metrics.mean_absolute_error    | å¹³å‡ç»å¯¹è¯¯å·®   |
| metrics.mean_squared_error     | å‡æ–¹è¯¯å·®       |
| metrics.mean_squared_log_error | å‡æ–¹å¯¹æ•°è¯¯å·®   |
| metrics.median_absolute_error  | ä¸­å€¼ç»å¯¹è¯¯å·®   |
| metrics.r2_score               | å†³å®šç³»æ•° $R^2$ |



æ€»å¹³æ–¹å’Œ(sum of squares for total)ï¼šSST
æ®‹å·®å¹³æ–¹å’Œ(sum of squares for error)ï¼šSSE
å›å½’å¹³æ–¹å’Œ(sum of squares for regression)ï¼šSSR

![](image/å›å½’æŒ‡æ ‡.png)

##### å›å½’æŒ‡æ ‡çš„é€šç”¨å‚æ•°

| **å‚æ•°** | **è¯´æ˜** |
| -------- | -------- |
| y_true   | çœŸå®å€¼   |
| y_pred   | é¢„æµ‹å€¼   |

```python
# è®¡ç®—å›å½’æŒ‡æ ‡
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import median_absolute_error
from sklearn.metrics import r2_score

#å›å½’æ•ˆæœè¯„ä¼°
pred_y = lr.predict(test_x)
pred_y_train = lr.predict(train_x)

print("è®­ç»ƒé›†çš„å‡æ–¹è¯¯å·®: ", round(mean_squared_error(train_y, pred_y_train),3))
print("è®­ç»ƒé›†çš„å¹³å‡ç»å¯¹è¯¯å·®: ", round(mean_absolute_error(train_y, pred_y_train),3))
print("è®­ç»ƒé›†çš„ä¸­å€¼ç»å¯¹è¯¯å·®: ", round(median_absolute_error(train_y, pred_y_train),3))
print("è®­ç»ƒé›†çš„å†³å®šç³»æ•°: ", round(metrics.r2_score(train_y, pred_y_train),3))
print("-"*30)
print("æµ‹è¯•é›†çš„å‡æ–¹è¯¯å·®: ", round(mean_squared_error(test_y, pred_y),3))
print("æµ‹è¯•é›†çš„å¹³å‡ç»å¯¹è¯¯å·®: ", round(mean_absolute_error(test_y, pred_y),3))
print("æµ‹è¯•é›†çš„ä¸­å€¼ç»å¯¹è¯¯å·®: ", round(median_absolute_error(test_y, pred_y),3))
print("æµ‹è¯•é›†çš„å†³å®šç³»æ•°: ", round(metrics.r2_score(test_y, pred_y_test),3))
'''
è®­ç»ƒé›†çš„å‡æ–¹è¯¯å·®:  37360229.973
è®­ç»ƒé›†çš„å¹³å‡ç»å¯¹è¯¯å·®:  4228.405
è®­ç»ƒé›†çš„ä¸­å€¼ç»å¯¹è¯¯å·®:  2583.366
è®­ç»ƒé›†çš„å†³å®šç³»æ•°:  0.751
------------------------------
æµ‹è¯•é›†çš„å‡æ–¹è¯¯å·®:  34655210.533
æµ‹è¯•é›†çš„å¹³å‡ç»å¯¹è¯¯å·®:  4207.363
æµ‹è¯•é›†çš„ä¸­å€¼ç»å¯¹è¯¯å·®:  2648.645
æµ‹è¯•é›†çš„å†³å®šç³»æ•°:  0.75
''' # æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½å¾—åˆ†ä¸åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°ç›¸å·®ä¸å¤§


```



#### èšç±»åˆ†ææŒ‡æ ‡

```python
# æ„å»ºKMeansèšç±»æ¨¡å‹
from sklearn.cluster import KMeans
# è®­ç»ƒæ¨¡å‹
model = KMeans(n_clusters=3,random_state=111).fit(auto_scaled)
```

| **æ–¹æ³•**                    | **è¯´æ˜**     |
| --------------------------- | ------------ |
| metrics.silhouette_score    | è½®å»“ç³»æ•°     |
| metrics.adjusted_rand_score | è°ƒæ•´å…°å¾·ç³»æ•° |

##### è½®å»“ç³»æ•°

![](image/è½®å»“.png)

##### metrics.silhouette_score

| **å‚æ•°**     | **è¯´æ˜**                                                     |
| ------------ | ------------------------------------------------------------ |
| X            | ç‰¹å¾æ•°ç»„ï¼Œæˆ–å½“metric=â€˜precomputedâ€™æ—¶ï¼ŒXä¸ºæ ·æœ¬é—´è·ç¦»çš„æ•°ç»„    |
| labels       | é¢„æµ‹æ ‡ç­¾                                                     |
| metric       | è®¡ç®—è·ç¦»çš„æŒ‡æ ‡ï¼Œå¸¸ç”¨æ–¹æ³•æœ‰ï¼šâ€˜euclideanâ€™ï¼ˆé»˜è®¤ï¼‰å’Œâ€˜manhattanâ€™ç­‰ |
| random_state | é€‰å–å­é›†æ—¶è®¾ç½®çš„éšæœºç§å­                                     |

```python
# è®¡ç®—ä¸åŒkå€¼çš„è½®å»“ç³»æ•°
# é€‰æ‹©ä¸åŒkå€¼æ¯”è¾ƒèšç±»æ•ˆæœ
for i in np.arange(2,7):
    model = KMeans(n_clusters=i,random_state=111).fit(auto_scaled)   
    labels = model.labels_
    print('è½®å»“ç³»æ•°(k=%d):'%(i),round(silhouette_score(auto_scaled,labels),3))
'''
è½®å»“ç³»æ•°(k=2): 0.415
è½®å»“ç³»æ•°(k=3): 0.318
è½®å»“ç³»æ•°(k=4): 0.254
è½®å»“ç³»æ•°(k=5): 0.25
è½®å»“ç³»æ•°(k=6): 0.201
'''
## è½®å»“ç³»æ•°çš„ç¼ºç‚¹
## è½®å»“ç³»æ•°è®¡ç®—ä¸€ä¸ªç°‡çš„ç´§è‡´åº¦ï¼Œå…¶å€¼è¶Šå¤§è¶Šå¥½ï¼Œä½†åœ¨å½¢çŠ¶å¤æ‚çš„æ•°æ®ä¸Šæ•ˆæœå¹¶ä¸ç†æƒ³
```

##### å…°å¾·æŒ‡æ•°ï¼ˆrand indexï¼‰

![](image/å…°å¾·.png)

è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼ˆadjusted rand indexï¼‰

å…°å¾·æŒ‡æ•°çš„ç¼ºç‚¹ï¼šåœ¨èšç±»ç»“æœéšæœºäº§ç”Ÿçš„æƒ…å†µä¸‹ï¼Œä¸èƒ½ä¿è¯ç³»æ•°æ¥è¿‘äº0

è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼ˆadjusted rand indexï¼‰ï¼š$ARI=\frac{RIâˆ’E(RI)}{max(RI)âˆ’E(RI)}$

ARIå–å€¼èŒƒå›´ä¸º[-1,1]ï¼Œè´Ÿæ•°ä»£è¡¨ç»“æœä¸å¥½ï¼Œè¶Šæ¥è¿‘äº1è¶Šå¥½

##### metrics.adjusted_rand_score

| **å‚æ•°**    | **è¯´æ˜** |
| ----------- | -------- |
| labels_true | çœŸå®æ ‡ç­¾ |
| labels_pred | é¢„æµ‹æ ‡ç­¾ |

##### è¯„ä¼°KMeanså’ŒDBSCANæ¨¡å‹

```python
# ä½¿ç”¨sklearn.datasetsé‡Œçš„make_moonså‡½æ•°ç”ŸæˆåŠç¯å½¢å›¾æ•°æ®ï¼Œå¹¶è¿›è¡Œæ ‡å‡†åŒ–
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN
from sklearn.metrics.cluster import adjusted_rand_score 
# ç”Ÿæˆéšæœºæ•°æ®
X, y = make_moons(n_samples=200, noise=0.06, random_state=1)
# å°†æ•°æ®ç¼©æ”¾æˆå¹³å‡å€¼ä¸º0ã€æ–¹å·®ä¸º1 
scaler = StandardScaler() 
scaler.fit(X)
X_scaled = scaler.transform(X)

## è®¡ç®—è°ƒæ•´å…°å¾·æŒ‡æ•°
# ç»˜åˆ¶ç°‡åˆ†é…ç»“æœå›¾å½¢
fig, axes = plt.subplots(1,2,figsize=(7, 3),subplot_kw={'xticks':(),'yticks':()}) 
# åˆ—å‡ºè¦ä½¿ç”¨çš„ç®—æ³•
algorithms = [KMeans(n_clusters=2),DBSCAN()]
for ax, algorithm in zip(axes,algorithms): 
    clusters = algorithm.fit_predict(X_scaled) 
    ax.scatter(X_scaled[:, 0],X_scaled[:, 1],c=clusters,cmap='Set3',s=20)
    ax.set_title("{} - ARI: {:.2f}".format(algorithm.__class__.__name__,adjusted_rand_score(y,clusters))) # æ³¨æ„ï¼šå…°å¾·æŒ‡æ•°ä¼ å…¥çš„æ˜¯çœŸå®æ ‡ç­¾


```



### æ¨¡å‹é€‰æ‹©ä¸model_selectionæ¨¡å—

##### æ•°æ®é›†åˆ’åˆ†

model_selection.train_test_splitå‡½æ•°å°†æ•°æ®é›†åˆ‡åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸¤ç±»

| **å‚æ•°**     | **è¯´æ˜**         |
| ------------ | ---------------- |
| *array       | ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®é›† |
| test_size    | æŒ‡å®šæµ‹è¯•é›†çš„å¤§å° |
| train_size   | æŒ‡å®šè®­ç»ƒé›†çš„å¤§å° |
| random_state | éšæœºç§å­         |

```python
# æµ‹è¯•é›†æ¯”ä¾‹ä¸º20%
## æ•°æ®é›†åˆ’åˆ†
from sklearn.model_selection import train_test_split

X = data.drop(['car_acceptability'], axis=1)
y = data['car_acceptability']
train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 10)
# ä¹Ÿå¯å°†train_sizeå‚æ•°è®¾ä¸º0.8
```





#### äº¤å‰éªŒè¯



##### æ•°æ®åˆ†ç±»å™¨

| **ç±»**                          | **è¯´æ˜**           |
| ------------------------------- | ------------------ |
| model_selection.KFold           | kæŠ˜äº¤å‰åˆ†ç±»å™¨      |
| model_selection.StratifiedKFold | åˆ†å±‚kæŠ˜äº¤å‰åˆ†ç±»å™¨  |
| model_selection.ShuffleSplit    | æ‰“ä¹±æ•°æ®é›†åå†åˆ’åˆ† |

##### Kfoldä¸StratifiedKFold

| **å‚æ•°**     | **è¯´æ˜**                         |
| ------------ | -------------------------------- |
| n_splits     | kæŠ˜æ•°ï¼Œæœ€å°ä¸º2                   |
| shuffle      | å¸ƒå°”å€¼ï¼Œè®¾ç½®åœ¨åˆ’åˆ†å‰æ˜¯å¦æ‰“ä¹±æ•°æ® |
| random_state | éšæœºç§å­                         |

| **æ–¹æ³•**     | **è¯´æ˜**                   |
| ------------ | -------------------------- |
| get_n_splits | è¿”å›åˆ’åˆ†è¿­ä»£æ¬¡æ•°ï¼Œå³kæŠ˜æ•°  |
| split        | åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† |

```python
# å¯è§†åŒ–åˆ’åˆ†ç»“æœ
## å¯¹æ¯”Kfoldä¸StratifiedKFoldåˆ†ç±»å™¨çš„åˆ’åˆ†ç»“æœï¼Œå„åˆ’åˆ†3æŠ˜ï¼Œè§‚å¯Ÿä¸åŒæ–¹æ³•çš„æ¯”ä¾‹ï¼š
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold

kf = KFold(n_splits=3,random_state=0)
skf = StratifiedKFold(n_splits=3,random_state=0)
kf.split(X,y)
skf.split(X,y)
```

![](image/K&S.png)

```python
# è®¾ç½®shuffleå‚æ•°
## å‚æ•°shuffle=Trueè¡¨ç¤ºå°†æ•°æ®é›†æ‰“ä¹±åå†åˆ’åˆ†ï¼Œæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
```

![](image/s.png)

##### model_selection.ShuffleSplit

| **å‚æ•°**     | **è¯´æ˜**                               |
| ------------ | -------------------------------------- |
| n_splits     | åˆ’åˆ†è¿­ä»£æ¬¡æ•°                           |
| test_size    | æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆæµ®ç‚¹æ•°ï¼‰æˆ–æ ·æœ¬æ•°é‡ï¼ˆæ•´æ•°ï¼‰ |
| train_size   | è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆæµ®ç‚¹æ•°ï¼‰æˆ–æ ·æœ¬æ•°é‡ï¼ˆæ•´æ•°ï¼‰ |
| random_state | éšæœºç§å­                               |

| **æ–¹æ³•**     | **è¯´æ˜**                   |
| ------------ | -------------------------- |
| get_n_splits | è¿”å›åˆ’åˆ†è¿­ä»£æ¬¡æ•°           |
| split        | åˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›† |

```python
# ä½¿ç”¨ShuffleSplitä¹Ÿä¼šå°†æ•°æ®æ‰“ä¹±åå†åˆ’åˆ†ï¼Œä¸”å¯ä»¥è®¾ç½®è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å¤§å°ï¼Œä¸äº¤å‰éªŒè¯åŒºåˆ«çš„æ˜¯ï¼šéšæœºåˆ’åˆ†ä¸èƒ½ä¿è¯æ¯æŠ˜éƒ½ä¸ç›¸åŒ
from sklearn.model_selection import ShuffleSplit
rs = ShuffleSplit(n_splits=3, test_size=.2, random_state=0)
rs.split(X,y)
rs.get_n_splits()
'''
3
'''
```

##### æ¨¡å‹éªŒè¯

| **å‡½æ•°**                          | **è¯´æ˜**                 |
| --------------------------------- | ------------------------ |
| model_selection.cross_val_score   | æ ¹æ®äº¤å‰éªŒè¯è®¡ç®—æ¨¡å‹åˆ†æ•° |
| model_selection.cross_val_predict | æ ¹æ®äº¤å‰éªŒè¯å¾—å‡ºé¢„æµ‹å€¼   |
| model_selection.learning_curve    | å­¦ä¹ æ›²çº¿                 |

##### model_selection.cross_val_score

| **å‚æ•°**  | **è¯´æ˜**                                                     |
| --------- | ------------------------------------------------------------ |
| estimator | æŒ‡å®šçš„æ¨¡å‹                                                   |
| X         | æ•°æ®é›†ä¸­çš„æ ·æœ¬é›†                                             |
| y         | çœŸå®æ ‡ç­¾                                                     |
| scoring   | æŒ‡å®šè¯„ä»·æŒ‡æ ‡çš„å­—ç¬¦ä¸²ï¼Œé»˜è®¤é‡‡ç”¨.scoreæ–¹æ³•ï¼Œå¯é€‰å‚æ•°ä¸ºï¼šâ€˜accuracyâ€™ã€â€˜f1â€™ã€â€˜loglossâ€™å’Œâ€˜mean_squared_errorâ€™ç­‰ |
| cv        | é»˜è®¤ä¸º3æŠ˜äº¤å‰åˆ†ç±»å™¨ï¼Œå¦‚æœä¸ºæ•´æ•°å³ä¸ºæŒ‡å®šçš„kå€¼ï¼Œä¹Ÿå¯ç›´æ¥æŒ‡å®škæŠ˜äº¤å‰åˆ†ç±»å™¨ |

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

print('Cross validation score is:â€™,cross_val_score(LogisticRegression(),X,y,cv=5))
'''
Cross validation score is: [0.84682081 0.84682081 0.86127168 0.89855072 0.77681159]
''' # 5æŠ˜äº¤å‰éªŒè¯åˆ™è¿”å›5ä¸ªæ­£ç¡®ç‡
```

##### å­¦ä¹ æ›²çº¿

å­¦ä¹ æ›²çº¿ï¼ˆlearning curveï¼‰æ˜¯å…³äºæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„é¢„æµ‹æ€§èƒ½æ›²çº¿æ¨ªè½´ä¸ºè®­ç»ƒé›†çš„æ ·æœ¬æ•°é‡ï¼Œçºµè½´ä¸ºäº¤å‰éªŒè¯çš„æ­£ç¡®ç‡ã€‚èƒ½å¤Ÿåˆ¤æ–­æ¨¡å‹çš„æ–¹å·®æˆ–åå·®æ˜¯å¦è¿‡é«˜ï¼Œä»¥åŠå¢å¤§è®­ç»ƒé›†æ˜¯å¦å¯ä»¥å‡å°è¿‡æ‹Ÿåˆã€‚ï¼ˆé€šè¿‡ç”»å‡ºä¸åŒè®­ç»ƒé›†å¤§å°æ—¶è®­ç»ƒé›†å’Œäº¤å‰éªŒè¯çš„å‡†ç¡®ç‡ï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œè¿›è€Œæ¥åˆ¤æ–­æ¨¡å‹æ˜¯å¦æ–¹å·®åé«˜æˆ–åå·®è¿‡é«˜ï¼Œä»¥åŠå¢å¤§è®­ç»ƒé›†æ˜¯å¦å¯ä»¥å‡å°è¿‡æ‹Ÿåˆã€‚ï¼‰

![](image/å­¦ä¹ æ›²çº¿.png)

1.  é«˜åå·®ï¼šè®­ç»ƒé›†ä¸éªŒè¯é›†æ”¶æ•›ï¼Œä½†æ˜¯ä¸¤è€…æ”¶æ•›åçš„æ­£ç¡®ç‡è¿œå°äºæˆ‘ä»¬çš„æœŸæœ›ï¼Œæ‰€ä»¥æ¨¡å‹å±äºæ¬ æ‹Ÿåˆé—®é¢˜ã€‚éœ€è¦å¢åŠ æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæ¯”å¦‚ï¼Œå¢åŠ ç‰¹å¾ã€å¢åŠ æ ‘çš„æ·±åº¦ã€å‡å°æ­£åˆ™é¡¹ç­‰ç­‰ï¼Œæ­¤æ—¶å†å¢åŠ æ•°æ®é‡æ˜¯ä¸èµ·ä½œç”¨çš„ã€‚
2.  é«˜æ–¹å·®ï¼šè®­ç»ƒé›†æ­£ç¡®ç‡é«˜äºæœŸæœ›å€¼ï¼ŒéªŒè¯é›†åˆ™ä½äºæœŸæœ›å€¼ï¼Œä¸¤è€…ä¹‹é—´æœ‰å¾ˆå¤§çš„é—´è·ï¼Œè¯¯å·®å¾ˆå¤§ï¼Œå¯¹äºæ–°çš„æ•°æ®é›†æ¨¡å‹é€‚åº”æ€§è¾ƒå·®ï¼Œæ¨¡å‹å±äºè¿‡æ‹Ÿåˆé—®é¢˜ã€‚æ‰€ä»¥éœ€è¦é™ä½æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæ¯”å¦‚å‡å°æ ‘çš„æ·±åº¦ã€å¢å¤§åˆ†è£‚èŠ‚ç‚¹æ ·æœ¬æ•°ã€å¢å¤§æ ·æœ¬æ•°ã€å‡å°‘ç‰¹å¾æ•°ç­‰ç­‰ã€‚
3.  ä¸€ä¸ªæ¯”è¾ƒç†æƒ³çš„å­¦ä¹ æ›²çº¿å›¾åº”å½“æ˜¯ï¼šä½åå·®ã€ä½æ–¹å·®ï¼Œå³æ”¶æ•›ä¸”è¯¯å·®å°ã€‚

![](image/å­¦ä¹ æ›²çº¿2.png)

##### model_selection.learning_curve

| **å‚æ•°**    | **è¯´æ˜**                                                     |
| ----------- | ------------------------------------------------------------ |
| estimator   | æŒ‡å®šçš„æ¨¡å‹                                                   |
| X           | è®­ç»ƒé›†                                                       |
| y           | è®­ç»ƒé›†å¯¹åº”çš„æ ‡ç­¾                                             |
| train_sizes | æŒ‡å®šè€ƒå¯Ÿæ•°æ®é›†çš„æ¯”ä¾‹ï¼ˆæµ®ç‚¹æ•°ï¼‰æˆ–æ•°é‡ï¼ˆæ•´æ•°ï¼‰                 |
| cv          | é»˜è®¤ä¸º3æŠ˜äº¤å‰åˆ†ç±»å™¨ï¼Œå¦‚æœä¸ºæ•´æ•°å³ä¸ºæŒ‡å®šçš„kå€¼ï¼Œä¹Ÿå¯ç›´æ¥æŒ‡å®škæŠ˜äº¤å‰åˆ†ç±»å™¨ |
| scoring     | æŒ‡å®šè¯„ä»·æŒ‡æ ‡çš„å­—ç¬¦ä¸²ï¼Œé»˜è®¤é‡‡ç”¨.scoreæ–¹æ³•ï¼Œå¯é€‰å‚æ•°ä¸ºï¼šâ€˜accuracyâ€™ã€â€˜f1â€™ã€â€˜loglossâ€™å’Œâ€˜mean_squared_errorâ€™ç­‰ |

```python
# å¯è§†åŒ–å­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve

estimator = LogisticRegression()
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
learning_curve(estimator, X, y,cv=cv,train_sizes=np.linspace(.1, 1.0, 5))
## æ¨¡å‹é€‰æ‹©LogisticRegressionï¼Œåˆ†ç±»å™¨é€‰æ‹©ShuffleSplitï¼Œè¯¥æ¨¡å‹çš„å­¦ä¹ æ›²çº¿å¦‚ä¸‹å›¾ 
```

![](image/å­¦ä¹ æ›²çº¿3.png)

#### å‚æ•°è°ƒä¼˜

å­¦ä¹ å™¨æ¨¡å‹ä¸­çš„ä¸¤ç§å‚æ•°ï¼š

*   æ¨¡å‹å‚æ•°ï¼šä»æ•°æ®ä¸­å­¦ä¹ ä¼°è®¡å¾—åˆ°ï¼Œå¦‚ï¼šçº¿æ€§å›å½’ä¸­çš„ç³»æ•°
*   æ¨¡å‹è¶…å‚æ•°ï¼šæ— æ³•ä»æ•°æ®ä¸­ä¼°è®¡ï¼Œéœ€æ ¹æ®ç»éªŒäººå·¥è®¾ç½®ï¼Œå¦‚ï¼škè¿‘é‚»ç®—æ³•ä¸­çš„kå€¼

å‚æ•°è°ƒä¼˜ï¼Œå³è°ƒæ•´è¶…å‚æ•°æ¥æå‡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œå¸¸ç”¨æ–¹æ³•ä¸ºï¼š

*   ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯ï¼ˆGridSearchCVï¼‰ï¼šä»¥ç©·ä¸¾çš„æ–¹å¼éå†æ‰€æœ‰å¯èƒ½çš„å‚æ•°ç»„åˆ
*   éšæœºæœç´¢äº¤å‰éªŒè¯ï¼ˆRandomizedSearchCVï¼‰ï¼šä¾æ®æŸç§åˆ†å¸ƒå¯¹å‚æ•°ç©ºé—´é‡‡æ ·ï¼Œéšæœºçš„å¾—åˆ°ä¸€äº›å€™é€‰å‚æ•°ç»„åˆæ–¹æ¡ˆ

##### ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯

![](image/ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯.png)

##### éšæœºæœç´¢äº¤å‰éªŒè¯

![](image/éšæœºæœç´¢äº¤å‰éªŒè¯.png)

##### å‚æ•°è°ƒä¼˜å¸¸ç”¨ç±»

| **ç±»**                             | **è¯´æ˜**         |
| ---------------------------------- | ---------------- |
| model_selection.GridSearchCV       | ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯ |
| model_selection.RandomizedSearchCV | éšæœºæœç´¢äº¤å‰éªŒè¯ |

##### model_selection.GridSearchCV

| **å‚æ•°**   | **è¯´æ˜**                                                     |
| ---------- | ------------------------------------------------------------ |
| estimator  | æŒ‡å®šçš„æ¨¡å‹                                                   |
| param_grid | å…³äºå‚æ•°åï¼ˆé”®ï¼‰å’Œå‚æ•°å–å€¼ï¼ˆå€¼ï¼‰çš„å­—å…¸æˆ–å­—å…¸çš„åˆ—è¡¨           |
| scoring    | æŒ‡å®šè¯„ä»·æŒ‡æ ‡çš„å­—ç¬¦ä¸²ï¼Œé»˜è®¤é‡‡ç”¨.scoreæ–¹æ³•ï¼Œå¯é€‰å‚æ•°ä¸ºï¼šâ€˜accuracyâ€™ã€â€˜f1â€™ã€â€˜loglossâ€™å’Œâ€˜mean_squared_errorâ€™ç­‰ |
| cv         | é»˜è®¤ä¸º3æŠ˜äº¤å‰åˆ†ç±»å™¨ï¼Œå¦‚æœä¸ºæ•´æ•°å³ä¸ºæŒ‡å®šçš„kå€¼ï¼Œä¹Ÿå¯ç›´æ¥æŒ‡å®škæŠ˜äº¤å‰åˆ†ç±»å™¨ |

| **å±æ€§**        | **è¯´æ˜**                                   |
| --------------- | ------------------------------------------ |
| cv_results_     | è¾“å‡ºä»¥å­—å…¸å½¢å¼å­˜å‚¨çš„æ¯ä¸ªå‚æ•°ç»„åˆçš„å¾—åˆ†æƒ…å†µ |
| best_estimator_ | è¾“å‡ºç­›é€‰å‡ºæ¥çš„æœ€ä½³æ¨¡å‹                     |
| best_score_     | æœ€ä½³æ¨¡å‹çš„æ€§èƒ½è¯„åˆ†                         |
| best_params_    | æœ€ä½³å‚æ•°ç»„åˆ                               |

| **æ–¹æ³•**        | **è¯´æ˜**                                     |
| --------------- | -------------------------------------------- |
| fit(X[,y])      | æ‰§è¡Œå‚æ•°ä¼˜åŒ–                                 |
| predict(X)      | ä½¿ç”¨ç­›é€‰çš„æœ€ä½³æ¨¡å‹é¢„æµ‹æ•°æ®                   |
| predict_prob(X) | ä½¿ç”¨ç­›é€‰çš„æœ€ä½³æ¨¡å‹é¢„æµ‹æ•°æ®ä¸ºå„ç±»åˆ«çš„æ¦‚ç‡     |
| score(X[,y])    | é€šè¿‡ç»™å®šçš„æ•°æ®é›†åˆ¤æ–­ç­›é€‰çš„æœ€ä½³æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ |

##### model_selection.RandomizedSearchCV

| **å‚æ•°**            | **è¯´æ˜**                                                     |
| ------------------- | ------------------------------------------------------------ |
| param_distributions | å…³äºå‚æ•°åï¼ˆé”®ï¼‰å’Œå‚æ•°åˆ†å¸ƒï¼ˆå€¼ï¼‰çš„å­—å…¸æˆ–å­—å…¸çš„åˆ—è¡¨ï¼Œé€šå¸¸ä½¿ç”¨scipy.statsæ¨¡å—ä¸­æä¾›çš„åˆ†å¸ƒï¼Œå¦‚ï¼šscipy.exponæŒ‡æ•°åˆ†å¸ƒå’Œscipy.uniformå‡åŒ€åˆ†å¸ƒç­‰ |
| n_iter              | æŒ‡å®šæ¯ä¸ªå‚æ•°é‡‡æ ·çš„æ•°é‡                                       |

```python
# ä½¿ç”¨ç½‘æ ¼æœç´¢é¢„æµ‹å®¢æˆ·æ±½è½¦æ»¡æ„åº¦
## æ„å»ºéçº¿æ€§æ”¯æŒå‘é‡æœºæ¨¡å‹é¢„æµ‹æ•°æ®ï¼Œå³ä½¿ç”¨SVCï¼Œè¯¥æ¨¡å‹ä¸­æœ‰ä¸¤ä¸ªé‡è¦å‚æ•°Cæƒ©ç½šç³»æ•°å’Œgammaæ ¸å®½åº¦ï¼Œç°å°è¯•ä¸åŒå–å€¼çš„ç»„åˆ
param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100]} 
print("Parameter grid:\n{}".format(param_grid))
'''
Parameter grid:
{'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100]}
'''

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
grid_search = GridSearchCV(SVC(), param_grid, cv=5)

grid_search.fit(train_x, train_y)
print("Test set score: {:.3f}".format(grid_search.score(test_x,
test_y)))
'''
Test set score: 0.983
'''
print("Best parameters: {}".format(grid_search.best_params_)) 
print("Best cross validation score: {:.3f}".format(grid_search.best_score_))
'''
Best parameters: {'C': 100, 'gamma': 0.1}
Best cross validation score: 0.993
'''

# ä½¿ç”¨éšæœºç½‘æ ¼æœç´¢é¢„æµ‹å®¢æˆ·æ±½è½¦æ»¡æ„åº¦
from sklearn.model_selection import RandomizedSearchCV
import scipy

param = {'C': range(1,200,1), 'gamma':scipy.stats.expon()} 
random_search = RandomizedSearchCV(SVC(), param, cv=5, n_iter=10)
random_search.fit(train_x, train_y)
print("Test set score: {:.3f}".format(random_search.score(test_x, test_y)))
print("Best parameters: {}".format(random_search.best_params_)) 
print("Best cross validation score: {:.3f}".format(random_search.best_score_))
'''
Test set score: 1.000
Best parameters: {'C': 195, 'gamma': 0.8817567732529071}
Best cross validation score: 0.993
''' # å‚æ•°Cï¼šå–å€¼1-200ï¼Œå‚æ•°gammaï¼šæˆæŒ‡æ•°åˆ†å¸ƒï¼Œéšæœºæœç´¢åæµ‹è¯•é›†çš„è¯„åˆ†é«˜è¾¾1
```



## **é›†æˆæ¨¡å‹**

### é›†æˆæ¨¡å‹å’Œensembleæ¨¡å—ä»‹ç»

é›†æˆæ€æƒ³ï¼š

*   å…ˆäº§ç”Ÿä¸€ç»„åŸºæ¨¡å‹ï¼Œå†ç”¨æŸç§ç­–ç•¥å°†ä»–ä»¬ç»“åˆèµ·æ¥
*   é€šå¸¸å¯è·å¾—æ¯”å•ä¸€å­¦ä¹ å™¨ä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½
*   åŸºæ¨¡å‹å¯ä»¥ç›¸åŒï¼Œä¹Ÿå¯ä»¥ä¸åŒ

é›†æˆæ–¹æ³•

å¸¸ç”¨çš„é›†æˆæ–¹æ³•ï¼šè£…è¢‹æ³•ï¼ˆBaggingï¼‰ã€æå‡æ³•ï¼ˆ Boosting ï¼‰å’Œå †å æ³•ï¼ˆStackingï¼‰


*   è£…è¢‹æ³•ï¼š
    å¹¶è¡Œçš„å»ºç«‹ä¸€äº›å­¦ä¹ å™¨ï¼Œå°½å¯èƒ½ä½¿å…¶ç›¸äº’ç‹¬ç«‹ã€‚ä»æ–¹å·®-åå·®çš„è§’åº¦çœ‹ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°æ–¹å·®
*   æå‡æ³•ï¼š
    ä¸²è¡Œçš„å»ºç«‹ä¸€äº›å­¦ä¹ å™¨ï¼Œé€šè¿‡ä¸€å®šç­–ç•¥æå‡å¼±åˆ†ç±»å™¨æ•ˆæœï¼Œç»„åˆå¾—åˆ°å¼ºåˆ†ç±»å™¨ã€‚ä»æ–¹å·®-åå·®çš„è§’åº¦çœ‹ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°åå·®
*   å †å æ³•ï¼š
    å»ºç«‹å¤šä¸ªä¸åŒåŸºæ¨¡å‹ï¼Œå°†æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœå½“åšè¾“å…¥ï¼Œå»ºç«‹ä¸€ä¸ªé«˜å±‚çš„ç»¼åˆæ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆæ”¹è¿›é¢„æµ‹

##### ensembleé›†æˆæ¨¡å—çš„ä¸»è¦ç±»

| **ç±»**                              | **è¯´æ˜**       |
| ----------------------------------- | -------------- |
| ensemble.AdaBoostClassifier         | AdaBooståˆ†ç±»   |
| ensemble.AdaBoostRegressor          | AdaBoostå›å½’   |
| ensemble.RandomForestClassifier     | éšæœºæ£®æ—åˆ†ç±»   |
| ensemble.RandomForestRegressor      | éšæœºæ£®æ—å›å½’   |
| ensemble.GradientBoostingClassifier | æ¢¯åº¦æå‡åˆ†ç±»   |
| ensemble.GradientBoostingRegressor  | æ¢¯åº¦æå‡å›å½’   |
| ensemble.BaggingClassifier          | Baggingåˆ†ç±»    |
| ensemble.BaggingRegressor           | Baggingå›å½’    |
| Ensemble.VotingClassifier           | æŠ•ç¥¨è¡¨å†³åˆ†ç±»å™¨ |

### Bagging

![](image/Bagging.png)



##### BaggingClassifierå’ŒBaggingRegressorç±»

| **å‚æ•°**           | **åŠŸèƒ½**                                  |
| ------------------ | ----------------------------------------- |
| n_estimators       | åŸºæ¨¡å‹çš„ä¸ªæ•°ï¼Œé»˜è®¤ä¸º10                    |
| base_estimator     | ä½¿ç”¨çš„åŸºæ¨¡å‹ï¼Œé»˜è®¤æ˜¯å†³ç­–æ ‘                |
| bootstrap          | æ ·æœ¬çš„æŠ½æ ·æ–¹å¼ï¼Œé»˜è®¤ä¸ºTrueï¼ˆæœ‰æ”¾å›ï¼‰      |
| bootstrap_features | ç‰¹å¾çš„æŠ½æ ·æ–¹å¼ï¼Œé»˜è®¤ä¸ºFalseï¼ˆæ— æ”¾å›ï¼‰     |
| obb_score          | æ˜¯å¦ä½¿ç”¨è¢‹å¤–æ ·æœ¬ä¼°è®¡æ³›åŒ–èƒ½åŠ›ï¼Œé»˜è®¤ä¸ºFalse |

```python
# åŸºäºå†³ç­–æ ‘çš„Baggingæ¨¡å‹

# å»ºç«‹è£…è¢‹æ¨¡å‹
from sklearn.ensemble import BaggingClassifier

model_DF = DecisionTreeClassifier(random_state=10)
bag_DF = BaggingClassifier(base_estimator=model_DF, oob_score=True, random_state=10)
bag_DF.fit(X_train, y_train)

# æŸ¥çœ‹è¢‹å¤–æ ·æœ¬å¾—åˆ†
bag_DF.oob_score_
```

![](image/Bagging1.png)

```python
# åŸºäºé€»è¾‘å›å½’çš„Baggingæ¨¡å‹

# å»ºç«‹åŸºäºé€»è¾‘å›å½’çš„è£…è¢‹æ¨¡å‹
model_LR = LogisticRegression(class_weight='balanced', random_state=10)
bag_LR = BaggingClassifier(base_estimator=model_LR, oob_score=True, random_state=10)
bag_LR.fit(X_train, y_train)

# æŸ¥çœ‹è¢‹å¤–æ ·æœ¬å¾—åˆ†
bag_LR.oob_score_
```

![](image/Bagging2.png)

```python
# åŸºäºå†³ç­–æ ‘çš„Baggingæ¨¡å‹å‚æ•°è°ƒä¼˜
# ç½‘æ ¼æœç´¢è°ƒå‚
from sklearn.model_selection import GridSearchCV

grid_n = [20, 50, 100, 150, 200, 500]
grid_fea = [True, False]

grid_search = GridSearchCV(estimator=bag_DF, param_grid={'n_estimators':grid_n,'bootstrap_features':grid_fea},cv=5, scoring='roc_auc')
grid_search.fit(X_train, y_train)

'''
æœ€ä¼˜å‚æ•°ï¼š{'bootstrap_features': True, 'n_estimators': 100}
æµ‹è¯•é›†AUCï¼š0.76

	precision	recall	f1-score	support
0	0.77	0.98	0.86	64
1	0.91	0.34	0.50	29
avg/total	0.81	0.78	0.75	93
'''
```



#### éšæœºæ£®æ—

##### RandomForestClassifierç±»

| **å‚æ•°**     | **åŠŸèƒ½**                                                     |
| ------------ | ------------------------------------------------------------ |
| n_estimators | åŸºå†³ç­–æ ‘çš„ä¸ªæ•°ï¼Œé»˜è®¤ä¸º10                                     |
| criterion    | æœ€ä½³åˆ’åˆ†çš„è¯„ä»·æ ‡å‡†ï¼Œé»˜è®¤ä¸ºâ€œginiâ€ï¼Œå¯é€‰â€œentropyâ€              |
| max_features | å»ºç«‹æ¯æ£µæ ‘ä½¿ç”¨çš„ç‰¹å¾ä¸ªæ•°ï¼Œé»˜è®¤ä¸ºâ€œautoâ€ï¼ˆ$m=\sqrt{d}$ï¼‰ï¼Œè¿˜å¯é€‰ï¼š<br />â€œsqrtâ€ï¼ˆåŒâ€œautoâ€ï¼‰<br />â€œlog2â€ï¼ˆ$m = log_2d$ï¼‰<br />â€œNoneâ€ï¼ˆ$ğ‘š = ğ‘‘$ï¼‰ï¼Œä¹Ÿå¯ç›´æ¥è¾“å…¥ä¸ªæ•°ï¼ˆæ­£æ•´æ•°ï¼‰ |
| bootstrap    | æ ·æœ¬çš„æŠ½æ ·æ–¹å¼ï¼Œé»˜è®¤ä¸ºTrueï¼ˆæœ‰æ”¾å›ï¼‰                         |
| obb_score    | æ˜¯å¦ä½¿ç”¨è¢‹å¤–æ ·æœ¬ä¼°è®¡æ³›åŒ–èƒ½åŠ›ï¼Œé»˜è®¤ä¸ºFalse                    |
| class_weight | è®¾ç½®åˆ†ç±»æƒé‡ï¼Œé»˜è®¤ä¸ºâ€œNoneâ€                                   |

```python
# ä¸åŒçš„å†³ç­–æ ‘æ•°é‡å¯¹è¢‹å¤–æ ·æœ¬å¾—åˆ†çš„å½±å“

# è½½å…¥ç®—æ³•ç±»
from sklearn.ensemble import RandomForestClassifier

# è®¾ç½®åŸºæ¨¡å‹ä¸ªæ•°åˆ—è¡¨
grid_n = [20, 40, 60, 80, 100, 120]

# è®¡ç®—åœ¨ä¸åŒåŸºæ¨¡å‹ä¸ªæ•°ä¸‹çš„è¢‹å¤–æ ·æœ¬å¾—åˆ†
oob_score = []
for item in grid_n:
    model = RandomForestClassifier(n_estimators=item, random_state=10, oob_score=True)
    model.fit(X_train, y_train)
    oob_score.append(model.oob_score_)
    # éšç€baggingçš„åŸºå†³ç­–æ ‘æ•°ç›®çš„å¢åŠ ï¼Œè¢‹å¤–æ ·æœ¬å¾—åˆ†ä¸æ–­ä¸Šå‡
```

![](image/jicheng.png)

##### éšæœºæ£®æ—ç½‘æ ¼æœç´¢å‚æ•°è°ƒä¼˜

```python
# ç½‘æ ¼æœç´¢å‚æ•°è°ƒä¼˜
from sklearn.model_selection import GridSearchCV

grid_n = [20, 50, 100, 150, 200, 500]
grid_fea = np.arange(2, 19) 
grid_weight = ['balanced', None]

model_RF = RandomForestClassifier(random_state=10)
grid_search = GridSearchCV(estimator=model_RF, param_grid={'n_estimators':grid_n, 'max_features':grid_fea,  'class_weight':grid_weight},cv=5, scoring='roc_auc')
grid_search.fit(X_train, y_train)
'''
æœ€ä¼˜å‚æ•°ï¼š{â€˜n_estimatorsâ€™: 500, 'max_featuresâ€™:6, â€˜class_weightâ€™:None}
æµ‹è¯•é›†AUCï¼š0.79

	precision	recall	f1-score	support
0	0.77	0.98	0.86	64
1	0.91	0.34	0.50	29
avg/total	0.81	0.78	0.75	93
'''
```

##### ç‰¹å¾é‡è¦æ€§è¯„ä¼°

```python
# ç‰¹å¾é‡è¦æ€§è¯„ä¼°
best_RF = grid_search.best_estimator_
best_RF.fit(X_train, y_train)

# æ°´å¹³æ¡å½¢å›¾ç»˜åˆ¶
plt.figure(figsize=(8, 6))
pd.Series(best_RF.feature_importances_, index=X_train.columns).sort_values().plot(kind='barh')
    # ç”³è¯·äººæ”¶å…¥ã€è´·æ¬¾é‡‘é¢ã€æ˜¯å¦æœ‰è¿çº¦å†å²ã€å…±åŒç”³è¯·äººæ”¶å…¥è¿™äº”ä¸ªç‰¹å¾çš„é‡è¦æ€§æ’åœ¨å‰äº”ä½
```

![](image/ç‰¹å¾é‡è¦æ€§.png)



### Boosting

#### AdaBoost

##### AdaBoostClassifierç±»

| **å‚æ•°**       | **åŠŸèƒ½**                                                   |
| -------------- | ---------------------------------------------------------- |
| base_estimator | åŸºå­¦ä¹ å™¨çš„è®¾ç½®ï¼Œé»˜è®¤æ˜¯å†³ç­–æ ‘                               |
| n_estimators   | åŸºå­¦ä¹ å™¨çš„ä¸ªæ•°ï¼Œé»˜è®¤ä¸º50                                   |
| Learining_rate | å­¦ä¹ ç‡çš„è®¾ç½®ï¼Œé»˜è®¤ä¸º1.0                                    |
| algorithm      | äºŒåˆ†ç±»æ¨å¹¿åˆ°å¤šåˆ†ç±»ä½¿ç”¨çš„ç®—æ³•ï¼Œé»˜è®¤ä¸ºâ€œSAMME.Râ€ï¼Œå¯é€‰â€œSAMMEâ€ |

| **å±æ€§**             | **è¯´æ˜**               |
| -------------------- | ---------------------- |
| estimator_weights_   | æ¯ä¸ªå¼±åˆ†ç±»å™¨çš„æƒé‡     |
| estimator_errors_    | æ¯ä¸ªå¼±åˆ†ç±»å™¨çš„åˆ†ç±»è¯¯å·® |
| feature_importances_ | ç‰¹å¾é‡è¦æ€§             |

```python
# æ„å»ºåŸºäºå†³ç­–æ ‘çš„AdaBoostæ¨¡å‹
from sklearn.ensemble import AdaBoostClassifier

# åŸºåˆ†ç±»å™¨ä¸ºå†³ç­–æ ‘
model_ada = AdaBoostClassifier(random_state=10, algorithm='SAMME')
model_ada.fit(X_train,y_train)

# å¼±åˆ†ç±»å™¨çš„æƒé‡å’Œè¯¯å·®å˜åŒ–æŠ˜çº¿å›¾
plt.figure(figsize=(8, 6))
plt.plot(model_ada.estimator_weights_, 'y-+', label='weight')
plt.plot(model_ada.estimator_errors_, 'b--*', label='error')
plt.xlabel('estimator_order')
plt.legend()
    # ç¬¬ä¸€ä¸ªå¼±åˆ†ç±»å™¨æƒé‡æœ€é«˜ï¼Œè¯¯å·®æœ€ä½ï¼Œå…¶ä½™å¼±åˆ†ç±»å™¨é—´æƒé‡å’Œè¯¯å·®è¾ƒä¸ºæ¥è¿‘
    # æµ‹è¯•é›†AUCï¼š0.71
```

![](image/jicheng2.png)

```python
# ç‰¹å¾é‡è¦æ€§è¯„ä¼°
plt.figure(figsize=(8, 6))
pd.Series(model_ada.feature_importances_,       index=X_train.columns).sort_values().plot(kind='barh')
```

![](image/jicheng3.png)

```python
# å­¦ä¹ ç‡å¯¹åˆ†ç±»è¯¯å·®çš„å½±å“
grid_rate = np.linspace(0.01, 1, 10)

error = []
for item in grid_rate:
    model = AdaBoostClassifier(random_state=10, learning_rate=item)
    model.fit(X_train, y_train)
    error.append(model.score(X_test, y_test))
```

![](image/jicheng4.png)

```python
# åŸºåˆ†ç±»å™¨ä¸ºé€»è¾‘å›å½’
model_LR = LogisticRegression(random_state=10, class_weight='balanced')
model_adaLR = AdaBoostClassifier(base_estimator=model_LR, random_state=10)

grid_n = [20, 50, 100, 150, 200, 500]
grid_rate = np.linspace(0.01, 1, 10)

# ç½‘æ ¼æœç´¢è°ƒå‚
grid_search = GridSearchCV(estimator=model_adaLR, param_grid={'n_estimators':grid_n, 'learning_rate':grid_rate},cv=5, scoring='roc_auc')
grid_search.fit(X_train, y_train)
'''
æœ€ä¼˜å‚æ•°ï¼š{'learning_rate': 0.56, 'n_estimators': 20}
æµ‹è¯•é›†AUCï¼š0.72

	precision	Recall	f1-score	support
0	0.78	0.98	0.84	64
1	0.68	0.45	0.54	29
avg/total	0.75	0.76	0.75	93
'''
```



#### GBDT

##### GradientBoostingClassifierç±»

| **å‚æ•°**      | **åŠŸèƒ½**                                                     |
| ------------- | ------------------------------------------------------------ |
| loss          | æŒ‡å®šæŸå¤±å‡½æ•°ï¼Œé»˜è®¤ä¸ºâ€œdevianceâ€ï¼ˆå¯¹æ•°æŸå¤±ï¼‰ï¼Œå¯ä¸ºâ€œexponentialâ€ï¼ˆæŒ‡æ•°æŸå¤±ï¼‰ |
| learning_rate | å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º0.1                                            |
| n_estimators  | å¼±åˆ†ç±»å™¨ä¸ªæ•°ï¼Œé»˜è®¤ä¸º100                                      |
| subsample     | æ‹ŸåˆåŸºå­¦ä¹ å™¨çš„è®­ç»ƒé›†æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹ï¼Œ0åˆ°1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œé»˜è®¤ä¸º1.0 |

| **å±æ€§**             | **è¯´æ˜**                                     |
| -------------------- | -------------------------------------------- |
| feature_importances_ | ç‰¹å¾é‡è¦æ€§                                   |
| oob_improvement_     | æ¯å¢åŠ ä¸€ä¸ªåŸºåˆ†ç±»å™¨ï¼Œè¢‹å¤–æ ·æœ¬çš„æŸå¤±å‡½æ•°å‡å°‘å€¼ |
| train_score_         | æ¯å¢åŠ ä¸€ä¸ªåŸºåˆ†ç±»å™¨ï¼Œè®­ç»ƒé›†ä¸ŠæŸå¤±å‡½æ•°çš„å€¼     |

##### GradientBoostingRegressorç±»

| **å‚æ•°**      | **åŠŸèƒ½**                                                     |
| ------------- | ------------------------------------------------------------ |
| loss          | æŒ‡å®šæŸå¤±å‡½æ•°ï¼Œé»˜è®¤ä¸ºâ€œlsâ€ï¼ˆå¹³æ–¹æŸå¤±ï¼‰ï¼Œè¿˜å¯ä¸ºï¼š<br />â€œladâ€ï¼ˆç»å¯¹å€¼æŸå¤±ï¼‰<br />â€œhuberâ€ï¼ˆhuberæŸå¤±ï¼‰<br />â€œquantileâ€ï¼ˆåˆ†ä½æ•°å›å½’ï¼‰ |
| alpha         | å½“loss=â€œhuberâ€æˆ–â€quantileâ€æ—¶æœ‰æ•ˆï¼Œä¸¤ä¸ªæŸå¤±ä¸­çš„å‚æ•°ï¼Œé»˜è®¤ä¸º0.9 |
| learning_rate | å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º0.1                                            |
| n_estimators  | å¼±åˆ†ç±»å™¨ä¸ªæ•°ï¼Œé»˜è®¤ä¸º100                                      |
| subsample     | æ‹ŸåˆåŸºå­¦ä¹ å™¨çš„è®­ç»ƒé›†æ ·æœ¬æ•°å æ€»æ ·æœ¬æ•°çš„æ¯”ä¾‹ï¼Œ0åˆ°1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œé»˜è®¤ä¸º1.0 |

| **å±æ€§**             | **è¯´æ˜**                                     |
| -------------------- | -------------------------------------------- |
| feature_importances_ | ç‰¹å¾é‡è¦æ€§                                   |
| oob_improvement_     | æ¯å¢åŠ ä¸€ä¸ªåŸºåˆ†ç±»å™¨ï¼Œè¢‹å¤–æ ·æœ¬çš„æŸå¤±å‡½æ•°å‡å°‘å€¼ |
| train_score_         | æ¯å¢åŠ ä¸€ä¸ªåŸºåˆ†ç±»å™¨ï¼Œè®­ç»ƒé›†ä¸ŠæŸå¤±å‡½æ•°çš„å€¼     |

```python
# æ„å»ºGBDTæ¨¡å‹
from sklearn.ensemble import GradientBoostingClassifier

# å»ºç«‹å¹¶è®­ç»ƒGBDTæ¨¡å‹
model_gbdt = GradientBoostingClassifier(random_state=10, subsample=.7)
model_gbdt.fit(X_train, y_train)

# å¼±åˆ†ç±»å™¨è¢‹å¤–æ ·æœ¬æŸå¤±å‡å°å€¼å’Œè®­ç»ƒé›†æŸå¤±
plt.figure(figsize=(8, 6))
plt.plot(model_gbdt.oob_improvement_, 'y-+', label='oob')
plt.plot(model_gbdt.train_score_, 'b--*', label='train')
plt.xlabel('estimator_order')
plt.legend()
    # éšç€å¼±åˆ†ç±»å™¨çš„å¢åŠ ï¼Œè®­ç»ƒé›†çš„æŸå¤±åœ¨ä¸æ–­é™ä½ï¼Œè¢‹å¤–æ ·æœ¬çš„æŸå¤±å‡å°å€¼æ…¢æ…¢è¶‹äºå¹³ç¨³
    # æµ‹è¯•é›†AUCï¼š0.75
```

![](image/jicheng5.png)

```python
# ç½‘æ ¼æœç´¢è°ƒå‚
grid_search = GridSearchCV(estimator=model_gbdt, 
param_grid={'n_estimators':grid_n, 'learning_rate':grid_rate,   'subsample':grid_sub},cv=5, scoring='roc_auc')
grid_search.fit(X_train, y_train)

# è¿”å›æœ€ä¼˜å‚æ•°
grid_search.best_params_
```

![](image/jicheng6.png)

```python
# ç‰¹å¾é‡è¦æ€§
best_gbdt = grid_search.best_estimator_
best_gbdt.fit(X_train, y_train)

plt.figure(figsize=(8, 6))
pd.Series(best_gbdt.feature_importances_, index=X_train.columns).sort_values().plot(kind='barh')

```

![](image/jicheng7.png)

### Voting Classifier

##### VotingClassifierç±»

| **å‚æ•°**   | **åŠŸèƒ½**                                                 |
| ---------- | -------------------------------------------------------- |
| estimators | åŸºåˆ†ç±»å™¨è®¾ç½®ï¼Œéœ€ä¼ å…¥ä¸€ä¸ªå…ƒç»„åˆ—è¡¨                         |
| voting     | æŠ•ç¥¨æ–¹å¼ï¼Œç¡¬æŠ•ç¥¨æˆ–è€…è½¯æŠ•ç¥¨ï¼Œé»˜è®¤ä¸ºâ€œhardâ€ï¼Œè½¯æŠ•ç¥¨ä¸ºâ€œsoftâ€ |
| weights    | åŸºåˆ†ç±»å™¨çš„æƒé‡ï¼Œéœ€ä¼ å…¥ä¸€ä¸ªåˆ—è¡¨å¯¹è±¡                       |

```python
# å•ä¸€æ¨¡å‹çš„ç¡¬æŠ•ç¥¨å’Œè½¯æŠ•ç¥¨

# å•ä¸€æ¨¡å‹çš„ç¡¬æŠ•ç¥¨
model_vote_hard = VotingClassifier(estimators=[('DF',model_DF), ('LR',model_LR)]) # å†³ç­–æ ‘ä¸é€»è¾‘å›å½’
model_vote_hard.fit(X_train, y_train)
# å¾—åˆ°é¢„æµ‹æ ‡ç­¾
y_pred_vote_hard = model_vote_hard.predict(X_test)
# è®¡ç®—AUC
fpr, tpr, threshold = roc_curve(y_score=y_pred_vote_hard, y_true=y_test)
print('ç¡¬æŠ•ç¥¨AUCå€¼ï¼š', auc(fpr, tpr))
'''ç¡¬æŠ•ç¥¨æµ‹è¯•é›†AUCï¼š0.67'''
# å•ä¸€æ¨¡å‹çš„è½¯æŠ•ç¥¨
model_vote_soft = VotingClassifier(estimators=[('DF',model_DF), ('LR',model_LR)], voting='soft')
model_vote_soft.fit(X_train, y_train)
# å¾—åˆ°é¢„æµ‹æ¦‚ç‡
y_prob_vote_soft = model_vote_soft.predict_proba(X_test)[:, 1]
# è®¡ç®—AUC
fpr, tpr, threshold = roc_curve(y_score=y_prob_vote_soft, y_true=y_test)
print('è½¯æŠ•ç¥¨AUCå€¼ï¼š', auc(fpr, tpr))
'''è½¯æŠ•ç¥¨æµ‹è¯•é›†AUCï¼š0.80'''

# é›†æˆæ¨¡å‹çš„ç¡¬æŠ•ç¥¨å’Œè½¯æŠ•ç¥¨
# é›†æˆæ¨¡å‹çš„è½¯æŠ•ç¥¨
best_RF = RandomForestClassifier(max_features=6, n_estimators=500, random_state=10)
best_gbdt = GradientBoostingClassifier(n_estimators=200, learning_rate=0.01, subsample=0.3, random_state=10)

model_vote_soft = VotingClassifier(estimators=[('RF',best_RF), ('GBDT',best_gbdt)], voting='soft')
model_vote_soft.fit(X_train, y_train)

# å¾—åˆ°é¢„æµ‹æ¦‚ç‡
y_prob_vote_soft = model_vote_soft.predict_proba(X_test)[:, 1]

# è®¡ç®—AUC
fpr, tpr, threshold = roc_curve(y_score=y_prob_vote_soft, y_true=y_test)
print('è½¯æŠ•ç¥¨AUCå€¼ï¼š', auc(fpr, tpr))
'''
ç¡¬æŠ•ç¥¨æµ‹è¯•é›†AUCï¼š0.66
è½¯æŠ•ç¥¨æµ‹è¯•é›†AUCï¼š0.79
'''

# AUCå€¼åŠ æƒè½¯æŠ•ç¥¨
# åŠ æƒè½¯æŠ•ç¥¨
## è·å¾—å•ä¸€æ¨¡å‹çš„AUCå€¼
model_DF.fit(X_train, y_train)
y_prob_DF = model_DF.predict_proba(X_test)[:, 1]
fpr, tpr, threshold = roc_curve(y_score=y_prob_DF, y_true=y_test)
print('å†³ç­–æ ‘AUCå€¼ï¼š', auc(fpr, tpr))
model_LR.fit(X_train, y_train)
y_prob_LR = model_LR.predict_proba(X_test)[:, 1]
fpr, tpr, threshold = roc_curve(y_score=y_prob_LR, y_true=y_test)
print('é€»è¾‘å›å½’AUCå€¼ï¼š', auc(fpr, tpr))
## AUCåŠ æƒè½¯æŠ•ç¥¨
model_vote_soft = VotingClassifier(estimators=[('DF',model_DF), ('LR',model_LR)], voting='soft', weights=[0.76, 0.73])
model_vote_soft.fit(X_train, y_train)
# å¾—åˆ°é¢„æµ‹æ¦‚ç‡
y_prob_vote_soft = model_vote_soft.predict_proba(X_test)[:, 1]
# è®¡ç®—AUC
fpr, tpr, threshold = roc_curve(y_score=y_prob_vote_soft, y_true=y_test)
print('è½¯æŠ•ç¥¨AUCå€¼ï¼š', auc(fpr, tpr))
'''
å†³ç­–æ ‘æµ‹è¯•é›†AUCï¼š0.76
è½¯æŠ•ç¥¨æµ‹è¯•é›†AUCï¼š0.80
é€»è¾‘å›å½’æµ‹è¯•é›†AUCï¼š0.73
'''
```





## **ç‰¹å¾æŠ½å–ä¸ç‰¹å¾é€‰æ‹©**

### ç‰¹å¾æå–ä¸ç‰¹å¾é€‰æ‹©æ¦‚è¿°

ç‰¹å¾å·¥ç¨‹ä¸»è¦åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢ï¼š

*   ç‰¹å¾æå–ï¼šå°†æ–‡æœ¬æˆ–å›¾åƒç­‰æ•°æ®è½¬æ¢ä¸ºå¯ç”¨äºæœºå™¨å­¦ä¹ çš„æ•°å­—ç‰¹å¾
*   ç‰¹å¾é€‰æ‹©ï¼šä»å…¨éƒ¨æ•°æ®ç‰¹å¾ä¸­é€‰å–ä¸€ä¸ªç‰¹å¾å­é›†
*   ç‰¹å¾æ„å»ºï¼šä»åŸå§‹ç‰¹å¾ä¸­äººå·¥æ„å»ºæ–°çš„ç‰¹å¾





### ç‰¹å¾æå–ä¸feature_extractionæ¨¡å—

feature_extractionçš„ä¸»è¦ç±»

*   é¢å‘å›¾åƒæ•°æ®

| **ç±»**                                                       | **è¯´æ˜**               |
| ------------------------------------------------------------ | ---------------------- |
| sklearn.feature_extraction.image.extract_patches_2d          | ä»2Då›¾åƒä¸­æå–å°å—å›¾åƒ |
| sklearn.feature_extraction.image.reconstruct_from_patches_2d | å°†å°å—å›¾åƒé‡ç»„ä¸º2Då›¾åƒ |

*   é¢å‘æ–‡æœ¬æ•°æ®(é‡ç‚¹)

| **ç±»**                                  | **è¯´æ˜**               |
| --------------------------------------- | ---------------------- |
| feature_extraction.text.CountVectorizer | å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯è¢‹æ¨¡å‹   |
| feature_extraction.text.TfidfVectorizer | å°†æ–‡æœ¬è½¬æ¢ä¸ºTF-IDFçŸ©é˜µ |

#### ä»æ–‡æœ¬æå–ç‰¹å¾

å°†æ–‡æœ¬æ•°æ®è½¬åŒ–æˆç‰¹å¾å‘é‡çš„è¿‡ç¨‹ï¼Œå¸¸ç”¨è¡¨ç¤ºæ³•ä¸ºè¯è¢‹æ³•

è¯è¢‹æ³•ï¼šä¸è€ƒè™‘è¯æ³•å’Œè¯­åºï¼Œæ¯ä¸ªè¯è¯­ç›¸äº’ç‹¬ç«‹
                                    Mary wants to go to Japan.
                                    Bill wants to go to Germany.
å¯ä»¥å»ºç«‹ä¸€ä¸ªè¯å…¸ç”¨äºæ„å»ºç‰¹å¾å‘é‡ï¼š
                         [Maryï¼Œwantsï¼Œtoï¼Œgoï¼ŒJapanï¼ŒBillï¼ŒGermany]
å‘é‡æ¯ä¸ªä½ç½®è¡¨ç¤ºçš„å•è¯ä¸ä¸Šé¢çš„æ•°ç»„ä¸€è‡´ï¼Œå€¼ä¸ºè¯¥å•è¯åœ¨å¥å­ä¸­å‡ºç°çš„æ¬¡æ•°
   ç¬¬ä¸€ä¸ªå¥å­ï¼š [1ï¼Œ1ï¼Œ2ï¼Œ1ï¼Œ1ï¼Œ0ï¼Œ0]
   ç¬¬äºŒä¸ªå¥å­ï¼š [0ï¼Œ1ï¼Œ2ï¼Œ1ï¼Œ0ï¼Œ1ï¼Œ1]



CountVectorizerï¼šåªè€ƒè™‘è¯æ±‡åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„é¢‘ç‡

TfidfVectorizerï¼šé™¤äº†è€ƒé‡æŸè¯æ±‡åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œè¿˜å…³æ³¨åŒ…å«è¿™ä¸ªè¯æ±‡çš„æ–‡æœ¬æ•°é‡é€‚åˆæ–‡æœ¬æ¡ç›®å¤šçš„æƒ…å†µ

##### feature_extraction.textçš„ä¸»è¦ç±»ï¼š

| **ç±»**                                  | **è¯´æ˜**               |
| --------------------------------------- | ---------------------- |
| feature_extraction.text.CountVectorizer | å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯è¢‹æ¨¡å‹   |
| feature_extraction.text.TfidfVectorizer | å°†æ–‡æœ¬è½¬æ¢ä¸ºTF-IDFçŸ©é˜µ |

| **é€šç”¨æ–¹æ³•**        | **è¯´æ˜**                       |
| ------------------- | ------------------------------ |
| fit(X, y)           | å­¦ä¹ æ–‡æœ¬çš„å‘é‡è¡¨ç¤º             |
| transform(X)        | è½¬åŒ–ä¸ºå‘é‡è¡¨ç¤ºçš„çŸ©é˜µ           |
| fit_transform(X, y) | å…ˆå­¦ä¹ å†è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º         |
| get_feature_names() | æŸ¥çœ‹æ‰€æœ‰æ–‡æœ¬çš„è¯æ±‡ï¼ˆåˆ—è¡¨å¯¹è±¡ï¼‰ |

##### CountVectorizerç±»

| å¸¸ç”¨å‚æ•°   | è¯´æ˜                                                         |
| ---------- | ------------------------------------------------------------ |
| stop_words | è®¾ç½®åœç”¨è¯ï¼Œé»˜è®¤ä¸ºNoneï¼ˆæ²¡æœ‰ï¼‰ï¼Œå¯è®¾ç½®ä¸ºâ€œenglishâ€æˆ–listï¼ˆè‡ªè¡Œç»™å®šï¼‰ |
| min_df     | è®¾å®šé˜ˆå€¼ï¼Œæ„å»ºè¯æ±‡è¡¨æ—¶ï¼Œå¿½ç•¥æ–‡æ¡£é¢‘ç‡ä¸¥æ ¼ä½äºè¿™ä¸ªé˜ˆå€¼çš„è¯æ±‡ï¼Œé»˜è®¤ä¸º1 |
| binary     | é»˜è®¤ä¸ºFalseï¼ŒTrueè¡¨ç¤ºå‘é‡è¡¨ç¤ºä¸­æ‰€æœ‰éé›¶é¡¹æ ‡è®°ä¸º1             |

| **å±æ€§åˆ—è¡¨** | **è¯´æ˜**               |
| ------------ | ---------------------- |
| vocabulary_  | è¿”å›è¯æ±‡è¡¨ï¼ˆå­—å…¸å¯¹è±¡ï¼‰ |
| stop_words_  | è¿”å›åœç”¨è¯è¡¨           |

| ç•¥è¯¦è¿°                                                       | 1    |
| ------------------------------------------------------------ | ---- |
| stop_words: å¯è®¾ä¸ºstring {'english'}ï¼Œlistæˆ–Noneï¼ˆé»˜è®¤ï¼‰ï¼›<br/>                       è®¾ä¸ºenglishå°†ä½¿ç”¨å†…ç½®çš„è‹±è¯­åœç”¨è¯ï¼›<br/>                       è®¾ä¸ºlistå¯è‡ªå®šä¹‰åœç”¨è¯ï¼›<br/>                       è®¾ä¸ºNoneä¸ä½¿ç”¨åœç”¨è¯ï¼›<br/>                       è®¾ä¸ºNoneä¸”å‚æ•°max_dfâˆˆ[0.7, 1.0)æ—¶å°†è‡ªåŠ¨æ ¹æ®å½“å‰çš„è¯­æ–™åº“å»ºç«‹åœç”¨è¯è¡¨<br />max_dfï¼šfloat in range [ 0.0ï¼Œ1.0 ]æˆ–intï¼Œdefaultï¼ˆé»˜è®¤ï¼‰= 1.0ï¼›<br/>                  ä½œä¸ºé˜ˆå€¼ï¼Œæ„é€ è¯æ±‡è¡¨æ—¶ï¼Œå¦‚æœæŸè¯çš„æ–‡æ¡£é¢‘ç‡å¤§äºmax_dfï¼Œåˆ™è¿™ä¸ªè¯ä¸ä¼šè¢«å½“ä½œå…³é”®è¯ï¼›<br/>                  ä¸ºæµ®ç‚¹å€¼ï¼Œåˆ™è¡¨ç¤ºè¯å‡ºç°çš„æ¬¡æ•°ä¸æ–‡æ¡£æ•°çš„ç™¾åˆ†æ¯”ï¼›<br/>                  ä¸ºæ•´æ•°ï¼Œåˆ™è¡¨ç¤ºè¯å‡ºç°çš„æ¬¡æ•°ï¼›<br/>                  å¦‚æœç»™å®šäº†å‚æ•°vocabularyï¼Œåˆ™æ­¤å‚æ•°æ— æ•ˆ<br /><br/>min_dfï¼šfloat in range [ 0.0ï¼Œ1.0 ]æˆ–intï¼Œdefault = 1ï¼›<br/>                 ä½œä¸ºé˜ˆå€¼ï¼Œæ„å»ºè¯æ±‡è¡¨æ—¶å¿½ç•¥æ–‡æ¡£é¢‘ç‡ä½äºmin_dfçš„è¯è¯­ï¼›<br/>                 ä¸max_dfçš„ä½œç”¨ç±»ä¼¼<br /><br/>vocabulary_ï¼šå­—å…¸ç±»å‹ï¼Œkeyä¸ºå…³é”®è¯ï¼Œvalueæ˜¯ç‰¹å¾ç´¢å¼• | 1    |



æ›´å¤šå‚æ•°è¯¦æƒ…è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html

```python
class sklearn.feature_extraction.text.CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\b\w\w+\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)

# åˆ›å»ºè¯è¢‹æ¨¡å‹
from sklearn.feature_extraction.text import CountVectorizer

# é‡‡ç”¨CountVectorizeræå–ç‰¹å¾å‘é‡ï¼Œå¹¶å»é™¤åœç”¨è¯
count_vec = CountVectorizer(stop_words='english')
x_count_train = count_vec.fit_transform(train_X)
x_count_test = count_vec.transform(test_X)
# æŸ¥çœ‹å‘é‡è¡¨ç¤ºçŸ©é˜µ
print(x_count_train) # æ˜¾ç¤ºè€Œå·²ï¼Œä¸ä¸€å®šéè¦æ‰“å°
''' ç¤ºä¾‹ï¼š
  (0, 5222)	1
  (0, 4749)	1
  (0, 7271)	1
   :   :
   '''ã€€# è§£é‡Šï¼šç¬¬0ä¸ªåˆ—è¡¨ï¼Œè¯å…¸ä¸­ç´¢å¼•ä¸º3534çš„å…ƒç´ ï¼Œè¯é¢‘ä¸º1

# è½¬åŒ–ä¸ºéå‹ç¼©çŸ©é˜µ
print(x_count_train.toarray())
''' ç¤ºä¾‹ï¼š
[[0 0 0 ..., 0 0 0]
 [0 0 0 ..., 0 0 0]
 ..., 
 [0 0 0 ..., 0 0 0]
 [0 0 0 ..., 0 0 0]]
'''
# æŸ¥çœ‹æ¯ä¸ªè¯åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­çš„è¯é¢‘
print(x_count_train.toarray().sum(axis=0))
''' ç¤ºä¾‹ï¼š
[ 7 26  2 ...  1  1  1]
'''
# æŸ¥çœ‹è¯æ±‡è¡¨
print(count_vec.vocabulary_)
''' ç¤ºä¾‹ï¼š
{'princess': 5222, 'great': 3122, 'hear': 3263, 'settling': 5835, 'happenin': 3218,â€¦â€¦} 
''' # æ‰€æœ‰è¯æ±‡å­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸å¯¹è±¡ä¸­ï¼Œå­—å…¸ä¸­çš„é”®æ˜¯è¯æ±‡ï¼Œå€¼æ˜¯è¯¥è¯æ±‡çš„ç´¢å¼•
# æŸ¥çœ‹ç´¢å¼•æ‰€å¯¹åº”çš„è¯æ±‡
print(count_vec.get_feature_names()[3122])
''' ç¤ºä¾‹ï¼š
great
'''
```

```python
# å»ºç«‹å¹¶è®­ç»ƒå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯æ¨¡å‹
# å»ºç«‹å¹¶è®­ç»ƒå¤šé¡¹å¼è´å¶æ–¯æ¨¡å‹
from sklearn.naive_bayes import MultinomialNB
mnb_count = MultinomialNB()
mnb_count.fit(x_count_train, train_y)   
mnb_count_y_predict = mnb_count.predict(x_count_test) 

# æ¨¡å‹è¯„ä¼°
from sklearn.metrics import classification_report
print("é¢„æµ‹æ­£ç¡®ç‡ï¼š", mnb_count.score(x_count_test, test_y))
print("åˆ†ç±»æŠ¥å‘Š:\n", classification_report(mnb_count_y_predict, test_y))
'''
é¢„æµ‹æ­£ç¡®ç‡ï¼š 0.98
åˆ†ç±»æŠ¥å‘Š:
              precision    recall  f1-score   support
          0       0.99      0.98      0.99       983
          1        0.88      0.95      0.92       132
avg / total       0.98      0.98      0.98      1115
'''
```

#### TF-IDF

æ€æƒ³ï¼šè‹¥æŸè¯æ±‡åœ¨ä¸€ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡é«˜ï¼Œä¸”åœ¨å…¶å®ƒæ–‡æ¡£ä¸­å‡ºç°å°‘ï¼Œåˆ™è®¤ä¸ºæ­¤è¯æ±‡å…·æœ‰å¾ˆå¥½çš„ç±»åˆ«åŒºåˆ†èƒ½åŠ›

è¯æ±‡çš„é‡è¦æ€§éšç€å®ƒåœ¨ä¸€ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°æˆæ­£æ¯”ï¼Œä¸å®ƒåœ¨æ‰€æœ‰æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°æˆåæ¯”ã€‚

TF-IDFå€¾å‘äºè¿‡æ»¤æ‰å¸¸è§çš„è¯è¯­ï¼Œä¿ç•™é‡è¦çš„è¯è¯­ã€‚

![](image/TF-IDF.png)

ä¸¾ä¾‹ï¼š

![](image/TF1.png)

##### TfidfVectorizerç±»

| **å¸¸ç”¨å‚æ•°** | **è¯´æ˜**                                                     |
| ------------ | ------------------------------------------------------------ |
| stop_words   | è®¾ç½®åœç”¨è¯ï¼Œé»˜è®¤ä¸ºNoneï¼ˆæ²¡æœ‰ï¼‰ï¼Œå¯è®¾ç½®ä¸ºâ€œenglishâ€æˆ–listï¼ˆè‡ªè¡Œç»™å®šï¼‰ |
| min_df       | è®¾å®šé˜ˆå€¼ï¼Œæ„å»ºè¯æ±‡è¡¨æ—¶ï¼Œå¿½ç•¥æ–‡æ¡£é¢‘ç‡ä¸¥æ ¼ä½äºè¿™ä¸ªé˜ˆå€¼çš„è¯æ±‡ï¼Œé»˜è®¤ä¸º1 |
| binary       | é»˜è®¤ä¸ºFalseï¼ŒTrueè¡¨ç¤ºå‘é‡è¡¨ç¤ºä¸­æ‰€æœ‰éé›¶é¡¹æ ‡è®°ä¸º1             |
| smooth_idf   | é»˜è®¤ä¸ºTrueï¼Œå‘é€†æ–‡æ¡£é¢‘ç‡æ·»åŠ ä¸€ä¸ªå¹³æ»‘é¡¹ï¼Œé˜²æ­¢å…¶è®¡ç®—çš„å€¼ä¸º0    |

| **å±æ€§åˆ—è¡¨** | **è¯´æ˜** |
| ------------ | -------- |
| vocabulary_  | è¯æ±‡è¡¨   |
| stop_words_  | åœç”¨è¯è¡¨ |
| idf_         | idfå€¼    |

```python
# è½¬æ¢ä¸ºTF-IDFç‰¹å¾
# ç±»å‚æ•°ï¼š
class TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',lowercase=True, max_df=1.0, max_features=None, min_df=1,ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,stop_words='english', strip_accents=None, sublinear_tf=False,token_pattern='(?u)\\b\\w\\w+\\b', tokenizer=None, use_idf=True,vocabulary=None)

# é‡‡ç”¨TfidfVectorizeræå–æ–‡æœ¬ç‰¹å¾å‘é‡ï¼Œä½¿ç”¨è‹±æ–‡åœç”¨è¯
from sklearn.feature_extraction.text import TfidfVectorizer
tfid_vec = TfidfVectorizer(stop_words='english')
x_tfid_train = tfid_vec.fit_transform(train_X)
x_tfid_test = tfid_vec.transform(test_X)

# å»ºç«‹å¹¶è®­ç»ƒå¤šé¡¹å¼è´å¶æ–¯æ¨¡å‹
mnb_tfid = MultinomialNB()
mnb_tfid.fit(x_tfid_train, train_y)
mnb_tfid_y_predict = mnb_tfid.predict(x_tfid_test)

# æ¨¡å‹è¯„ä¼°
print("æ¨¡å‹æ­£ç¡®ç‡ï¼š", mnb_tfid.score(x_tfid_test, test_y))
print("åˆ†ç±»æŠ¥å‘Š:\n", classification_report(mnb_tfid_y_predict, test_y))
'''
é¢„æµ‹æ­£ç¡®ç‡ï¼š 0.96
åˆ†ç±»æŠ¥å‘Š:
              precision    recall  f1-score   support
ham       0       1.00      0.96      0.98      1013
spam      1       0.71      1.00      0.83       102
avg / total       0.97      0.96      0.97      1115

'''
```



### ç‰¹å¾é€‰æ‹©ä¸feature_selectionæ¨¡å—

å®šä¹‰ï¼šä»å…¨éƒ¨ç‰¹å¾ä¸­é€‰å–ä¸€ä¸ªç‰¹å¾å­é›†æ¥å»ºç«‹æ¨¡å‹

ç›®çš„ï¼šé™ä½ç‰¹å¾ç»´åº¦ï¼Œæé«˜æ¨¡å‹æ€§èƒ½

åŸºæœ¬åŸåˆ™ï¼š
åŒ…å«ä¿¡æ¯è¾ƒå°‘ï¼ˆæ–¹å·®è¾ƒä½ï¼‰çš„ç‰¹å¾åº”è¯¥è¢«å‰”é™¤
ä¸ç›®æ ‡ç‰¹å¾ç›¸å…³æ€§é«˜çš„ç‰¹å¾åº”è¯¥ä¼˜å…ˆè¢«é€‰æ‹©

ç‰¹å¾é€‰æ‹©æ–¹æ³•:

| **æ–¹å¼**            | **è¯´æ˜**                                               | **ä¸»è¦æ–¹æ³•**                   |
| ------------------- | ------------------------------------------------------ | ------------------------------ |
| è¿‡æ»¤å¼ï¼ˆFilterï¼‰    | å…ˆè¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå†å»ºç«‹æ¨¡å‹ï¼Œç‰¹å¾é€‰æ‹©çš„è¿‡ç¨‹ä¸­ä¸æ¶‰åŠå»ºæ¨¡ | æ–¹å·®é€‰æ‹©æ³•å¡æ–¹æ£€éªŒæ³•ç›¸å…³ç³»æ•°æ³• |
| åŒ…è£¹å¼ï¼ˆWrapperï¼‰   | å»ºç«‹æ¨¡å‹å¹¶ç»™å®šè¯„ä»·æ ‡å‡†ï¼Œé€‰æ‹©æ•ˆæœæœ€ä¼˜çš„ç‰¹å¾å­é›†         | é€’å½’ç‰¹å¾æ¶ˆé™¤æ³•                 |
| åµŒå…¥å¼ï¼ˆEmbeddingï¼‰ | ç‰¹å¾é€‰æ‹©ä¸æ¨¡å‹è®­ç»ƒç»“åˆï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è¿›è¡Œç‰¹å¾é€‰æ‹©   | ***ğ¿****1***æ­£åˆ™åŒ–é¡¹ç‰¹å¾é‡è¦æ€§ |

#### feature_selectionæ¨¡å—

| **ç±»**                              | **è¯´æ˜**                                              |
| ----------------------------------- | ----------------------------------------------------- |
| feature_selection.VarianceThreshold | å‰”é™¤ä½äºæ–¹å·®é˜ˆå€¼çš„ç‰¹å¾                                |
| feature_selection.SelectKBest       | æ ¹æ®ç»™å®šçš„å¾—åˆ†å‡½æ•°é€‰å‡ºKä¸ªå¾—åˆ†æœ€é«˜çš„ç‰¹å¾               |
| feature_selection.SelectPercentile  | æ ¹æ®ç»™å®šçš„å¾—åˆ†å‡½æ•°é€‰å‡ºå‰p%ä¸ªå¾—åˆ†æœ€é«˜çš„ç‰¹å¾ï¼ˆpéœ€æŒ‡å®šï¼‰ |
| feature_selection.RFE               | é€’å½’ç‰¹å¾æ¶ˆé™¤æ³•                                        |
| feature_selection.RFECV             | åŒ…å«äº¤å‰éªŒè¯çš„é€’å½’ç‰¹å¾æ¶ˆé™¤æ³•                          |
| feature_selection.SelectFromModel   | åµŒå…¥å¼ç‰¹å¾æ¶ˆé™¤æ³•                                      |

| **é€šç”¨æ–¹æ³•**        | **è¯´æ˜**                                  |
| ------------------- | ----------------------------------------- |
| fit(X, y)           | å­¦ä¹ æ€æ ·è¿›è¡Œç‰¹å¾é€‰æ‹©                      |
| transform(X)        | å¯¹Xè¿›è¡Œç‰¹å¾é€‰æ‹©                           |
| fit_transform(X, y) | å…ˆå­¦ä¹ æ€æ ·è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå†å¯¹Xè¿›è¡Œç‰¹å¾é€‰æ‹© |
| get_support ( )     | è¿”å›å¸ƒå°”æ•°ç»„ï¼ŒæŒ‡æ˜å“ªäº›ç‰¹å¾è¢«é€‰æ‹©          |



#### è¿‡æ»¤å¼

##### æ–¹å·®é€‰æ‹©æ³•

æ€æƒ³ï¼šæ–¹å·®å°çš„ç‰¹å¾æ³¢åŠ¨æ€§å°ï¼ŒåŒ…å«çš„ä¿¡æ¯ä¹Ÿè¾ƒå°‘ï¼Œå¯¹æ¨¡å‹å½±å“è¾ƒå°

æ–¹æ³•ï¼š

*   ç»™å®šä¸€ä¸ªæ–¹å·®é˜ˆå€¼ï¼Œè®¡ç®—æ‰€æœ‰ç‰¹å¾çš„æ–¹å·®
*   ä¿ç•™æ–¹å·®å¤§äºé˜ˆå€¼çš„ç‰¹å¾
*   å‰”é™¤æ–¹å·®å°äºé˜ˆå€¼çš„ç‰¹å¾

##### VarianceThresholdç±»

| **å‚æ•°**  | **è¯´æ˜**                                                   |
| --------- | ---------------------------------------------------------- |
| threshold | è¾“å…¥æ–¹å·®é˜ˆå€¼ï¼ˆæµ®ç‚¹å‹ï¼‰ï¼Œé»˜è®¤ä¸º0.0ï¼Œä¼šè‡ªåŠ¨åˆ é™¤æ–¹å·®ä¸º0çš„ç‰¹å¾ |

| **å±æ€§**   | **è¯´æ˜**         |
| ---------- | ---------------- |
| variances_ | æŸ¥çœ‹å„ç‰¹å¾çš„æ–¹å·® |

```python
# å‰”é™¤ä½äºæ–¹å·®é˜ˆå€¼çš„ç‰¹å¾
from sklearn.feature_selection import VarianceThreshold
## è®¾å®šæ–¹å·®é˜ˆå€¼ä¸º0.5
sel_var = VarianceThreshold(threshold=0.5)
## å»æ‰Xä¸­æ–¹å·®ä½äºé˜ˆå€¼çš„ç‰¹å¾
sel_var_X = sel_var.fit_transform(X)
## æ‰§è¡Œå‰”é™¤åXçš„ç»´åº¦
sel_var_X.shape
## æŸ¥çœ‹æ²¡æœ‰è¢«å‰”é™¤çš„ç‰¹å¾
print(pd.Series(sel_var.get_support(), index=X.columns))
'''
ç‰¹å¾é€‰æ‹©åçš„ç»´åº¦ï¼šï¼ˆ241ï¼Œ7ï¼‰
é€‰å‡ºçš„ç‰¹å¾ï¼šage
cp
trestbps
chol
thalach
oldpeak
ca
'''
```

##### å¡æ–¹æ£€éªŒæ³•

![](image/kfjyf.png)

å¡æ–¹æ£€éªŒä¸¾ä¾‹

![](image/kfjyf1.png)

##### ç›¸å…³ç³»æ•°æ£€éªŒæ³•

##### SelectKBestå’ŒSelectPercentileç±»

| **å‚æ•°**                       | **è¯´æ˜**                                                     |
| ------------------------------ | ------------------------------------------------------------ |
| score_func                     | ç»™å‡ºç»Ÿè®¡æŒ‡æ ‡çš„å‡½æ•°ï¼Œå¸¸ç”¨å‡½æ•°å¦‚ä¸‹ï¼š  â€œf_regressionâ€ï¼ˆé»˜è®¤ï¼‰ï¼šç›¸å…³ç³»æ•°æ£€éªŒï¼Œç”¨äºå›å½’é—®é¢˜  â€œchi2â€ï¼šå¡æ–¹æ£€éªŒï¼Œç”¨äºåˆ†ç±»é—®é¢˜  â€œmutual_info_regressionâ€ï¼šè®¡ç®—äº’ä¿¡æ¯ï¼Œç”¨äºå›å½’é—®é¢˜  â€œf_classifâ€ï¼šåŸºäºæ–¹å·®åˆ†æçš„Fæ£€éªŒï¼Œç”¨äºåˆ†ç±»é—®é¢˜ |
| k ï¼ˆSelectKBestï¼‰              | æŒ‡å®šä¿ç•™å¾—åˆ†æœ€é«˜ç‰¹å¾çš„ä¸ªæ•°ï¼Œé»˜è®¤ä¸º10                         |
| Percentileï¼ˆSelectPercentileï¼‰ | æŒ‡å®šä¿ç•™å¾—åˆ†æœ€é«˜çš„å‰ç™¾åˆ†ä¹‹å‡ çš„ç‰¹å¾ï¼Œé»˜è®¤ä¸º10ï¼ˆ10%ï¼‰          |

| **å±æ€§** | **è¯´æ˜**                                            |
| -------- | --------------------------------------------------- |
| scores_  | è¿”å›å„ç‰¹å¾çš„å¾—åˆ†                                    |
| pvalues_ | è¿”å›å„ç‰¹å¾å¾—åˆ†çš„på€¼ï¼Œè‹¥score_funcä»…è¿”å›å¾—åˆ†ï¼Œåˆ™ä¸ºæ—  |

```python
# å¡æ–¹æ£€éªŒç‰¹å¾é€‰æ‹©
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

# ä¸ç›®æ ‡å˜é‡æœ€ç›¸å…³çš„7ä¸ªç‰¹å¾
sel_chi = SelectKBest(chi2,k=7).fit(X, y) 

# å„ä¸ªç‰¹å¾å˜é‡ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³ç¨‹åº¦
print(sel_chi.scores_) 

# è¿”å›ç­›é€‰ç»“æœ
print(pd.Series(sel_chi.get_support(), index=X.columns))
'''
age
cp
chol
thalach
exang
oldpeak
ca
'''
# ç‰¹å¾é€‰æ‹©å¯è§†åŒ–
import matplotlib.pyplot as plt
%matplotlib inline

# å°†ç‰¹å¾çš„é€‰å–è¿›è¡Œå¯è§†åŒ–â€”â€”çº¢è‰²ä¸ºTrueï¼Œç´«è‰²ä¸ºFalse
plt.figure(figsize=(8, 6))
plt.matshow(sel_chi.get_support().reshape(1, -1), cmap='rainbow')
plt.xticks(range(13), X.columns)
# ç»“æœå¦‚ä¸‹å›¾ï¼ˆå‰è€…å¡æ–¹ï¼Œåè€…æ–¹å·®ï¼‰
```

![](image/tzxz.png)



#### åŒ…è£¹å¼

##### é€’å½’ç‰¹å¾æ¶ˆé™¤æ³•ï¼ˆRFEï¼‰

æ€æƒ³ï¼šåˆ©ç”¨å¯ä»¥å­¦ä¹ åˆ°ç‰¹å¾æƒé‡æˆ–é‡è¦æ€§çš„æ¨¡å‹ï¼Œé€šè¿‡é€’å½’çš„æ–¹å¼å‡å°‘ç‰¹å¾ä¸ªæ•°æ¥è¿›è¡Œç‰¹å¾é€‰æ‹©

æ­¥éª¤:

*   åŸºäºæ‰€æœ‰ç‰¹å¾è®­ç»ƒæ¨¡å‹ï¼Œå¾—åˆ°æ¯ä¸ªç‰¹å¾çš„æƒé‡æˆ–é‡è¦æ€§
*   å‰”é™¤æƒé‡æˆ–é‡è¦æ€§æœ€å°çš„ç‰¹å¾ï¼ŒåŸºäºæ–°çš„ç‰¹å¾é›†åˆè®­ç»ƒæ¨¡å‹
*   æœ€åï¼Œé‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œè¿›è¡Œé€’å½’æ¶ˆé™¤ï¼Œç›´åˆ°å‰©ä¸‹çš„ç‰¹å¾ä¸ªæ•°æ»¡è¶³æ¡ä»¶ä¸ºæ­¢

æ‰§è¡ŒRFEçš„è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥é€šè¿‡äº¤å‰éªŒè¯çš„æ–¹å¼æ¥è¯„ä»·æ¨¡å‹åœ¨æŸä¸ªç‰¹å¾é›†åˆä¸Šçš„è¡¨ç°ï¼Œä»¥æ­¤æ¥é€‰æ‹©æœ€ä½³çš„ç‰¹å¾é›†åˆã€‚

##### RFEç±»

| **å‚æ•°**             | **è¯´æ˜**                                                     |
| -------------------- | ------------------------------------------------------------ |
| estimator            | ç›‘ç£å­¦ä¹ å™¨ï¼Œå¿…é¡»åŒ…å«coef_æˆ–feature_importances_å±æ€§          |
| n_features_to_select | æŒ‡å®šé€‰å‡ºå‡ ä¸ªç‰¹å¾ï¼Œé»˜è®¤ä¸ºNoneï¼ˆæ‰€æœ‰ç‰¹å¾çš„ä¸€åŠï¼‰               |
| step                 | æ¯æ¬¡è¿­ä»£å‰”é™¤çš„ç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º1ï¼Œå¤§äº1ä¸ºå‰”é™¤çš„ä¸ªæ•°ï¼Œ0~1ä¹‹é—´ä¸ºå‰”é™¤çš„æ¯”ä¾‹ |

| **å±æ€§** | **è¯´æ˜**                       |
| -------- | ------------------------------ |
| support_ | æŒ‡æ˜å“ªäº›ç‰¹å¾è¢«é€‰æ‹©ï¼ˆå¸ƒå°”æ•°ç»„ï¼‰ |
| ranking_ | ç‰¹å¾çš„æ’åï¼ˆè¡¨ç¤ºè¢«å‰”é™¤çš„é¡ºåºï¼‰ |

```python
# åŸºäºå†³ç­–æ ‘çš„RFE
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier

# æ‰§è¡ŒRFE
rfe = RFE(DecisionTreeClassifier(random_state=10), n_features_to_select=7)
rfe.fit_transform(train_X,train_y)

# å…ˆå¯¹test_Xè¿›è¡Œè½¬æ¢åï¼Œå†ç”¨è®¾å®šå¥½çš„å­¦ä¹ å™¨è¿›è¡Œé¢„æµ‹
y_pred = rfe.predict(test_X)

# å°†test_Xè¿›è¡Œç‰¹å¾é€‰æ‹©åï¼Œå†ç”¨è®¾å®šå¥½çš„å­¦ä¹ å™¨è¯„åˆ†
rfe.score(test_X, test_y)

# ç‰¹å¾æ’å
rfe.ranking_
'''
ç‰¹å¾é€‰æ‹©å‰å†³ç­–æ ‘æ¨¡å‹è¯„åˆ†ï¼š0.74
ç‰¹å¾é€‰æ‹©åå†³ç­–æ ‘æ¨¡å‹è¯„åˆ†ï¼š0.79

ç‰¹å¾	æ’å
age	1ï¼ˆè¢«é€‰æ‹©ï¼‰
sex         	4ï¼ˆç¬¬5æ¬¡è¢«å‰”é™¤ï¼‰
cp          	1ï¼ˆè¢«é€‰æ‹©ï¼‰
trestbps    	1ï¼ˆè¢«é€‰æ‹©ï¼‰
chol        	1ï¼ˆè¢«é€‰æ‹©ï¼‰
fbs         	7ï¼ˆç¬¬1æ¬¡è¢«å‰”é™¤ï¼‰
restecg     	6ï¼ˆç¬¬2æ¬¡è¢«å‰”é™¤ï¼‰
thalach     	1ï¼ˆè¢«é€‰æ‹©ï¼‰
exang       	3ï¼ˆç¬¬6æ¬¡è¢«å‰”é™¤ï¼‰
oldpeak     	1ï¼ˆè¢«é€‰æ‹©ï¼‰
slope      	  2ï¼ˆç¬¬7æ¬¡è¢«å‰”é™¤ï¼‰
ca          	1ï¼ˆè¢«é€‰æ‹©ï¼‰
thal 	       5ï¼ˆç¬¬3æ¬¡è¢«å‰”é™¤ï¼‰
'''
```

##### RFECVç±»

| **å‚æ•°**               | **è¯´æ˜**                                                     |
| ---------------------- | ------------------------------------------------------------ |
| estimator              | ç›‘ç£å­¦ä¹ å™¨ï¼Œå¿…é¡»åŒ…å«coef_æˆ–feature_importances_å±æ€§          |
| min_features_to_select | éœ€è¦é€‰æ‹©çš„æœ€å°ç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º1                                |
| step                   | æ¯æ¬¡è¿­ä»£å‰”é™¤çš„ç‰¹å¾æ•°ï¼Œé»˜è®¤ä¸º1ï¼Œå¤§äº1ä¸ºå‰”é™¤çš„ä¸ªæ•°ï¼Œ0~1ä¹‹é—´ä¸ºå‰”é™¤çš„æ¯”ä¾‹ |
| cv                     | äº¤å‰éªŒè¯çš„æŠ˜æ•°ï¼Œé»˜è®¤ä¸º3                                      |
| scoring                | å­—ç¬¦ä¸²æˆ–å¯è°ƒç”¨å¯¹è±¡ï¼Œç”¨äºè¯„ä¼°å­¦ä¹ å™¨çš„é¢„æµ‹æ€§èƒ½                 |

| grid_scores_ | å„ç‰¹å¾å­é›†çš„äº¤å‰éªŒè¯å¾—åˆ†       |
| ------------ | ------------------------------ |
| support_     | æŒ‡æ˜å“ªäº›ç‰¹å¾è¢«é€‰æ‹©ï¼ˆå¸ƒå°”æ•°ç»„ï¼‰ |
| ranking_     | ç‰¹å¾çš„æ’åï¼ˆè¡¨ç¤ºè¢«å‰”é™¤çš„é¡ºåºï¼‰ |

```python
# åŸºäºå†³ç­–æ ‘çš„äº¤å‰éªŒè¯RFE
# é€’å½’ç‰¹å¾æ¶ˆé™¤+äº¤å‰éªŒè¯æ³•
from sklearn.feature_selection import RFECV

rfeCV = RFECV(DecisionTreeClassifier(random_state=10), cv=5,scoring='roc_auc')
rfeCV.fit(train_X,train_y)

## æŸ¥çœ‹ç‰¹å¾æ’å
print(pd.Series(rfeCV.ranking_, index=X.columns))
'''
ç‰¹å¾	          æ’å
age	          1ï¼ˆè¢«é€‰æ‹©ï¼‰
sex         	1ï¼ˆè¢«é€‰æ‹©ï¼‰
cp          	1ï¼ˆè¢«é€‰æ‹©ï¼‰
trestbps    	1ï¼ˆè¢«é€‰æ‹©ï¼‰
chol        	1ï¼ˆè¢«é€‰æ‹©ï¼‰
fbs         	4ï¼ˆç¬¬1æ¬¡è¢«å‰”é™¤ï¼‰
restecg     	3ï¼ˆç¬¬2æ¬¡è¢«å‰”é™¤ï¼‰
thalach     	1ï¼ˆè¢«é€‰æ‹©ï¼‰
exang       	1ï¼ˆè¢«é€‰æ‹©ï¼‰
oldpeak     	1ï¼ˆè¢«é€‰æ‹©ï¼‰
slope      	  1ï¼ˆè¢«é€‰æ‹©ï¼‰
ca          	1ï¼ˆè¢«é€‰æ‹©ï¼‰
thal 	        2ï¼ˆç¬¬3æ¬¡è¢«å‰”é™¤ï¼‰
'''
```



#### åµŒå…¥å¼

æ€æƒ³ï¼šæ¨¡å‹è®­ç»ƒå’Œç‰¹å¾é€‰æ‹©ç»“åˆï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è‡ªåŠ¨é€‰æ‹©ç‰¹å¾

æ–¹æ³•ï¼š
l_1æ­£åˆ™åŒ–é¡¹ï¼šä¼˜åŒ–è¿‡ç¨‹ä¸­å¯å¾—åˆ°ç¨€ç–è§£ï¼Œç­‰åŒäºç‰¹å¾é€‰æ‹©ï¼Œå¦‚LASSO
ç‰¹å¾æƒé‡æˆ–é‡è¦æ€§ï¼šè®¾å®šé˜ˆå€¼ï¼Œè‹¥æŸç‰¹å¾çš„æƒé‡æˆ–é‡è¦æ€§ä½äºé˜ˆå€¼ï¼Œåˆ™å°†è¯¥ç‰¹å¾å‰”é™¤ï¼Œ
   å¦‚ï¼šå†³ç­–æ ‘ã€SVM

##### SelectFromModelç±»

| **å‚æ•°**  | **è¯´æ˜**                                                     |
| --------- | ------------------------------------------------------------ |
| estimator | ç›‘ç£å­¦ä¹ å™¨ï¼Œï¼Œå¿…é¡»åŒ…å«coef_æˆ–feature_importances_å±æ€§        |
| threshold | ç”¨äºç‰¹å¾é€‰æ‹©çš„é˜ˆå€¼ï¼Œè®¾ç½®æ–¹æ³•å¦‚ä¸‹ï¼š       è‹¥ä¸ºæµ®ç‚¹å‹ï¼šåˆ™è¡¨ç¤ºé˜ˆå€¼çš„ç»å¯¹å¤§å°       è‹¥ä¸ºå­—ç¬¦ä¸²ï¼Œå¯è®¾ä¸ºï¼šâ€meanâ€ï¼ˆé»˜è®¤ï¼‰ï¼ˆæƒé‡æˆ–é‡è¦æ€§çš„å‡å€¼ï¼‰                        â€œmedianâ€ï¼ˆæƒé‡æˆ–é‡è¦æ€§çš„ä¸­ä½æ•°ï¼‰       è‹¥estimatoråŒ…å«â€œl1â€æ­£åˆ™åŒ–é¡¹ï¼Œåˆ™é»˜è®¤å€¼ä¸º1e-5 |

```python
# åŸºäºéšæœºæ£®æ—çš„åµŒå…¥å¼ç‰¹å¾é€‰æ‹©
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

# ä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºç›‘ç£å­¦ä¹ å™¨
select = SelectFromModel(RandomForestClassifier(n_estimators=200, random_state=10), threshold='median')
select.fit(train_X, train_y)
'''
ç‰¹å¾é€‰æ‹©åå»ºç«‹éšæœºæ£®æ—æ¨¡å‹çš„AUCå€¼ï¼š0.80
'''
```

## **é™ç»´**

### é™ç»´æ–¹æ³•ä¸sklearné™ç»´æ¨¡å—æ¦‚è¿°

é™ç»´æœ¬è´¨ä¸Šæ˜¯é™ä½æ•°æ®ç»´åº¦ï¼Œæ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆLDAä¾‹å¤–ï¼‰

é™ç»´çš„ç›®çš„ï¼š

* æ‘’å¼ƒå†—ä½™ä¿¡æ¯ï¼ˆå…±çº¿æ€§ï¼‰ï¼Œåˆ é™¤å™ªéŸ³
* é™ä½è®¡ç®—æ—¶é—´ã€ç©ºé—´å¤æ‚åº¦
* ç®€å•æ¨¡å‹å…·æœ‰æ›´å¥½çš„é²æ£’æ€§
* å®ç°æ•°æ®å¯è§†åŒ–
* å¯ä»¥ä½œä¸ºç‰¹å¾æå–çš„æ–¹æ³•

é™ç»´æ–¹æ³•ï¼š

- åŸºäºçŸ©é˜µåˆ†è§£çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼šPCAã€LDAã€NMF
- åŸºäºæµå½¢å­¦ä¹ çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼šMDSã€LLEã€t-SNE

![](image/é™ç»´æ–¹æ³•.JPG)

**Sklearn é™ç»´æ¨¡å—ï¼š**

- Sklearnä¸­é™ç»´çš„æ–¹æ³•ä¸»è¦åˆ†å¸ƒåœ¨ä»¥ä¸‹ä¸‰ä¸ªæ¨¡å—ï¼š
  - sklearn.decompositionï¼šåŒ…å«äº†ç»å¤§éƒ¨åˆ†çš„çŸ©é˜µåˆ†è§£ç®—æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬PCAã€æ ¸PCAã€NMFæˆ–ICA
  - sklearn.discriminant_analysisï¼šåŒ…å«äº†LDAå’ŒQDAä¸¤ç§åˆ¤åˆ«åˆ†ææ–¹æ³•
  - sklearn.manifoldï¼šåŒ…å«äº†åŸºäºæµå½¢å­¦ä¹ çš„æ•°æ®é™ç»´æ–¹æ³•

æµå½¢æ˜¯å±€éƒ¨å…·æœ‰æ¬§å‡ é‡Œå¾—ç©ºé—´æ€§è´¨çš„ç©ºé—´ï¼Œä¸åŒäºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•å‡è®¾æ•°æ®å­˜åœ¨äºæ¬§å¼ç©ºé—´ï¼Œæµå½¢å­¦ä¹ å‡è®¾æ•°æ®åˆ†å¸ƒåµŒå…¥åœ¨å¤–å›´é«˜ç»´ç©ºé—´çš„ä¸€ä¸ªæ½œåœ¨æµå½¢ä¸Šã€‚

sklearn.decomposition æ¨¡å—

|              ç±»              | è¯´æ˜                  |
| :--------------------------: | --------------------- |
|      decomposition.PCA       | ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰     |
|      decomposition.NMF       | éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰   |
|    decomposition.FastICA     | ç‹¬ç«‹ä¸»æˆåˆ†åˆ†æï¼ˆICAï¼‰ |
| decomposition.FactorAnalysis | å› å­åˆ†æ              |
|   decomposition.KernelPCA    | æ ¸PCA                 |
|   decomposition.SparsePCA    | ç¨€ç–PCA               |
|  decomposition.SparseCoder   | ç¨€ç–ç¼–ç               |

sklearn.discriminant_analysis æ¨¡å—

|                         ç±»                          | è¯´æ˜                |
| :-------------------------------------------------: | ------------------- |
|  discriminant_analysis.LinearDiscriminantAnalysis   | çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰ |
| discriminant_analysis.QuadraticDiscriminantAnalysis | äºŒæ¬¡åˆ¤åˆ«åˆ†æï¼ˆQDAï¼‰ |

sklearn.manifold æ¨¡å—

|               ç±»                | è¯´æ˜                        |
| :-----------------------------: | --------------------------- |
| manifold.LocallyLinearEmbedding | å±€éƒ¨çº¿æ€§åµŒå…¥ï¼ˆLLEï¼‰         |
|          manifold.MDS           | å¤šç»´å°ºåº¦å˜æ¢ï¼ˆMDSï¼‰         |
|          manifold.TSNE          | t åˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ï¼ˆt-SNEï¼‰ |
|         manifold.Isomap         | ç­‰åº¦é‡æ˜ å°„ï¼ˆIsomapï¼‰        |



|       é€šç”¨æ–¹æ³•       | è¯´æ˜               |
| :------------------: | ------------------ |
|       .fit(X)        | è®­ç»ƒæ¨¡å‹           |
|    .transform(X)     | è¿›è¡Œé™ç»´           |
|  .fit_transform(X)   | è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œé™ç»´ |
| Inverse_transform(X) | è¿›è¡Œå‡ç»´ï¼ˆé€†å˜æ¢ï¼‰ |

### åŸºäºçŸ©é˜µåˆ†è§£çš„é™ç»´æ–¹æ³•



#### ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰

åŸºæœ¬æ€æƒ³ï¼šæ„é€ åŸå§‹ç‰¹å¾çš„ä¸€ç³»åˆ—çº¿æ€§ç»„åˆå½¢æˆä½ç»´çš„ç‰¹å¾ï¼Œä»¥å»é™¤æ•°æ®çš„ç›¸å…³æ€§ï¼Œå¹¶ä½¿é™ç»´åçš„æ•°æ®æœ€å¤§ç¨‹åº¦åœ°ä¿æŒåŸå§‹é«˜ç»´æ•°æ®çš„æ–¹å·®ä¿¡æ¯

![](image/PCA.png)

PCA ç±»ï¼š


|   **å‚æ•°**   | **è¯´æ˜**                                                     |
| :----------: | :----------------------------------------------------------- |
| n_components | ä¸€ä¸ªæ•´æ•°ï¼ŒæŒ‡å®šé™ç»´åçš„ç»´æ•°ï¼š       å¦‚æœä¸ºâ€œNoneâ€ï¼Œåˆ™å€¼ä¸ºmin{n_samples, n_features}       å¦‚æœä¸ºâ€œmleâ€ï¼Œåˆ™ä½¿ç”¨Minka â€˜s MLEç®—æ³•æ¥ä¼°è®¡é™ç»´åçš„ç»´æ•°       å¦‚æœä¸ºå¤§äº0å°äº1çš„æµ®ç‚¹æ•°ï¼Œåˆ™å€¼ä¸ºé™ç»´åçš„ç»´æ•°å åŸå§‹ç»´æ•°çš„ç™¾åˆ†æ¯” |
|    whiten    | å¯¹é™ç»´åçš„æ•°æ®çš„æ¯ä¸ªç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿æ–¹å·®ä¸º1ï¼ˆç™½åŒ–ï¼‰ï¼Œé»˜è®¤ä¸ºFalse |
|  svd_solver  | æŒ‡å®šå¥‡å¼‚å€¼åˆ†è§£SVDçš„æ–¹æ³•ï¼š       å¦‚æœä¸ºâ€œrandomizedâ€ï¼Œåˆ™ä½¿ç”¨åŠ å¿«SVDçš„éšæœºç®—æ³•æ¥åº”å¯¹æ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µ       å¦‚æœä¸ºâ€œfullâ€ï¼Œåˆ™ä¸ºä¼ ç»ŸSVDç®—æ³•       å¦‚æœä¸ºâ€œarpackâ€ï¼Œåˆ™ä¸ºscipyåº“çš„sparse  SVDç®—æ³•       å¦‚æœä¸ºâ€œautoâ€ï¼Œåˆ™è‡ªåŠ¨é€‰æ‹©SVDç®—æ³•æ¥é™ç»´ |

|         **å±æ€§**          | **è¯´æ˜**       |
| :-----------------------: | -------------- |
|    explained_variance_    | è¿”å›æ–¹å·®è´¡çŒ®å€¼ |
| explained_variance_ratio_ | è¿”å›æ–¹å·®è´¡çŒ®ç‡ |

```python
## å¯¼å…¥ç™Œç—‡æ•°æ®é›†åŠæ•°æ®é¢„å¤„ç†
# è½½å…¥æ•°æ®
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

# æŸ¥çœ‹æ•°æ®ç»´åº¦
print(cancer.data.shape)
# æ•°æ®ç»´åº¦ä¸º ï¼ˆ569, 30ï¼‰

# åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,
test_size=.2, random_state=10, stratify=cancer.target)		
# æ•°æ®æ ‡å‡†åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# è®­ç»ƒé›†æ ‡å‡†åŒ–
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

# æµ‹è¯•é›†æ ‡å‡†åŒ–
X_test_scaled = scaler.transform(X_test)
```

```python
## å»ºç«‹ SVM æ¨¡å‹å¹¶æŸ¥çœ‹æ¨¡å‹æ•ˆæœ
# å»ºç«‹è½¯é—´éš”SVMæ¨¡å‹
from sklearn.svm import LinearSVC

model_svm = LinearSVC(random_state=10)
model_svm.fit(X_train_scaled, y_train)

# å¾—åˆ°æµ‹è¯•é›†é¢„æµ‹æ ‡ç­¾
y_pred = model_svm.predict(X_test_scaled)

# è¾“å‡ºæµ‹è¯•é›†é¢„æµ‹æ­£ç¡®ç‡
model_svm.score(X_test_scaled, y_test)

# ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­å›¾
import seaborn as sns
from sklearn.metrics import confusion_matrix

ax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', xticklabels=["malignant(0)","benign(1)"], yticklabels=["malignant(0)","benign(1)"])
ax.set_ylabel('True')
ax.set_xlabel('Predict')
ax.set_title('Confusion Matrix Heatmap')
# å¾—åˆ°æµ‹è¯•é›†é¢„æµ‹æ­£ç¡®ç‡ä¸º ï¼š 0.956
```

![](image/9.0SVM.png)

åˆ©ç”¨ PCA è¿›è¡Œé™ç»´ï¼š

```python
# PCAé™ç»´
from sklearn.decomposition import PCA

pca = PCA(n_components=15)
## è®­ç»ƒé›†é™ç»´
X_train_pca = pca.fit_transform(X_train_scaled)

## æµ‹è¯•é›†é™ç»´
X_test_pca = pca.transform(X_test_scaled)

# æŸ¥çœ‹æ–¹å·®è´¡çŒ®ç‡
sum(pca.explained_variance_ratio_)

# æ–¹å·®è´¡çŒ®ç‡ä¸ºï¼š0.987
```

æŸ¥çœ‹é™ç»´åçš„æ¨¡å‹æ•ˆæœï¼š

```python
# å†æ¬¡å»ºç«‹è½¯é—´éš”SVMæ¨¡å‹æ¯”è¾ƒæ•ˆæœ
model_svm.fit(X_train_pca, y_train)

## å¾—åˆ°æµ‹è¯•é›†é¢„æµ‹æ ‡ç­¾
y_pred_pca = model_svm.predict(X_test_pca)

## è¾“å‡ºæµ‹è¯•é›†é¢„æµ‹æ­£ç¡®ç‡
model_svm.score(X_test_pca, y_test)

# ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­å›¾
import seaborn as sns
from sklearn.metrics import confusion_matrix

ax = sns.heatmap(confusion_matrix(y_test, y_pred_pca), 
                 annot=True, fmt='d', 
                 xticklabels=["malignant(0)","benign(1)"], yticklabels=["malignant(0)","benign(1)"])
ax.set_ylabel('True')
ax.set_xlabel('Predict')
ax.set_title('Confusion Matrix Heatmap')
# æµ‹è¯•é›†é¢„æµ‹æ­£ç¡®ç‡ä¸ºï¼š0.965
```

![](image/9.1.png)

åˆ©ç”¨ PCA å°†è®­ç»ƒé›†é™è‡³2ç»´è¿›è¡Œå¯è§†åŒ–ï¼š

```python
# é™ç»´
X_train_pca = PCA(n_components=2).fit_transform(X_train_scaled)

# ç»˜å›¾
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 8))
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train)
plt.xlabel('First principal component')
plt.ylabel('Second principal component')
# ä»å›¾ç‰‡å¯ä»¥çœ‹å‡ºï¼Œä¸¤ä¸ªç±»è¢«å¾ˆå¥½çš„åˆ†å¼€äº†
```

![](image/9.2.png)

#### çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLDAï¼‰

çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLinear Discriminant Analysis, LDAï¼‰æ˜¯ä¸€ç§å…¸å‹çš„æœ‰ç›‘ç£çº¿æ€§é™ç»´æ–¹æ³•ã€‚

LDAçš„ç›®æ ‡æ˜¯åˆ©ç”¨æ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾ä¿¡æ¯ï¼Œæ‰¾åˆ°ä¸€ä¸ªåˆ©äºæ•°æ®åˆ†ç±»çš„çº¿æ€§ä½ç»´è¡¨ç¤ºã€‚

è¿™ä¸ªç›®æ ‡å¯ä»¥ä»ä¸¤ä¸ªè§’åº¦æ¥é‡åŒ–

- ç¬¬ä¸€ä¸ªè§’åº¦æ˜¯ä½¿å¾—é™ç»´åç›¸åŒç±»æ ·æœ¬å°½å¯èƒ½è¿‘ï¼Œä½¿ç”¨ç±»å†…ç¦»æ•£åº¦ (within-class scatter)åº¦é‡
- ç¬¬äºŒä¸ªè§’åº¦æ˜¯ä½¿å¾—é™ç»´åä¸åŒç±»æ ·æœ¬å°½å¯èƒ½è¿œï¼Œä½¿ç”¨ç±»é—´ç¦»æ•£åº¦ (between-class scatter)åº¦é‡

LDAç®—æ³•åŸç†ç¤ºæ„ï¼š

- å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå°†åŸå§‹æ ·æœ¬åˆ†ä¸ºä¸¤ç±»å¹¶ç”¨ä¸åŒå›¾å½¢è¡¨ç¤º
- åˆ©ç”¨PCAæ–¹æ³•ï¼Œå°†å¾—åˆ°ç›´çº¿ğ‘ï¼Œå¯è§é™ç»´åçš„ä¸¤ç±»æ•°æ®æ— æ³•å¾ˆå¥½åŒºåˆ†
- åˆ©ç”¨LDAæ–¹æ³•ï¼Œèƒ½å¤Ÿæ‰¾åˆ°ç›´çº¿ğ‘ï¼Œä½¿å¾—é™ç»´åä¸¤ç±»æ•°æ®å¾ˆå¥½åœ°è¢«åŒºåˆ†å¼€
- ![](image/9.3.png)

ç±»å†…ç¦»æ•£åº¦å’Œç±»é—´ç¦»æ•£åº¦ï¼š

- ç±»å†…ç¦»æ•£åº¦
  - ç¬¬ $c$ ç±»æ ·æœ¬çš„ç±»å†…ç¦»æ•£åº¦ï¼š$S_c = \overset{n_c}\sum\limits_{i=1}(x_i-m_c)(x_i-m_c)^T$
  - æ€»çš„ç±»å†…ç¦»æ•£åº¦çŸ©é˜µï¼š$S_w=\sum\limits_{c=1}\frac{n_c}{n}S_c$
- ç±»é—´ç¦»æ•£åº¦
  - $S_b = \overset{C}\sum\limits_{c=1}\frac{n_c}{n}(m_c-m)(m_c-m)^T$

![](image/9.4LDA.png)

LinearDiscriminantAnalysisç±»ï¼š

| **å‚æ•°**     | **è¯´æ˜**                                                     |
| ------------ | ------------------------------------------------------------ |
| solver       | è®¾å®šæ±‚è§£æœ€ä¼˜åŒ–é—®é¢˜çš„ç®—æ³•{â€˜svdâ€™  ï¼ˆå¥‡å¼‚å€¼åˆ†è§£ï¼‰,â€™lsqrâ€™  ï¼ˆæœ€å°å¹³æ–¹å·®ç®—æ³•ï¼‰æˆ–â€˜eigenâ€™  ï¼ˆç‰¹å¾å€¼åˆ†è§£ç®—æ³•ï¼‰} |
| shrinkage    | æ­£åˆ™åŒ–å‚æ•°ï¼Œå¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œé»˜è®¤ä¸ºNoneï¼Œå¯è®¾ä¸ºâ€˜autoâ€™è‡ªä¸»é€‰æ‹©æ˜¯å¦æ­£åˆ™åŒ–æˆ–è®¾ä¸º(0,  1)ä¹‹é—´çš„å€¼ï¼Œä½†ä»…åœ¨solver=â€˜lsqrâ€™æˆ–â€˜eigenâ€™ä¸‹æ‰æœ‰æ„ä¹‰ |
| priors       | è®¾å®šæ¯ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ï¼Œé»˜è®¤ä¸ºNoneæ—¶è®¤ä¸ºå„ç±»å…ˆéªŒæ¦‚ç‡ç­‰å¯èƒ½   |
| n_components | è®¾å®šæ•°æ®é™ç»´åçš„ç»´åº¦ï¼Œé»˜è®¤ä¸ºNone                             |

| **å±æ€§**                  | **è¯´æ˜**       |
| ------------------------- | -------------- |
| covariance                | è¿”å›æ ·æœ¬ç›¸å…³é˜µ |
| explained_variance_ratio_ | è¿”å›æ–¹å·®è´¡çŒ®ç‡ |

æ›´å¤šå‚æ•°è¯¦æƒ…è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š

[https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)

```python
## åˆ©ç”¨ LDA å¯¹é¸¢å°¾èŠ±æ•°æ®é›†è¿›è¡Œåˆ†ç±»
# è½½å…¥æ•°æ®
from sklearn import datasets
iris = datasets.load_iris()

X = iris.data
y = iris.target
target_names = iris.target_names

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=.2, random_state=10, stratify=y)

# è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ¨¡å‹
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=2)
lda.fit(X_train, y_train)

y_pred = lda.predict(X_test)

lda.score(X_test, y_test)

# è¾“å‡ºæ–¹å·®è´¡çŒ®ç‡
print(sum(lda.explained_variance_ratio_))

# æµ‹è¯•é›†åˆ†ç±»æ­£ç¡®ç‡ï¼š1.0
# æ–¹å·®è´¡çŒ®ç‡ï¼š0.99
```

å°†PCAå’ŒLDAçš„é™ç»´ç»“æœè¿›è¡Œå¯è§†åŒ–å¹¶è¿›è¡Œå¯¹æ¯”ï¼š

```python
# PCAé™ç»´
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train)

# LDAé™ç»´
X_lda = lda.fit(X_train, y_train).transform(X_train)

# è®¾å®šç‚¹çš„é¢œè‰²
colors = ['navy', 'turquoise', 'darkorange']

## å¯è§†åŒ–
plt.figure(figsize=(10, 15))
plt.subplot(2, 1, 1)
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_pca[y_train == i, 0], X_pca[y_train == i, 1], color=color, alpha=.8,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('PCA of IRIS dataset')

plt.subplot(2, 1, 2)
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(X_lda[y_train == i, 0], X_lda[y_train == i, 1], alpha=.8, color=color,
                label=target_name)
plt.legend(loc='best', shadow=False, scatterpoints=1)
plt.title('LDA of IRIS dataset')
```

![](image/9.5.png)

#### éè´ŸçŸ©é˜µåˆ†è§£ï¼ˆNMFï¼‰

éè´ŸçŸ©é˜µåˆ†è§£(Non-negative Matrix Factorization)

ä¸€ç§ä¸»æˆåˆ†åˆ†æçš„æ–¹æ³•ï¼Œè¦æ±‚æ•°æ®å’Œæˆåˆ†éƒ½è¦éè´Ÿï¼Œå¯¹å›¾åƒæ•°æ®ååˆ†æœ‰æ•ˆã€‚

NMFå¸Œæœ›æ‰¾åˆ°ä¸¤ä¸ªçŸ©é˜µ $ğ–$ å’Œ $ğ‡$ï¼Œä½¿å¾— $ğ–ğ‡$ ä¸åŸæ•°æ® $X$ çš„è¯¯å·®å°½å¯èƒ½çš„å°‘ï¼Œå³ï¼š
$$
\arg \min\limits_{W.H} \frac{1}{2} ||X-WH||^2_{Fro}
$$


å…¶ä¸­ $||*||_{Fro}$ è¡¨ç¤º $Frobenius$ èŒƒæ•°ï¼Œ$X$ ä»£è¡¨ $N \times p$ çš„æ•°æ®çŸ©é˜µï¼Œ$W$ æ˜¯ $N\times r$ çš„åŸºçŸ©é˜µï¼Œ $H$ æ˜¯$r \times p$ çš„æƒé‡çŸ©é˜µï¼Œ $r \leqslant \maxâ¡(N,p)$ï¼Œå…¶ä¸­ $X$ï¼Œ$W$ å’Œ $H$ éƒ½æ˜¯éè´ŸçŸ©é˜µã€‚

åŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼Œå…¶ä¸­ï¼Œ$\alpha$ ä¸º $l_1$ ä¸ $l_2$ cæ­£åˆ™åŒ–é¡¹çš„å‚æ•°ï¼Œè€Œ $\rho$ ä¸º $l_1$ æ­£åˆ™åŒ–é¡¹å æ€»æ­£åˆ™åŒ–é¡¹çš„æ¯”ä¾‹ï¼š
$$
\arg \min\limits_{W,H} \frac{1}{2} ||X-WH||^2_{Fro} + \alpha \rho ||W||_1 + \alpha\rho||H||_1 + \frac{\alpha(1-\rho)}{2}||W||^2_{Fro} + \frac{\alpha(1-\rho)}{2}||H||^2_{Fro}
$$
åŠ å…¥æ­£åˆ™åŒ–é¡¹åï¼Œå¸¸ç”¨çš„æ‹Ÿç‰›é¡¿æ³•å’Œæ¢¯åº¦ä¸‹é™æ³•å¹¶ä¸é€‚ç”¨ï¼ŒSklearnä¸­é‡‡ç”¨åæ ‡è½´ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–ã€‚

ç®—æ³•æµç¨‹è¯·å‚è§è®ºæ–‡[ã€ŠAlgorithms for Non-negative Matrix Factorizationã€‹](https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf)

NMF ç±»ï¼š

| **å‚æ•°**     | **æ„ä¹‰**                                                     |
| ------------ | ------------------------------------------------------------ |
| n_components | è®¾å®šæ•°æ®é™ç»´åçš„ç»´åº¦                                         |
| init         | åˆå§‹åŒ–æ–¹æ³•ï¼Œâ€™â€™randomâ€˜ï¼Œnndsvdâ€™ï¼Œâ€˜nndsvdaâ€™ï¼Œâ€˜nndsvdarâ€™ï¼Œâ€˜customâ€™ï¼š  <br />â€™randomâ€˜ï¼šéè´ŸéšæœºçŸ©é˜µ  <br />â€˜nndsvd'ï¼šéè´ŸåŒå¥‡å¼‚å€¼åˆ†è§£ï¼ˆæ›´é€‚åˆç¨€ç–æ€§ï¼‰  <br />'nndsvda'ï¼šé›¶å¡«å……å¹³å‡å€¼ï¼ˆå½“ä¸éœ€è¦ç¨€ç–æ—¶æ›´å¥½ï¼‰  <br />'nndsvdar'ï¼šé›¶å¡«å……å°çš„éšæœºå€¼ï¼ˆå½“ä¸éœ€è¦ç¨€ç–æ—¶ï¼Œé€šå¸¸æ›´å¿«ï¼Œæ›´å‡†ç¡®åœ°æ›¿ä»£NNDSVDa)  <br />â€˜customâ€™ï¼šè‡ªå®šä¹‰çŸ©é˜µ |
| solver       | é€‰æ‹©ä¼˜åŒ–ç®—æ³•ï¼Œåˆ†åˆ«æ˜¯â€˜cdâ€™ï¼šåæ ‡ä¸‹é™æ³•ï¼Œâ€™pgâ€™ï¼šæ¢¯åº¦æŠ•å½±ç®—æ³•     |
| alpha        | æ€»çš„æ­£åˆ™åŒ–å¼ºåº¦ï¼Œé»˜è®¤ä¸º0                                      |
| l1_ratio     | l_1æ­£åˆ™åŒ–é¡¹å¼ºåº¦æ‰€å çš„æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º0ï¼Œå–å€¼åœ¨[0, 1]ä¹‹é—´         |

$A T \& T$ è„¸éƒ¨æ•°æ®é›†ï¼š

```python
	# AT&Tè„¸éƒ¨æ•°æ®é›†åŒ…å«1992å¹´4æœˆè‡³1994å¹´4æœˆåœ¨å®éªŒå®¤æ‹æ‘„çš„ä¸€ç»„è„¸éƒ¨å›¾åƒ
	# ä¸€å…± 400 å¼ å›¾ç‰‡ï¼Œæ¥è‡ª 40 ä¸ªä¸åŒçš„äººï¼Œæ¯ä¸ªäºº 10 å¼ å›¾ç‰‡ï¼Œå›¾ç‰‡å¤§å°ä¸º 64*64

# è½½å…¥æ•°æ®
from sklearn.datasets import fetch_olivetti_faces
dataset = fetch_olivetti_faces(shuffle=True, random_state=10)
faces = dataset.data

# æŸ¥çœ‹æ•°æ®çš„ç»´åº¦
faces.shape		# æ•°æ®ç»´åº¦ï¼š(400, 4096ï¼‰

# å±•ç¤ºæ•°æ®
## å±•ç¤º6å¼ å›¾ç‰‡ï¼ŒæŒ‰2è¡Œ3åˆ—æ”¾ç½®
n_row, n_col = 2, 3

# äººè„¸æ•°æ®å›¾ç‰‡çš„æ˜¾ç¤ºå¤§å°
image_shape = (64, 64)

# å®šä¹‰ç»˜å›¾å‡½æ•°
def plot_gallery(title, images, n_col=n_col, n_row=n_row):
    plt.figure(figsize=(10, 8))
    plt.suptitle(title, size=15)
    
    for i, comp in enumerate(images):
        plt.subplot(n_row, n_col, i + 1)
        
        # ä»¥ç°åº¦å›¾å½¢å¼æ˜¾ç¤º
        plt.imshow(comp.reshape(image_shape),cmap=plt.cm.gray)
        
        # ä¸æ˜¾ç¤ºåæ ‡è½´åˆ»åº¦
        plt.xticks(())
        plt.yticks(())

# ç»˜å›¾å±•ç¤ºæ•°æ®
plot_gallery("Original Faces", faces[:6])
```

![](image/9.6.png)

```python
# PCAé™ç»´
pca = PCA(n_components=6, whiten=True, random_state=10)
pca.fit(faces)
faces_pca = pca.components_

# è¿˜åŸå›¾åƒ
plot_gallery("PCA Faces", faces_pca[:6])
```

é¢éƒ¨ç‰¹å¾è¾ƒä¸ºæ¨¡ç³Šï¼š![](image/9.7.png)

```python
# NMFé™ç»´
nmf = NMF(n_components=6, init='nndsvda', random_state=10)
nmf.fit(faces)
faces_nmf = nmf.components_

# è¿˜åŸå›¾åƒ
plot_gallery("NMF Faces", faces_nmf[:6])
```

é¢éƒ¨ç‰¹å¾è¾ƒä¸ºæ¸…æ™°ï¼š![](image/9.8.png)

### åŸºäºæµå½¢å­¦ä¹ çš„é™ç»´æ–¹æ³•

æµå½¢ï¼šä¸€ä¸ªä½ç»´ç©ºé—´åœ¨ä¸€ä¸ªé«˜ç»´ç©ºé—´ä¸­è¢«æ‰­æ›²ä¹‹åçš„ç»“æœã€‚

ä½çº¬æµå½¢åµŒå…¥åœ¨é«˜çº¬ç©ºé—´ä¸­ã€‚

ä¸€ç§å½¢è±¡çš„è§£é‡Šï¼šâ€œä¸€å—å¸ƒæ˜¯äºŒç»´å¹³é¢ï¼ŒæŠŠå®ƒåœ¨ä¸‰ç»´ç©ºé—´ä¸­æ‰­ä¸€æ‰­ï¼Œå°±å˜æˆäº†ä¸€ä¸ªæµå½¢â€

![](image/9.9.png)

æµå½¢å­¦ä¹ ä¸é™ç»´ï¼šæ ·æœ¬åœ¨é«˜ç»´ç©ºé—´åˆ†å¸ƒå¤æ‚ï¼Œä½†åœ¨å±€éƒ¨ä¸Šå…·æœ‰æ¬§å¼ç©ºé—´çš„æ€§è´¨ã€‚åœ¨å±€éƒ¨å»ºç«‹é™ç»´æ˜ å°„å…³ç³»ï¼Œå†ç”±å±€éƒ¨æ¨å¹¿åˆ°å…¨å±€ã€‚æ•°æ®è¢«é™è‡³2ç»´æˆ–3ç»´ï¼Œæ–¹ä¾¿è¿›è¡Œå¯è§†åŒ–ã€‚

![](image/9.10.png)

#### å±€éƒ¨çº¿æ€§åµŒå…¥ï¼ˆLLEï¼‰

ç®—æ³•æ€æƒ³ï¼š

- å°†æ•°æ®é™åˆ°ä½ç»´ç©ºé—´ä¸­ï¼Œä½†æ˜¯ä¿ç•™æ•°æ®å±€éƒ¨çš„çº¿æ€§å…³ç³»
- æ¯ä¸€ä¸ªæ ·æœ¬ç‚¹å¯ä»¥å†™æˆå…¶ ğ‘˜ ä¸ªè¿‘é‚»ç‚¹çš„çº¿æ€§ç»„åˆï¼Œä»é«˜ç»´åµŒå…¥ï¼ˆembeddingï¼‰åˆ°ä½ç»´æ—¶å°½é‡ä¿æŒå±€éƒ¨çš„çº¿æ€§å…³ç³»

ç®—æ³•æ­¥éª¤ï¼š

1. é€‰æ‹©è¿‘é‚»æ ·æœ¬
   - å¯¹äºæ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ªæ•°æ® $x_i$ ï¼Œä½¿ç”¨ $K$ è¿‘é‚»ç®—æ³•æ‰¾åˆ°å…¶ $k$ ä¸ªè¿‘é‚»æ ·æœ¬
   - ![](image/9.11.png)
2.  å±€éƒ¨çº¿æ€§é‡æ„
   - å‡è®¾æ ·æœ¬ä¹‹é—´çš„çº¿æ€§å…³ç³»ç”¨çŸ©é˜µ $W \in \mathbb{R}^{nÃ—n}$ è¡¨ç¤ºï¼Œå…¶å…ƒç´  w_ijè¡¨ç¤ºæ ·æœ¬ ğ‘— åœ¨é‡æ„æ ·æœ¬ä¸º ğ‘– æ—¶çš„ç³»æ•°ï¼Œéœ€è¦æœ€å°åŒ–æ ·æœ¬çš„é‡æ„è¯¯å·® $\min\limits_{W} \overset{n}\sum\limits_{i=1} ||x_i-\sum\limits_{j}w_{ij}x_j||^2_2$ 
3.  å¯»æ‰¾ä½çº¬è¡¨ç¤º
   - lä½¿ç”¨æƒé‡ ğ– ï¼Œæ‰¾åˆ°æ•°æ®çš„ä½ç»´è¡¨ç¤ºğ˜ï¼Œéœ€è¦æœ€å°åŒ–ä½ç»´æ ·æœ¬çš„é‡æ„è¯¯å·® $\min\limits_{Y} \overset{n}\sum\limits_{i=1} ||y_i-\sum\limits_{j}w_{ij}y_j||^2_2$

LocallyLinearEmbeddingç±»ï¼š

| **å‚æ•°**            | **è¯´æ˜**                                                     |
| ------------------- | ------------------------------------------------------------ |
| n_neighbors         | è®¾ç½®è¿‘é‚»å‚æ•°kï¼Œé»˜è®¤å€¼ä¸º5                                     |
| reg                 | æ­£åˆ™åŒ–é¡¹çš„ç³»æ•°ï¼Œé»˜è®¤å€¼ä¸º0.001                                |
| method              | è®¾ç½®LLEåœ¨è¿›è¡Œå±€éƒ¨çº¿æ€§é‡æ„æ—¶ä½¿ç”¨çš„ç®—æ³•ï¼Œå¯é€‰æ‹©â€˜standardâ€™, â€˜hessianâ€™, â€˜modifiedâ€™æˆ–â€™ltsaâ€™ |
| neighbors_algorithm | è®¾å®šè®¡ç®—æœ€è¿‘é‚»çš„ç®—æ³•{â€˜ball_treeâ€™, â€˜kd_treeâ€™, â€˜bruteâ€™æˆ–â€˜autoâ€˜} |
| eigen_solver        | è®¾ç½®æ±‚è§£ç‰¹å¾å€¼çš„ç®—æ³•{â€˜autoâ€™, â€˜arpackâ€™æˆ–â€˜denseâ€™}              |
| n_components        | è®¾ç½®é™ç»´åçš„ç»´æ•°ï¼Œä¸€èˆ¬è®¾ä¸º2-5                                |

- Bandwidthå‚æ•°å¯é€‰ï¼Œå¦‚æœä¸è¾“å…¥ï¼Œåˆ™ä½¿ç”¨estimate_bandwidthå‡½æ•°æ¥è®¡ç®—ã€‚é»˜è®¤æ˜¯ä½¿ç”¨æ ·æœ¬ä¸¤ä¸¤ä¹‹é—´è·ç¦»çš„0.3åˆ†ä½æ•°ä½œä¸ºå¸¦å®½

  Seedså‚æ•°åŒæ ·ï¼Œè‹¥æ²¡æœ‰è®¾ç½®ä¸”bin_seeding=Trueåˆ™ä½¿ç”¨get_bin_seedså‡½æ•°æ¥è®¡ç®—ï¼Œé»˜è®¤ä½¿ç”¨bandwidthä½œä¸ºç½‘æ ¼å¤§å°æ¥é€‰æ‹©èµ·å§‹ç‚¹

  cluster_allä¸ºTrueæ—¶ï¼Œå­¤ç«‹ç‚¹ä¼šè¢«åˆ†åˆ°æœ€è¿‘çš„ç°‡å†…ï¼›è‹¥ä¸ºFalseï¼Œåˆ™å¯¹å­¤ç«‹ç‚¹æ ‡è®°ä¸º-1

æ„å»ºä¸‰ç»´Så‹æ›²çº¿ï¼š

```python
# è½½å…¥å¿…è¦åº“
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
%matplotlib inline

# äº§ç”Ÿæ•°æ®
n_points = 1000
X, color = datasets.samples_generator.make_s_curve(n_points, random_state=10)

# ç»˜å›¾
fig = plt.figure(figsize=(12, 8))
ax = Axes3D(fig, elev=10, azim=80)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax.set_title('S Curve', fontsize=20)
```

![](image/9.13.png)

è¿‘é‚»æ•°å¯¹é™ç»´ç»“æœçš„å½±å“ï¼š

```python
from sklearn.manifold import LocallyLinearEmbedding

# è®¾ç½®ä¸åŒè¿‘é‚»æ•°
n_neighbors = [5, 10, 15, 20, 50, 80] 
# é™è‡³2ç»´
n_components = 2

fig = plt.figure(figsize=(20, 8))

for i, number in enumerate(n_neighbors):
    # LLEè¿›è¡Œé™ç»´
    Y = LocallyLinearEmbedding(n_neighbors=number, n_components=n_components, random_state=10).fit_transform(X)
 
    # ç»˜å›¾
    ax = fig.add_subplot(231 + i)
    plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
    plt.title("n_neighbors = %s" % (number))
```

![](image/9.14.png)

- é™ç»´åé¢œè‰²ç›¸è¿‘çš„ç‚¹èšé›†åœ¨ä¸€èµ·ï¼Œè¯æ˜ä¿ç•™äº†æ•°æ®å±€éƒ¨çš„çº¿æ€§å…³ç³»

å±€éƒ¨çº¿æ€§é‡æ„ç®—æ³•å¯¹é™ç»´ç»“æœçš„å½±å“ï¼š

```python
# è®¾ç½®ä¸åŒçš„ç®—æ³•
methods = ['standard', 'ltsa', 'hessian', 'modified']

fig = plt.figure(figsize=(20, 5))

for i, methods in enumerate(methods):
    # LLEè¿›è¡Œé™ç»´ï¼Œè¿‘é‚»æ•°å›ºå®šä¸º50
    Y = LocallyLinearEmbedding(n_neighbors=50, n_components=n_components, method=methods, random_state=10).fit_transform(X)
 
    # ç»˜å›¾
    ax = fig.add_subplot(141 + i)
    plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
    plt.title("method = %s" % (methods))
```

![](image/9.15.png)

#### å¤šç»´æ ‡åº¦æ³•ï¼ˆMDSï¼‰

å¤šç»´å°ºåº¦å˜æ¢(Multi-dimensional Scalingï¼ŒMDS)çš„ç›®æ ‡æ˜¯æ‰¾åˆ°æ•°æ®çš„ä½ç»´è¡¨ç¤ºï¼Œä½¿å¾—é™ç»´å‰åæ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ä¿¡æ¯å°½é‡å¾—ä»¥ä¿ç•™ã€‚

å¤šç»´å°ºåº¦å˜æ¢èƒ½å¤Ÿåªåˆ©ç”¨æ ·æœ¬é—´çš„è·ç¦»ä¿¡æ¯ï¼Œæ‰¾åˆ°æ¯ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾è¡¨ç¤ºï¼Œä¸”åœ¨è¯¥ç‰¹å¾è¡¨ç¤ºä¸‹æ ·æœ¬çš„è·ç¦»ä¸åŸå§‹çš„è·ç¦»å°½é‡æ¥è¿‘ã€‚

MDSç®—æ³•åŸç†åŠæ­¥éª¤ï¼š

- å‡è®¾æ•°æ®é›†åŒ…å«nä¸ªæ ·æœ¬ï¼Œæ•°æ®é›†ç”¨ $n \times p$ çš„çŸ©é˜µ $X$ è¡¨ç¤ºï¼Œæ ·æœ¬ $i$ èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ª $p$ ç»´çš„ç‰¹å¾å‘é‡ $x_i$ è¡¨ç¤ºï¼Œæ ·æœ¬é—´è·ç¦»çŸ©é˜µå¯ä»¥ç”¨ $n\times n$ çš„çŸ©é˜µ $D$ è¡¨ç¤ºï¼Œå…¶å…ƒç´  $d_ij$ è¡¨ç¤ºæ ·æœ¬ $i$ å’Œæ ·æœ¬ $j$ çš„è·ç¦»

- å‡è®¾ $z_i$ ä¸ºæ ·æœ¬ $x_i$ åœ¨ä½ç»´ç©ºé—´çš„è¡¨ç¤ºï¼Œå³æ ·æœ¬å’Œæ ·æœ¬çš„æ¬§å¼è·ç¦»ä¸º $||z_i-z_j||_2$

- $MDS$ çš„ä¼˜åŒ–ç›®æ ‡ä¸ºï¼š $\min\limits_{z_1,z_2,\dots,z_n} \sum\limits_{j \neq j}(||z_i-z_j||_2-d_{ij})^2$

- ç”±ç‰¹å¾çŸ©é˜µ $X$ æ„é€ çŸ©é˜µ $B=XX^T$ ï¼Œåˆ™ $B$ ä¸­çš„å…ƒç´ å¯ä»¥è¡¨ç¤ºä¸º $b_{ij}=x_i^Tx_j$ 

- å‡è®¾ç‰¹å¾çŸ©é˜µ $X$ èƒ½å®Œå…¨ä¿ç•™è·ç¦»ä¿¡æ¯ï¼Œåˆ™ $d_{ij}^2 = (x_i-x_j)^T(x_i-x_j) = b_{ii} + b_{jj} - 2b_{ij}$

- è¿›ä¸€æ­¥è®¡ç®—å‡º $B$ çš„å…¬å¼ä¸º

  - $$
    b_{ij} = (\frac{1}{n}\overset{n}\sum\limits_{i=1}d_{ij}^2
    + \frac{1}{n}\overset{n}\sum\limits_{j=1}d_{ij}^2
    - \frac{1}{n^2}\overset{n}\sum\limits_{i=1}\overset{n}\sum\limits_{j=1}d_{ij}^2
    - d_{ij}^2)
    $$

- å¯¹çŸ©é˜µ $B$ è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ $B=U \wedge U^T$

- å–å‰ $l$ ä¸ªæœ€å¤§ç‰¹å¾å€¼æ„æˆå¯¹è§’çŸ©é˜µ $\wedge_l$ 

- ç”¨å¯¹åº”çš„ç‰¹å¾å‘é‡æŒ‰åˆ—ç»„åˆæˆ $U_l$

- é™ç»´åçš„æ•°æ®çŸ©é˜µè®¡ç®—æ–¹æ³• $Z=U_l \wedge_l^{\frac{1}{2}}$ 

MDS ç±»ï¼š

| **å‚æ•°**      | **è¯´æ˜**                                                     |
| ------------- | ------------------------------------------------------------ |
| n_components  | è®¾ç½®é™ç»´åçš„æ•°æ®ç»´åº¦ï¼Œé»˜è®¤ä¸º2                                |
| metric        | å¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸ºTrueï¼Œä½¿ç”¨è·ç¦»åº¦é‡ï¼›å¦åˆ™ä½¿ç”¨éè·ç¦»åº¦é‡SMACOF   |
| dissimilarity | å®šä¹‰å¦‚ä½•è®¡ç®—ä¸ç›¸ä¼¼åº¦ï¼Œâ€˜euclideanâ€™ä½¿ç”¨æ¬§æ°è·ç¦»ï¼Œâ€˜precomputedâ€™è‡ªè¡Œè®¡ç®—è·ç¦» |
| eps           | æµ®ç‚¹æ•°ï¼Œç”¨äºæŒ‡å®šæ”¶æ•›é˜ˆå€¼ï¼Œé»˜è®¤ä¸º1e-3                         |

| **å±æ€§**   | **æ„ä¹‰**                             |
| ---------- | ------------------------------------ |
| embedding_ | è¿”å›åŸå§‹æ•°æ®é›†åœ¨ä½ç»´ç©ºé—´ä¸­çš„åµŒå…¥çŸ©é˜µ |

åˆ©ç”¨MDSå°†ä¸‰ç»´Så‹æ›²çº¿å°†è‡³2ç»´ï¼š

```python
from sklearn.manifold import MDS

## ä½¿ç”¨MDSé™ç»´å¹¶å¾—åˆ°é™ç»´ç»“æœ
mds = MDS(n_components, random_state=10)
Y = mds.fit_transform(X)

## é™ç»´å¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
plt.title("MDS") 
```

![](image/9.16.png)



æ ¹æ®åŸå¸‚é—´è·ç¦»è¿˜åŸåŸå¸‚åæ ‡ï¼š

```python
## æ„å»ºè·ç¦»çŸ©é˜µ
import pandas as pd

d = pd.DataFrame([[0, 1064, 1055, 1187],
                  [1064, 0, 1675, 1717],
                  [1055, 1675, 0, 2192],
                  [1187, 1717, 2192, 0]], 
                 columns=['åŒ—äº¬','ä¸Šæµ·','å“ˆå°”æ»¨','å…°å·'], 
                 index=['åŒ—äº¬','ä¸Šæµ·','å“ˆå°”æ»¨','å…°å·'])
```

| **å•ä½ï¼šå…¬é‡Œ** | **åŒ—äº¬** | **ä¸Šæµ·** | **å“ˆå°”æ»¨** | **å…°å·** |
| -------------- | -------- | -------- | ---------- | -------- |
| **åŒ—äº¬**       | 0        | 1064     | 1055       | 1187     |
| **ä¸Šæµ·**       | 1064     | 0        | 1675       | 1717     |
| **å“ˆå°”æ»¨**     | 1055     | 1675     | 0          | 2192     |
| **å…°å·**       | 1187     | 1717     | 2192       | 0        |

```python
## ä½¿ç”¨MDSé™ç»´å¹¶å¾—åˆ°é™ç»´ç»“æœ
mds = MDS(n_components, random_state=10)
Y = mds.fit_transform(d)

## é™ç»´å¯è§†åŒ–
plt.figure(figsize=(6, 4))
plt.scatter(Y[:, 0], Y[:, 1], c='red', s=20)
plt.title("æ ¹æ®åŸå¸‚é—´è·ç¦»è¿˜åŸåŸå¸‚åæ ‡") 
for item,city in enumerate(d.columns):
    plt.text(Y[item, 0]+20, Y[item, 1]+20, city)
```

![](image/9.17.png)

#### t-åˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ç®—æ³•ï¼ˆt-SNEï¼‰

SNM çš„ä¸»è¦æ€æƒ³æ˜¯å‡è®¾x_i ,x_jæ˜¯é«˜ç»´ç©ºé—´ä¸­çš„ä¸¤ä¸ªç‚¹ï¼Œx_jçš„å–å€¼æœä»ä»¥x_iä¸ºä¸­å¿ƒæ–¹å·®ä¸ºÏƒ_içš„é«˜æ–¯åˆ†å¸ƒï¼Œ x_içš„å–å€¼æœä»ä»¥x_jä¸ºä¸­å¿ƒæ–¹å·®ä¸ºÏƒ_jçš„é«˜æ–¯åˆ†å¸ƒï¼Œåˆ™x_i ä¸x_j ç›¸ä¼¼åº¦çš„æ¡ä»¶æ¦‚ç‡ä¸ºï¼š
$$
p_{(j|i)} = \frac{\exp(-||x_i-x_j||^2 / (2\sigma_i^2))}{\sum\limits_{k\neq i} exp(-||x_i-x_k||^2 / (2\sigma_i^2))}
$$
å‡è®¾ $x_i,x_j$ æ˜ å°„åˆ°ä½çº¬ç©ºé—´åå¯¹åº” $y_i,y_j$ ï¼Œåˆ™å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ä¸ºï¼ˆä½çº¬ç©ºé—´ä¸­çš„æ–¹å·®ç›´æ¥è®¾ç½®ä¸º $\sigma_i=\frac{1}{\sqrt{2}}$ï¼‰ï¼š
$$
q_{(j|i)} = \frac{\exp(-||x_i-x_j||^2)}{\sum\limits_{k\neq i} exp(-||x_i-x_k||^2)}
$$
è‹¥é™ç»´æ•ˆæœå¥½ï¼Œå±€éƒ¨ç‰¹å¾ä¿ç•™å®Œæ•´ï¼Œåˆ™åº”æœ‰ $p_{j|i} = q_{j|i}$ ï¼Œæ‰€ä»¥ä¼˜åŒ–ä¸¤ä¸ªåˆ†éƒ¨ä¹‹é—´çš„è·ç¦»ç”¨ $KL$ æ•£åº¦ï¼Œç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š
$$
C=\sum\limits_{i}KL(P_i||Q_i) = \sum\limits_{i}\sum\limits_{j} p_{(j|i)}\log \frac{p_{(j|i)}}{q_{(j|i)}}
$$
**å›°æƒ‘åº¦ï¼š**

å›°æƒ‘åº¦å¯ä»¥åæ˜ ä¸€ä¸ªç‚¹é™„è¿‘çš„æœ‰æ•ˆè¿‘é‚»ç‚¹ä¸ªæ•°ï¼š$Perp(P_i) = 2^{H(p_i)}$ (H(x)ä¸ºç†µ)

$SNE$ å¯¹å›°æƒ‘åº¦çš„è°ƒæ•´è¾ƒä¸ºé²æ£’ï¼Œé€šå¸¸å–å€¼ä¸º 5-50 ä¹‹é—´ï¼Œç¡®å®šåç”¨äºŒåˆ†æœç´¢å¯»æ‰¾åˆé€‚çš„ $\sigma$

t-SNE ï¼š

tåˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ï¼ˆt-Distribution Stochastic Neighbour Embeddingï¼‰ç”±Maatenäº2008å¹´æå‡ºï¼Œæ˜¯åŸºäº2002å¹´Hintonæå‡ºçš„éšæœºé‚»åŸŸåµŒå…¥ï¼ˆ Stochastic Neighbour Embeddingï¼ŒSNEï¼‰çš„æ”¹è¿›ç®—æ³•ã€‚

t-SNEè¾ƒSNEåšå‡ºå¦‚ä¸‹è°ƒæ•´ï¼š

- åŠ å…¥å¯¹ç§°æ€§ï¼Œä½¿ $p_{ij}=p_{ji},q_{ij}=q_{ji}$ 
- åœ¨ä½ç»´ç©ºé—´ä¸‹ä½¿ç”¨tåˆ†å¸ƒæ›¿ä»£é«˜æ–¯åˆ†å¸ƒè¡¨ç¤ºä¸¤ç‚¹ç›¸ä¼¼åº¦ï¼Œè§£å†³â€œæ‹¥æŒ¤â€é—®é¢˜
- ä¸ºè§£å†³å¼‚å¸¸å€¼é—®é¢˜ï¼Œä¿®æ­£è”åˆæ¦‚ç‡åˆ†å¸ƒä¸ºï¼š $p_{ij} = \frac{p_{(i|j)} + p_{(j|i)}}{2}$ 
- ä½¿ç”¨ $t$ åˆ†å¸ƒåçš„ç›¸ä¼¼åº¦ä¸º ï¼š$q_{ij} = \frac{(1+|y_i-y_j|^2)^{-1}}{\sum\limits_{k\neq l}(1+|y_k-y_l|^2)^{-1}}$ 

TSNE ç±»ï¼š

| **å‚æ•°**      | **è¯´æ˜**                                               |
| ------------- | ------------------------------------------------------ |
| n_components  | é™ç»´åçš„ç»´åº¦ï¼Œé»˜è®¤ä¸º2                                  |
| learning_rate | å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º200.0ï¼Œå»ºè®®åœ¨[10,  1000]ä¹‹é—´             |
| perplexity    | å›°æƒ‘åº¦ï¼Œé»˜è®¤ä¸º30ï¼Œæ•°æ®é›†è¶Šå¤§æ•°å€¼åº”è¶Šå¤§ï¼Œå»ºè®®åœ¨5-50ä¹‹é—´ |

| **å±æ€§**   | **è¯´æ˜**                             |
| ---------- | ------------------------------------ |
| embedding_ | è¿”å›åŸå§‹æ•°æ®é›†åœ¨ä½ç»´ç©ºé—´ä¸­çš„åµŒå…¥çŸ©é˜µ |

åˆ©ç”¨t-SNEå°†ä¸‰ç»´Så‹æ›²çº¿é™è‡³2ç»´ï¼š

```python
from sklearn.manifold import TSNE

## ä½¿ç”¨t-SNEé™ç»´å¹¶å¾—åˆ°é™ç»´ç»“æœ
tsne = TSNE(n_components, random_state=10)
Y = tsne.fit_transform(X)

## é™ç»´å¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
plt.title("t-SNE")
```

![](image/9.18.png)

å›°æƒ‘åº¦å¯¹t-SNEé™ç»´ç»“æœçš„å½±å“ï¼š

```python
# è®¾ç½®ä¸åŒå›°æƒ‘åº¦
perplexs = [2, 5, 15, 30, 50, 70]

fig = plt.figure(figsize=(20, 8))

for i, perplex in enumerate(perplexs):
    # t-SNEè¿›è¡Œé™ç»´
    Y = TSNE(n_components, perplexity=perplex, random_state=10).fit_transform(X)
 
    # ç»˜å›¾
    ax = fig.add_subplot(231 + i)
    plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
    plt.title("perplexity = %s" % (perplex))
```

![](image/9.19.png)

æ‰‹å†™æ•°å­—è¯†åˆ«æ•°æ®é›†çš„é™ç»´åˆ†æï¼š

```python
# è½½å…¥æ‰‹å†™æ•°å­—è¯†åˆ«æ•°æ®é›†
from sklearn.datasets import load_digits

## é€‰æ‹©å…¶ä¸­çš„äº”ç±»
digits = load_digits(n_class=5)
X = digits.data
y = digits.target

# å±•ç¤ºæ•°æ®
fig = plt.figure(figsize=(15, 8))

for item in range(5):
    ax = fig.add_subplot(151 + item)
    plt.imshow(X[item,:].reshape(8, 8), cmap=plt.cm.rainbow)
```

![](image/9.20.png)

```python
# æ„å»º2Då±•ç¤ºå‡½æ•°
import numpy as np

def plot_embedding_2d(X, title):
    ## æ•°æ®å½’ä¸€åŒ–
    x_min, x_max = np.min(X,axis=0), np.max(X,axis=0)
    X = (X - x_min) / (x_max - x_min)

    ## ç»˜åˆ¶æ•£ç‚¹å›¾
    fig = plt.figure(figsize=(12, 8))
    for i in range(X.shape[0]):
        plt.text(X[i][0], X[i][1],str(digits.target[i]),
                 color=plt.cm.Set1(y[i] / 5.),
                 fontdict={'size': 9})
    ## æ·»åŠ æ ‡é¢˜
    plt.title(title)
tsne = TSNE(n_components=2, random_state=10)
Y = tsne.fit_transform(X)
plot_embedding_2d(Y, "t-SNE 2D")

# æ„å»º3Dç»˜å›¾å‡½æ•°
def plot_embedding_3d(X, title):
    
    ## æ•°æ®å½’ä¸€åŒ–
    x_min, x_max = np.min(X,axis=0), np.max(X,axis=0)
    X = (X - x_min) / (x_max - x_min)
    
    ## ç»˜åˆ¶æ•£ç‚¹å›¾
    fig = plt.figure(figsize=(12, 8))
    ax = Axes3D(fig, elev=10, azim=60)
    for i in range(X.shape[0]):
        ax.text(X[i][0], X[i, 1], X[i,2],str(digits.target[i]),
                 color=plt.cm.Set1(y[i] / 5.),
                 fontdict={ 'size': 9})
    ## æ·»åŠ æ ‡é¢˜
    plt.title(title)
tsne = TSNE(n_components=3, random_state=10)
Y = tsne.fit_transform(X)
plot_embedding_3d(Y, "t-SNE 3D")
```

![](image/9.21.png)

![](image/9.22.png)

## **æœºå™¨å­¦ä¹ æµæ°´çº¿**

æ•°æ®é¢„å¤„ç†ã€å»ºç«‹è®­ç»ƒå¹¶è¯„ä¼°Kè¿‘é‚»æ¨¡å‹

```python
# è¯»å–æ•°æ®
import pandas as pd
diabetes = pd.read_csv('./input/india_diabetes.csv')

# ç›®æ ‡ä¸æ•°æ®åˆ†ç¦»
X = diabetes.drop(['Outcome'], axis=1)
y = diabetes['Outcome']

# åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=.2, random_state=10)

# æ ‡å‡†åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X_train)

## å¯¹è®­ç»ƒé›†è¿›è¡Œæ ‡å‡†åŒ–
X_train_scaled = scaler.transform(X_train)

## å¯¹æµ‹è¯•é›†è¿›è¡Œç›¸åŒæ ‡å‡†åŒ–
X_test_scaled = scaler.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
# å»ºç«‹æ¨¡å‹
model_knn = KNeighborsClassifier()

# è®­ç»ƒæ¨¡å‹
model_knn.fit(X_train_scaled, y_train)

# è¯„ä»·æ¨¡å‹
print("æµ‹è¯•é›†åˆ†ç±»æ­£ç¡®ç‡ï¼š", round(model_knn.score(X_test_scaled, y_test), 2))
```

æœ‰æ²¡æœ‰ä»€ä¹ˆæ–¹æ³•å¯ä»¥æŠŠä¸€äº›æ­¥éª¤æ•´åˆèµ·æ¥ï¼Œå‡å°‘ä»£ç é‡?

### Pipeline ç±»

Sklearn ä¸­ `pipeline` æ¨¡å—ä¸­çš„ `Pipeline` ç±»ï¼Œå®ç°æœºå™¨å­¦ä¹ è¿‡ç¨‹ä¸­å…¨éƒ¨æ­¥éª¤çš„æµå¼åŒ–å°è£…å’Œç®¡ç†ï¼Œå¤§å¹…å‡å°‘ä»£ç é‡ã€‚

Pipelineé€šå¸¸æ­¥éª¤ï¼š

1. æ•°æ®é¢„å¤„ç†å­¦ä¹ å™¨
2. ç‰¹å¾é€‰æ‹©å­¦ä¹ å™¨
3. æ‰§è¡Œé¢„æµ‹çš„å­¦ä¹ å™¨

* é™¤æœ€åä¸€ä¸ªå­¦ä¹ å™¨ï¼Œå…¶ä½™å­¦ä¹ å™¨å¿…é¡»æœ‰ `transform` æ–¹æ³•ï¼Œç”¨äºæ•°æ®è½¬æ¢ã€‚

![](image/pipeline.jpg)

| **å‚æ•°** | **è¯´æ˜**                                                 |
| -------- | -------------------------------------------------------- |
| steps    | å­¦ä¹ å™¨åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºä»¥å…ƒç»„åˆ—è¡¨çš„æ–¹å¼ç»™å‡ºï¼Œæœ€åä¸€ä¸ªæ˜¯ä¼°è®¡å™¨ |

| **å±æ€§**   | **è¯´æ˜**                                               |
| ---------- | ------------------------------------------------------ |
| name_steps | æŸ¥çœ‹æ¯ä¸ªæ­¥éª¤çš„åç§°å’Œå‚æ•°ï¼Œå­—å…¸å¯¹è±¡ï¼Œé”®ä¸ºåç§°ï¼Œå€¼ä¸ºå‚æ•° |

| **æ–¹æ³•**             | **è¯´æ˜**                                 |
| -------------------- | ---------------------------------------- |
| fit(X, y)            | è®­ç»ƒæ¨¡å‹                                 |
| fit_predict(X,  y)   | å…ˆè®­ç»ƒæ¨¡å‹ï¼Œå†è¿›è¡Œé¢„æµ‹                   |
| fit_transform(X,  y) | å…ˆè®­ç»ƒæ¨¡å‹ï¼Œå†åˆ©ç”¨æœ€åä¸€ä¸ªå­¦ä¹ å™¨è¿›è¡Œè½¬æ¢ |
| predict(X)           | è¿›è¡Œé¢„æµ‹                                 |
| predict_log_proba(X) | é¢„æµ‹å¯¹æ•°æ¦‚ç‡                             |
| predict_proba(X)     | é¢„æµ‹æ¦‚ç‡                                 |
| score(X, y)          | æ¨¡å‹è¯„ä»·                                 |
| set_params( )        | ä¿®æ”¹å­¦ä¹ å™¨çš„å‚æ•°                         |

æ„å»ºæœºå™¨å­¦ä¹ æµæ°´çº¿ï¼š

![](image/pipeline_0.png)

```python
from sklearn.pipeline import Pipeline

# æ„å»ºæµæ°´çº¿
pipe = Pipeline(steps=[('scaler',StandardScaler()), 
                       ('knn', KNeighborsClassifier())])

# è®­ç»ƒ
pipe.fit(X_train, y_train)

# è¯„ä¼°
print("æµ‹è¯•é›†åˆ†ç±»æ­£ç¡®ç‡ï¼š", round(pipe.score(X_test, y_test), 2))

""" è¿è¡Œç»“æœï¼š
æµ‹è¯•é›†åˆ†ç±»æ­£ç¡®ç‡ï¼š0.77
"""
# ä¹‹å‰è¿™äº›æ­¥éª¤ä¸€å…±ç¼–å†™äº†6è¡Œä»£ç 
```

```python
# æŸ¥çœ‹å…·ä½“æ­¥éª¤
pipe.named_steps['knn']
"""
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=3, p=2,
           weights='uniform')
"""

# ä¿®æ”¹å­¦ä¹ å™¨å‚æ•°
pipe.set_params(knn__weights='distance')
"""
Pipeline(memory=None,
     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=3, p=2,
           weights='distanceâ€˜))])
"""
```

åœ¨æµæ°´çº¿ä¸­ä½¿ç”¨ç½‘æ ¼æœç´¢ï¼š

![](image/ç½‘æ ¼æœç´¢.png)

å…¶ä¸­ï¼Œäº¤å‰éªŒè¯åœ¨æœ€å¤–å±‚ã€‚å¯¹äºæ•°æ®é›†ï¼Œä¹Ÿè¦å…ˆåˆ†å‰²æ•°æ®é›†ï¼Œå†æ‰§è¡ŒPipe

```python
from sklearn.model_selection import GridSearchCV

# è®¾ç½®å‚æ•°ç½‘ç»œ
param_grid = {'knn__n_neighbors': [2, 4, 6, 8, 10],
              'knn__weights': ['uniform', 'distance']}
	# é”®çš„å½¢å¼ä¸ºï¼šâ€œæ­¥éª¤åç§°__å‚æ•°åç§°â€

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# æµ‹è¯•é›†ä¸Šçš„å¾—åˆ†
grid_search.score(X_test, y_test)
"""
æµ‹è¯•é›†ä¸Šåˆ†ç±»æ­£ç¡®ç‡ï¼š0.79                           

æœ€ä¼˜å‚æ•°ï¼š{'knn__n_neighbors': 6, 'knn__weights': 'distance'}
"""
```

åœ¨ç½‘æ ¼æœç´¢ä¸­åŠ å…¥å­¦ä¹ å™¨çš„é€‰æ‹©ï¼š

```python
# åœ¨ç½‘æ ¼æœç´¢ä¸­åŠ å…¥å­¦ä¹ å™¨çš„é€‰æ‹©
from sklearn.preprocessing import MinMaxScaler

# è®¾å®šéœ€è¦é€‰æ‹©çš„å­¦ä¹ å™¨
scale_selector = [StandardScaler(), MinMaxScaler()]

# è®¾ç½®å‚æ•°ç½‘ç»œ
param_grid = {'scaler': scale_selector,
              'model__n_neighbors': [2, 4, 6, 8, 10],
              'model__weights': ['uniform', 'distance']}

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# æŸ¥çœ‹æœ€ä¼˜å­¦ä¹ å™¨
grid_search.best_estimator_

# æ¨¡å‹è¯„ä»·
grid_search.score(X_test, y_test)
"""
æµ‹è¯•é›†ä¸Šåˆ†ç±»æ­£ç¡®ç‡ï¼š0.78
"""
```

![](image/æµæ°´çº¿_0.png)

```python
# åœ¨ç½‘æ ¼æœç´¢ä¸­åŠ å…¥å­¦ä¹ å™¨çš„é€‰æ‹©
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# è®¾å®šéœ€è¦é€‰æ‹©çš„å­¦ä¹ å™¨
model_selector = [SVC(), LogisticRegression(random_state=10)]

# è®¾ç½®å‚æ•°ç½‘ç»œ
param_grid = {'scaler':scale_selector,
              'model': model_selector,
              'model__class_weight':['balanced', 
                                      None],
              'model__C':[0.01, 0.1, 
                          0.2, 0.5, 1]}

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)
"""
æµ‹è¯•é›†ä¸Šåˆ†ç±»æ­£ç¡®ç‡ï¼š0.79
"""
```

![](image/æµæ°´çº¿_1.png)

åœ¨æµæ°´çº¿ä¸­åŠ å…¥ç‰¹å¾é€‰æ‹©å¹¶è¿›è¡Œç½‘æ ¼æœç´¢ï¼š

![](image/æµæ°´çº¿_2.png)

```python
from sklearn.feature_selection import RFECV
from sklearn.tree import DecisionTreeClassifier

# åœ¨æµæ°´çº¿ä¸­åŠ å…¥ç‰¹å¾é€‰æ‹©
pipe_new = Pipeline(steps=[('scaler',StandardScaler()), 
                           ('selector', RFECV(DecisionTreeClassifier(random_state=10), cv=5)),
                           ('model', KNeighborsClassifier())])

# è®¾ç½®å‚æ•°ç½‘ç»œ
param_grid = {'scaler':scale_selector,
              'model': model_selector,
              'model__class_weight':['balanced', None],
              'model__C':[0.01, 0.1, 0.2, 0.5, 1]}  

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(estimator=pipe_new, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# è®¿é—®æ­¥éª¤å±æ€§ï¼ŒæŸ¥çœ‹ç‰¹å¾æ’å
pd.Series(grid_search.best_estimator_.named_steps['selector'].ranking_, index=X_train.columns)
"""è¿è¡Œç»“æœå¦‚ä¸‹ï¼š"""
```

|                 **ç‰¹å¾** | **æ’å** |
| -----------------------: | -------- |
|              Pregnancies | 2        |
|                  Glucose | 1        |
|            BloodPressure | 1        |
|            SkinThickness | 3        |
|                  Insulin | 4        |
|                      BMI | 1        |
| DiabetesPedigreeFunction | 1        |
|                      Age | 1        |

åœ¨æµæ°´çº¿ä¸­åŠ å…¥ç‰¹å¾é™ç»´å¹¶è¿›è¡Œç½‘æ ¼æœç´¢ï¼š

![](image/æµæ°´çº¿_3.png)

```python
from sklearn.decomposition import PCA

# åœ¨ç®¡é“ä¸­åŠ å…¥PCA
pipe_new = Pipeline(steps=[('scaler',StandardScaler()), 
                           ('decomposition', PCA(3)),
                           ('model', KNeighborsClassifier())])

# è®¾ç½®å‚æ•°ç½‘ç»œ
param_grid = {'scaler':scale_selector,
              'model': model_selector,
              'decomposition__n_components':[2, 3, 4, 5, 6],
              'model__class_weight':['balanced', None],
              'model__C':[0.01, 0.1, 0.2, 0.5, 1]}  
# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(estimator=pipe_new, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)   

# æŸ¥çœ‹æ–¹å·®è´¡çŒ®ç‡
grid_search.best_estimator_.named_steps['decomposition'].explained_variance_ratio_.sum()
"""
æ–¹å·®è´¡çŒ®ç‡ï¼š0.90
"""
```

æ„å»ºå¤æ‚æœºå™¨å­¦ä¹ æµæ°´çº¿ï¼š

![](image/æµæ°´çº¿_4.png)

```python
# æ„å»ºæµæ°´çº¿
pipe_rf = Pipeline(steps=[('scaler1', StandardScaler()),
                          ('model1', RandomForestClassifier(n_estimators=100,            
                                                            random_state=10))])

pipe_knn = Pipeline(steps=[('scaler2', StandardScaler()),
                           ('decomposition1', PCA(n_components=6)),
                           ('model2', KNeighborsClassifier(n_neighbors=6,  
                                                           weights='distance'))])

pipe_lr = Pipeline(steps=[('scaler3', MinMaxScaler()),
                          ('decomposition2',     
                           RFECV(DecisionTreeClassifier(random_state=10), cv=5)),
                          ('model3', LogisticRegression(random_state=10))])

# åˆ›å»ºæµæ°´çº¿åç§°å­—å…¸
pipe_dic = {0: 'éšæœºæ£®æ—', 1: 'åŠ æƒK-è¿‘é‚»', 2:'é€»è¾‘å›å½’'}
                           
# åˆ›å»ºæµæ°´çº¿åˆ—è¡¨
pipelines = [pipe_rf, pipe_knn, pipe_lr]

# è®­ç»ƒæµæ°´çº¿
for pipe in pipelines:
    pipe.fit(X_train, y_train)

# æ¨¡å‹è¯„ä»·
for index, item in enumerate(pipelines):
    print('%såœ¨æµ‹è¯•é›†ä¸Šåˆ†ç±»æ­£ç¡®ç‡: %.3f' % (pipe_dic[index], 
                                         item.score(X_test, y_test)))
"""
éšæœºæ£®æ—åœ¨æµ‹è¯•é›†ä¸Šåˆ†ç±»æ­£ç¡®ç‡: 0.825
åŠ æƒK-è¿‘é‚»åœ¨æµ‹è¯•é›†ä¸Šåˆ†ç±»æ­£ç¡®ç‡: 0.780
é€»è¾‘å›å½’åœ¨æµ‹è¯•é›†ä¸Šåˆ†ç±»æ­£ç¡®ç‡: 0.740
"""
```

make_pipeline ç±»ï¼š

* Pipeline ç±»éœ€è¦ä¸ºæ¯ä¸€ä¸ªæ­¥éª¤æä¾›ç”¨æˆ·è‡ªè¡ŒæŒ‡å®šçš„åç§°
* make_pipeline ç±»å¯ä»¥æ ¹æ®æ¯ä¸ªæ­¥éª¤æ‰€å±çš„ç±»åˆ«ä¸ºå…¶è‡ªåŠ¨å‘½å
* å…¶åŒ…å«çš„å±æ€§å’Œæ–¹æ³•ä¸ Pipeline ç±»ä¸€è‡´

| **å‚æ•°** | **è¯´æ˜**         |
| -------- | ---------------- |
| steps    | æŒ‰é¡ºåºç»™å‡ºå­¦ä¹ å™¨ |

ä½¿ç”¨ make_pipeline æ„å»ºæµæ°´çº¿ï¼š

```python
# è½½å…¥åº“å‡½æ•°
from sklearn.pipeline import make_pipeline

# æ„å»ºæµæ°´çº¿
model_LR = LogisticRegression(random_state=10)
pipe_make = make_pipeline(StandardScaler(), 
                          RFECV(model_LR, cv=5), 
                          KNeighborsClassifier())

# è®­ç»ƒæ¨¡å‹
pipe_make.fit(X_train, y_train)

# å…·ä½“æŸ¥çœ‹æ¯ä¸ªæ­¥éª¤
pipe_make.named_steps
"""
{'kneighborsclassifier': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
            metric_params=None, n_jobs=1, n_neighbors=5, p=2,
            weights='uniform'), 
 'rfecv': RFECV(cv=5,estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
           penalty='l2', random_state=10, solver='liblinear', tol=0.0001,
           verbose=0, warm_start=False),
 n_jobs=1, scoring=None, step=1, verbose=0), 
 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True)}
""" # å„æ­¥éª¤è‡ªåŠ¨å‘½åçš„åç§°
```

### FeatureUnion ç±»

- å°†è‹¥å¹²ä¸ª**transformer**ï¼ˆè½¬æ¢å™¨ï¼‰å¯¹è±¡ç»„åˆæˆä¸€ä¸ªæ–°çš„**transformer**
- ä¸€ä¸ª**FeatureUnion**å¯¹è±¡æ¥å—è¾“å…¥ä¸€ä¸ª**transformer**å¯¹è±¡åˆ—è¡¨
- è®­ç»ƒé˜¶æ®µï¼Œåˆ—è¡¨ä¸­çš„**transformer**å¹¶è¡Œåº”ç”¨äºæ•°æ®ï¼Œç„¶åå°†ç»“æœæ¨ªå‘è¿æ¥ï¼Œæ‹¼æ¥æˆä¸€ä¸ªæ›´å¤§çš„ç‰¹å¾å‘é‡çŸ©é˜µ
- æœ‰åˆ©äºå°†å¤šä¸ªç‰¹å¾æå–æœºåˆ¶ç»„åˆåˆ°ä¸€ä¸ª**transformer**ä¸­

![](image/æµæ°´çº¿_5.png)

| **å‚æ•°**            | **è¯´æ˜**                                                     |
| ------------------- | ------------------------------------------------------------ |
| transformer_list    | åº”ç”¨äºæ•°æ®çš„transformerå¯¹è±¡åˆ—è¡¨                              |
| transformer_weights | è®¾ç½®æ¯ä¸ªtransformerçš„æƒé‡ï¼Œå­—å…¸å¯¹è±¡ï¼Œé”®ä¸ºtransformerçš„åç§°ï¼Œå€¼ä¸ºæƒé‡ |

| **å±æ€§**            | **è¯´æ˜**                |
| ------------------- | ----------------------- |
| transformer_list    | æŸ¥çœ‹transformerå¯¹è±¡åˆ—è¡¨ |
| transformer_weights | æŸ¥çœ‹transformerçš„æƒé‡   |

| **æ–¹æ³•**             | **è¯´æ˜**               |
| -------------------- | ---------------------- |
| fit(X, y)            | è®­ç»ƒæ¨¡å‹               |
| fit_transform(X,  y) | å…ˆè®­ç»ƒæ¨¡å‹ï¼Œå†è¿›è¡Œè½¬æ¢ |
| predict(X)           | è¿›è¡Œé¢„æµ‹               |
| get_params([deep])   | è·å–transformerçš„å‚æ•°  |
| set_params()         | ä¿®æ”¹transformerçš„å‚æ•°  |
| transform(X)         | è¿›è¡Œè½¬æ¢               |

æ„å»º FeatureUnion ï¼š

```python
from sklearn.pipeline import FeatureUnion
from sklearn.preprocessing import PolynomialFeatures

# æ„å»ºFeatureUnion
combined_features = FeatureUnion([('pca', PCA(n_components=3)),
                                  ('poly', PolynomialFeatures(degree=2, include_bias=False))])

X_features = combined_features.fit_transform(X_train)

# æŸ¥çœ‹è½¬æ¢åæ•°æ®çš„ç»´åº¦
X_features.shape
"""
è½¬æ¢åæ•°æ®çš„ç»´åº¦ï¼š(800, 47)
"""
```

![](image/æµæ°´çº¿_6.png)

ï¼ˆä¸Šå›¾ä¸­çš„ New_data æœ‰47ç»´ï¼‰

å°† FeatureUnion åŠ å…¥æµæ°´çº¿ï¼š

```python
# æ„å»ºæµæ°´çº¿
pipeline = Pipeline([("features", combined_features), ("lr", LogisticRegression(random_state=10))])

param_grid = dict(features__pca__n_components=[1, 2, 3,  
                                               4, 5, 6],
                  lr__C=[0.1, 0.2, 0.5, 1],
                  lr__class_weight=[None, 'balanced'],
                  lr__penalty=['l1', 'l2'])

grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# æŸ¥çœ‹æœ€ä¼˜æµæ°´çº¿åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°
grid_search.score(X_test, y_test)
```

![](image/æµæ°´çº¿_7.png)

make_union ç±»ï¼š

- **FeatureUnion**ç±»éœ€è¦ä¸ºæ¯ä¸€ä¸ª**transformer**æä¾›ç”¨æˆ·è‡ªè¡ŒæŒ‡å®šçš„åç§°
- **make_union**ç±»å¯ä»¥ä¸ºæ¯ä¸ª**transformer**è‡ªåŠ¨å‘½å
- å…¶åŒ…å«çš„å±æ€§å’Œæ–¹æ³•ä¸**FeatureUnion**ç±»ä¸€è‡´

ä½¿ç”¨ make_union æ„å»º FeatureUnion ï¼š

```python
from sklearn.pipeline import make_union

# æ„å»ºFeatureUnion
combined_features = make_union(PCA(n_components=3), PolynomialFeatures(degree=2, include_bias=False))

# æŸ¥çœ‹æ¯ä¸ªtransformerçš„åç§°
combined_features.transformer_list
"""
[('pca',PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,svd_solver='auto', tol=0.0, whiten=False)),
 ('polynomialfeatures',PolynomialFeatures(degree=2, include_bias=False, interaction_only=False))]
""" # å„transformerè‡ªåŠ¨å‘½åçš„åç§°
```